{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import spacy\n",
    "# Download the spacy model first: python -m spacy download en_core_web_sm\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# disable chained assignments to avoid annoying warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Any, Tuple, List, NamedTuple\n",
    "\n",
    "# Dataloader & Configs\n",
    "from src.data.data_generator import SQuAD, Dataset\n",
    "from configs.config import *\n",
    "\n",
    "# Embeddings\n",
    "from src.utils.embeddings import GloVe\n",
    "\n",
    "# Model\n",
    "from src.models.layers import Encoder, Decoder, CustomMasking\n",
    "from src.models.loss import MaskedLoss\n",
    "from src.models.trainers.metrics import Perplexity, MaskedAccuracy\n",
    "from src.models.callbacks import BatchLogs, CustomLearningRateScheduler, GetEpochNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists! Loading from .pkl...\n",
      "\n",
      "Dir path ./data/squadv1.1.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset_creator = SQuAD()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator(dataset_config, path_config, tokenized=False, tensor_type=False, compute_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists! Loading from .pkl...\n",
      "\n",
      "Dir path ./data/squadv1.1.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset_creator = SQuAD()\n",
    "X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized = dataset_creator(dataset_config, path_config, tokenized=True, tensor_type=False, compute_pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists! Loading from .pkl...\n",
      "\n",
      "Dir path ./data/squadv1.1.pkl\n",
      "Computing POS tags for the train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing POS tags for the context...: 100%|██████████| 100/100 [00:03<00:00, 31.40seq/s]\n",
      "Computing POS tags for the question...: 100%|██████████| 100/100 [00:01<00:00, 70.39seq/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing POS tags for the val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing POS tags for the context...: 100%|██████████| 100/100 [00:02<00:00, 36.22seq/s]\n",
      "Computing POS tags for the question...: 100%|██████████| 100/100 [00:01<00:00, 67.39seq/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing POS tags for the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing POS tags for the context...: 100%|██████████| 100/100 [00:02<00:00, 38.61seq/s]\n",
      "Computing POS tags for the question...: 100%|██████████| 100/100 [00:01<00:00, 75.89seq/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ericr\\Documents\\GitHub\\NeuralQuestionGenerationNLP\\src\\data\\data_generator.py:190: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.save(...)` instead.\n",
      "Sentences max lenght: 102\n",
      "Questions max lenght: 27\n"
     ]
    }
   ],
   "source": [
    "dataset_creator = SQuAD()\n",
    "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(dataset_config, path_config, tokenized=True, compute_pos=True)\n",
    "\n",
    "max_length_context = dataset_creator.max_length_context\n",
    "max_length_question = dataset_creator.max_length_question\n",
    "\n",
    "model_config[\"max_length_context\"] = dataset.train.element_spec[0].shape[1]\n",
    "model_config[\"max_length_question\"] = dataset.train.element_spec[1].shape[1]\n",
    "\n",
    "print(f'Sentences max lenght: {max_length_context}')\n",
    "print(f'Questions max lenght: {max_length_question}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sampled_tokens \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mcategorical(\n\u001b[1;32m----> 2\u001b[0m     logits\u001b[39m=\u001b[39mdecoder_logits[:, \u001b[39m0\u001b[39m, :],\n\u001b[0;32m      3\u001b[0m     \u001b[39m# num_samples=1,\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     seed\u001b[39m=\u001b[39mdataset_config[\u001b[39m'\u001b[39m\u001b[39mrandom_seed\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m vocab \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(word_to_idx_question[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m      7\u001b[0m samples \u001b[39m=\u001b[39m sampled_tokens\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder_logits' is not defined"
     ]
    }
   ],
   "source": [
    "from src.models.trainers.trainer import Trainer\n",
    "\n",
    "# Utility function in order to build the compiled model\n",
    "def build_model(model_config,\n",
    "                embedding_matrix_context,\n",
    "                embedding_matrix_question,\n",
    "                compile_info):\n",
    "  print(\"Model Configuration \\nParameters: {}\".format(model_config))\n",
    "  print(\"Compile \\nParameters: {}\".format(compile_info))\n",
    "  model = Trainer(model_config,\n",
    "                  embedding_matrix_context=embedding_matrix_context,\n",
    "                  embedding_matrix_question=embedding_matrix_question)\n",
    "\n",
    "  model.compile(**compile_info)\n",
    "  return model\n",
    "\n",
    "def train_model(model,\n",
    "                dataset: NamedTuple,\n",
    "                training_info):\n",
    "    \"\"\"\n",
    "    Training routine for the Keras model.\n",
    "    At the end of the training, retrieved History data is shown.\n",
    "\n",
    "    :param model: Keras built model\n",
    "    :param dataset: the split dataset\n",
    "    :param training_info: dictionary storing model fit() argument information\n",
    "\n",
    "    :return\n",
    "        model: trained Keras model\n",
    "    \"\"\"\n",
    "    print(\"Start training \\nParameters: {}\".format(training_info))\n",
    "    history = model.fit(dataset.train,\n",
    "                        validation_data=dataset.val,\n",
    "                        use_multiprocessing=True,\n",
    "                        **training_info)\n",
    "    print(\"Training completed\")\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those should go into the configuration file\n",
    "\n",
    "# Initialize the callbacks\n",
    "batch_loss = BatchLogs('batch_loss')\n",
    "perplexity = BatchLogs('perplexity')\n",
    "accuracy = BatchLogs('accuracy')\n",
    "lr_scheduler = CustomLearningRateScheduler()\n",
    "epoch_counter = GetEpochNumber()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_perplexity', \n",
    "                                                  patience=3, \n",
    "                                                  mode='min', \n",
    "                                                  restore_best_weights=True) \n",
    "\n",
    "training_info = {\n",
    "    'verbose': 1,\n",
    "    'epochs': 20,\n",
    "    'batch_size': dataset_config['batch_size'],\n",
    "    'callbacks': [\n",
    "                  batch_loss,\n",
    "                  perplexity,\n",
    "                  accuracy,\n",
    "                  # lr_scheduler,\n",
    "                  # tensorboard_callback,\n",
    "                  # epoch_counter,\n",
    "                  early_stopping\n",
    "                  ],\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'batch_size': dataset_config['batch_size'],\n",
    "    'enc_units': 256,\n",
    "    'dec_units': 256,\n",
    "    'max_length_context': dataset_creator.max_length_context,\n",
    "    'max_length_question': dataset_creator.max_length_question,\n",
    "    'dropout_rate': 0.3,\n",
    "    'regularizer': 1e-2,\n",
    "}\n",
    "\n",
    "compile_info = {\n",
    "    'loss': MaskedLoss(),\n",
    "    'optimizer': keras.optimizers.Adam(learning_rate=1e-5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_model = build_model(model_config, embedding_matrix_context, embedding_matrix_question, compile_info)\n",
    "history, qg_model = train_model(model=qg_model, dataset=dataset, training_info=training_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nqg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c7fd4cba0f632c1713332fdcbac9062cfdb9f4a77b9d9c69a9efa920498cc87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
