{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erhtric/NeuralQuestionGenerationNLP/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Miscellanous"
      ],
      "metadata": {
        "id": "ZS1Y56XayL9_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OU-wpGL18xj"
      },
      "source": [
        "This is the main file: its purpouse is to collect all the code coming from the coding pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Htbwd0W_Y9IC"
      },
      "outputs": [],
      "source": [
        "#!pip install -U tensorflow-addons\n",
        "#!pip install -q \"tensorflow-text==2.8.*\"\n",
        "# !pip install keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvONYDvKAHz",
        "outputId": "79dbf351-c712-4bcb-e965-e2447f48fa70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "import re\n",
        "import os\n",
        "import typing\n",
        "from typing import Any, Tuple, List, NamedTuple\n",
        "import spacy\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "#import seaborn as sns\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from keras.layers import (\n",
        "    Layer, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    Dense, \n",
        "    Bidirectional, \n",
        "    Input, \n",
        "    AdditiveAttention,\n",
        "    Dropout)\n",
        "\n",
        "# import keras_nlp\n",
        "import nltk\n",
        "#from nltk import punkt, pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIZdy1hp14x8",
        "outputId": "5e2d60be-1214-436c-cf97-898c55b311eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this variable if you want to switch between the custom and builtin implementations."
      ],
      "metadata": {
        "id": "kCMJLRkOyIZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_builtins = True"
      ],
      "metadata": {
        "id": "eLQChu7ByIAS"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class is used to check shapes."
      ],
      "metadata": {
        "id": "H3YCIFxxyAwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Shape checker\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "cellView": "code",
        "id": "yDIoL26Kx-Fm"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqKVWPel_ybt"
      },
      "source": [
        "Commands to prepare the folder to accomodate data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKVGy7JPJCoi",
        "outputId": "4d0aa290-f9fa-415d-a4bb-f03643f63320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cVw6eUwM-dRL9BhqtXULyOqeXDrYkwmH/NLP/Project/Testing folder/Eric\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP/Project/Testing folder/Eric\n",
        "%pwd\n",
        "\n",
        "# disable chained assignments to avoid annoying warning\n",
        "pd.options.mode.chained_assignment = None "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ikmZGjUlitqS"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./data'):\n",
        "  print('Data folder does not exists. Creating it')\n",
        "  os.makedirs('./data')\n",
        "\n",
        "if not os.path.exists('./training_checkpoints'):\n",
        "  print('Training checkpoint folder does not exists. Creating it')\n",
        "  os.makedirs('./training_checkpoints')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ePFW3UcrVtr",
        "outputId": "02153d6a-4049-4b0d-9072-4aedbc6fca8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'2.8.2'}\n"
          ]
        }
      ],
      "source": [
        "print({tf.__version__})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hyzKyj_-GjI"
      },
      "source": [
        "This is for the `configuration.json` file, or something similar: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "5bS3uLkE-Mvf"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "units = 600\n",
        "\n",
        "dataset_config = {\n",
        "    # 'num_examples': 18896,\n",
        "    'num_examples': 500,\n",
        "    'num_words_context': 45000,\n",
        "    'num_words_question': 28000,\n",
        "    'buffer_size': 32000,\n",
        "    'batch_size': batch_size,\n",
        "    'random_seed': 13,\n",
        "}\n",
        "\n",
        "encoder_config = {\n",
        "    'context_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_context': None\n",
        "}\n",
        "\n",
        "decoder_config = {\n",
        "    'question_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_question': None,\n",
        "}\n",
        "\n",
        "trainer_config = {\n",
        "    'epochs': 15,\n",
        "    'optimizer': tf.optimizers.Nadam(learning_rate=1.),\n",
        "    # 'optimizer': tf.optimizers.SGD(learning_rate=1.),\n",
        "    'loss': tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "}\n",
        "\n",
        "path = {\n",
        "    'training_json_path': \"./data/training_set.json\",\n",
        "    'save_pkl_path': \"./data/squadv2.pkl\",\n",
        "    'checkpoint_dir': \"./training_checkpoints\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFBKCIE3Jxf2"
      },
      "source": [
        "# 1. Data handling and Pre-processing\n",
        "\n",
        "\n",
        "Things to do:\n",
        "1. Add to each sentence $x$ a start of sequence `<SOS>` tag and end of sequence `<EOS>` tag,\n",
        "2. Clean the sentences by removing special chars,\n",
        "3. Perform other preprocessing steps,\n",
        "4. Create a **vocabulary** with a word-to-index and index-to-word mappings by using a **tokenizer**, \n",
        "5. Extract the sentences that contain an answer and use them as input features, whereas the question will be our target\n",
        "6. Pad each context to maximum length.\n",
        "\n",
        "The resulting data that will be used hereinafter will be of type `tf.data.Dataset`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "XyCKxqwRZelj"
      },
      "outputs": [],
      "source": [
        "class Dataset(NamedTuple):\n",
        "  \"\"\"\n",
        "  This class represent a a 3-way split processed dataset. \n",
        "  \"\"\"\n",
        "  # Reference :- https://github.com/topper-123/Articles/blob/master/New-interesting-data-types-in-Python3.rst\n",
        "  train: tf.data.Dataset\n",
        "  val: tf.data.Dataset\n",
        "  test: tf.data.Dataset\n",
        "\n",
        "class SQuAD:\n",
        "  def __init__(self):\n",
        "    self.random_seed = None\n",
        "    self.squad_df = None\n",
        "    self.preproc_squad_df = None\n",
        "    self.tokenizer = None\n",
        "    self.buffer_size = 0\n",
        "    self.batch_size = 0\n",
        "\n",
        "  def __call__(self,\n",
        "           num_examples, \n",
        "           buffer_size, \n",
        "           batch_size, \n",
        "           random_seed,\n",
        "           training_json_path,\n",
        "           save_pkl_path,\n",
        "           num_words_context=None,\n",
        "           num_words_question=None,\n",
        "           tokenized=True,\n",
        "           pos_ner_tag=True,\n",
        "           tensor_type=True):\n",
        "    \"\"\"The call() method loads the SQuAD dataset, preprocess it and optionally it returns \n",
        "    it tokenized. Moreover it also perform a 3-way split.\n",
        "\n",
        "    Args:\n",
        "        num_examples (int): number of examples to be taken from the original SQuAD dataset\n",
        "        num_words (int): the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. \n",
        "        buffer_size (int): buffer size for the shuffling operation\n",
        "        batch_size (int): size of the batches\n",
        "        tokenized (boolean): specifies if the context and question data should be both tokenized\n",
        "        pos_ner_tag (boolean):\n",
        "        tensro_type (boolean): \n",
        "\n",
        "    Returns (depending on the input parameters):\n",
        "        pd.DataFrame: training dataset\n",
        "        pd.DataFrame: validation dataset\n",
        "        pd.DataFrame: testing dataset\n",
        "          OR\n",
        "        NamedTuple: dataset, (dict, dict, dict)\n",
        "    \"\"\"\n",
        "    self.random_seed = random_seed\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "    self.training_json_path = training_json_path\n",
        "    self.save_pkl_path = save_pkl_path\n",
        "    self.pos_ner_tag = pos_ner_tag\n",
        "    self.max_length_context = 0\n",
        "    self.max_length_question = 0\n",
        "\n",
        "    # Load dataset from file\n",
        "    self.load_dataset(num_examples)\n",
        "    # Extract answer\n",
        "    self.extract_answer()\n",
        "    # Preprocess context and question\n",
        "    self.preprocess()\n",
        "    \n",
        "    # Perform splitting\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.split_train_val(self.preproc_squad_df)\n",
        "    \n",
        "    if self.pos_ner_tag: \n",
        "      pass\n",
        "\n",
        "    # Initialize Tokenizer for the source: in our case the context phrases\n",
        "    # alternatively TextVectorization \n",
        "    self.tokenizer_context = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_context)\n",
        "    # initialize also for the target, namely the question phrases\n",
        "    self.tokenizer_question = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_question)\n",
        "\n",
        "    if tokenized:\n",
        "      X_train_tokenized, word_to_idx_train_context = self.__tokenize_context(X_train)\n",
        "      y_train_tokenized, word_to_idx_train_question = self.__tokenize_question(y_train)\n",
        "\n",
        "      # update the max length for the other splits\n",
        "      self.max_length_context = X_train_tokenized.context.iloc[0].shape[0]\n",
        "      self.max_length_question = y_train_tokenized.iloc[0].shape[0]\n",
        "\n",
        "      X_val_tokenized, word_to_idx_val_context = self.__tokenize_context(X_val)\n",
        "      y_val_tokenized, word_to_idx_val_question = self.__tokenize_question(y_val)\n",
        "\n",
        "      X_test_tokenized, word_to_idx_test_context = self.__tokenize_context(X_test)\n",
        "      y_test_tokenized, word_to_idx_test_question = self.__tokenize_question(y_test)\n",
        "\n",
        "      word_to_idx_context = (word_to_idx_train_context, word_to_idx_val_context, word_to_idx_test_context)\n",
        "      word_to_idx_question = (word_to_idx_train_question, word_to_idx_val_question, word_to_idx_test_question)\n",
        "      \n",
        "      if tensor_type:\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "        # Returns tf.Data.Dataset objects (tokenized)\n",
        "        train_dataset = self.to_tensor(X_train_tokenized, y_train_tokenized)\n",
        "        val_dataset = self.to_tensor(X_val_tokenized, y_val_tokenized)\n",
        "        test_dataset = self.to_tensor(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "        # Configure the dataset for performance\n",
        "        train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        dataset = Dataset(\n",
        "            train=train_dataset, \n",
        "            val=val_dataset,\n",
        "            test=test_dataset)\n",
        "\n",
        "        return dataset, word_to_idx_context, word_to_idx_question\n",
        "      else:\n",
        "        # Returns pd.DataFrame objects (tokenized)\n",
        "        return X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized\n",
        "    else:\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def load_dataset(self, num_examples):\n",
        "    \"\"\"\n",
        "    Extract the dataset from the json file. Already grouped by title.\n",
        "\n",
        "    :param path: [Optional] specifies the local path where the training_set.json file is located\n",
        "\n",
        "    :return\n",
        "        - the extracted dataset in a dataframe format\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.save_pkl_path):\n",
        "      print('File already exists! Loading from .pkl...\\n')\n",
        "      self.squad_df = pd.read_pickle(self.save_pkl_path)\n",
        "      self.squad_df = self.squad_df[:num_examples]\n",
        "    else:\n",
        "      print('Loading from .json...\\n')\n",
        "      with open(self.training_json_path) as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      df_array = []\n",
        "      for current_subject in data['data']:\n",
        "          title = current_subject['title']\n",
        "\n",
        "          for current_context in current_subject['paragraphs']:\n",
        "              context = current_context['context']\n",
        "\n",
        "              for current_question in current_context['qas']:\n",
        "                  question = current_question['question']\n",
        "                  id = current_question['id']\n",
        "\n",
        "              for answer_text in current_question['answers']:\n",
        "                    answer = answer_text['text']\n",
        "                    answer_start = answer_text['answer_start']\n",
        "                    record = { \"id\": id,\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"answer_start\": answer_start,\n",
        "                                \"answer\": answer\n",
        "                                }\n",
        "\n",
        "              df_array.append(record)\n",
        "      \n",
        "      # Save file\n",
        "      pd.to_pickle(pd.DataFrame(df_array), self.save_pkl_path)\n",
        "      self.squad_df = pd.DataFrame(df_array)[:num_examples]\n",
        "\n",
        "  def preprocess(self):\n",
        "    df = self.squad_df.copy()\n",
        "\n",
        "    # Pre-processing context\n",
        "    context = list(df.context)\n",
        "    preproc_context = []\n",
        "\n",
        "    for c in context:\n",
        "      c = self.__preprocess_sentence(c, question=False)\n",
        "      preproc_context.append(c)\n",
        "    \n",
        "    df.context = preproc_context\n",
        "\n",
        "    # Pre-processing questions\n",
        "    question = list(df.question)\n",
        "    preproc_question = []\n",
        "\n",
        "    for q in question:\n",
        "      q = self.__preprocess_sentence(q, question=True)\n",
        "      preproc_question.append(q)\n",
        "    \n",
        "    df.question = preproc_question\n",
        "\n",
        "    # Remove features that are not useful\n",
        "    df = df.drop(['id'], axis=1)\n",
        "    self.preproc_squad_df = df\n",
        "\n",
        "  def __preprocess_sentence(self, sen, question):\n",
        "    # Creating a space between a word and the punctuation following it\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    sen = re.sub(r\"([?.!,¿])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sen = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", sen)\n",
        "\n",
        "    sen = sen.strip()\n",
        "\n",
        "    # Adding a start and an end token to the sentence so that the model know when to \n",
        "    # start and stop predicting.\n",
        "    # if not question: sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    return sen\n",
        "\n",
        "  def __answer_start_end(self, df):\n",
        "    \"\"\"\n",
        "    Creates a list of starting indexes and ending indexes for the answers.\n",
        "\n",
        "    :param df: the target Dataframe\n",
        "\n",
        "    :return: a dataframe containing the start and the end indexes foreach answer (ending index is excluded).\n",
        "\n",
        "    \"\"\"\n",
        "    start_idx = df.answer_start\n",
        "    end_idx = [start + len(list(answer)) for start, answer in zip(list(start_idx), list(df.answer))]\n",
        "    return pd.DataFrame(list(zip(start_idx, end_idx)), columns=['start', 'end'])\n",
        "\n",
        "  def split_train_val(self, df, train_size=0.8):\n",
        "    \"\"\"\n",
        "    This method splits the dataframe in training and test sets, or eventually, in training, validation and test sets.\n",
        "\n",
        "    Args\n",
        "        :param df: the target Dataframe\n",
        "        :param random_seed: random seed used in the splits\n",
        "        :param train_size: represents the absolute number of train samples\n",
        "        :param val: boolean for choosing between a 3-way split or 2-way one.\n",
        "\n",
        "    Returns:\n",
        "        - Data and labels for training, validation and test sets if val is True \n",
        "        - Data and labels for training and test sets if val is False \n",
        "\n",
        "    \"\"\"\n",
        "    # Maybe we have also to return the index for the starting answer\n",
        "    X = df.drop(['answer_start', 'question', 'answer'], axis=1).copy()\n",
        "    idx = self.__answer_start_end(df)\n",
        "    X['start'] = idx['start']\n",
        "    X['end'] = idx['end']\n",
        "    y = df['question']\n",
        "\n",
        "    # In the first step we will split the data in training and remaining dataset\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X, groups=X['title'])\n",
        "    train_idx, rem_idx = next(split)\n",
        "\n",
        "    X_train = X.iloc[train_idx]\n",
        "    y_train = y.iloc[train_idx]\n",
        "    X_rem = X.iloc[rem_idx]\n",
        "    y_rem = y.iloc[rem_idx]\n",
        "\n",
        "\n",
        "    # Val and test test accounts for 10% of the total data. Both 5%.\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X_rem, groups=X_rem['title'])\n",
        "    val_idx, test_idx = next(split)\n",
        "\n",
        "    X_val = X_rem.iloc[val_idx]\n",
        "    y_val = y_rem.iloc[val_idx]\n",
        "\n",
        "    X_test = X_rem.iloc[test_idx]\n",
        "    y_test = y_rem.iloc[test_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def __tokenize_context(self, X):\n",
        "    context = X.context\n",
        "    self.tokenizer_context.fit_on_texts(context)\n",
        "    context_tf = self.tokenizer_context.texts_to_sequences(context)\n",
        "\n",
        "    # context_lengths = [len(seq) for seq in context_tf]\n",
        "    # sns.boxplot(context_lengths)\n",
        "\n",
        "    if self.max_length_context != 0:\n",
        "      context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, maxlen=self.max_length_context, padding='post')\n",
        "    else:\n",
        "      context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, padding='post')\n",
        "\n",
        "\n",
        "    for i, _ in enumerate(context):\n",
        "      X['context'].iloc[i] = context_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_context.word_index['<pad>'] = 0\n",
        "    self.tokenizer_context.index_word[0] = '<pad>'\n",
        "\n",
        "    return X, self.tokenizer_context.word_index\n",
        "\n",
        "  def __tokenize_question(self, y):\n",
        "    question = y\n",
        "    self.tokenizer_question.fit_on_texts(question)\n",
        "    question_tf = self.tokenizer_question.texts_to_sequences(question)\n",
        "\n",
        "    # question_lengths = [len(seq) for seq in question_tf]\n",
        "    # sns.boxplot(question_lengths)\n",
        "    \n",
        "    if self.max_length_question != 0:\n",
        "      question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, maxlen=self.max_length_question, padding='post')\n",
        "    else:\n",
        "      question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(question):\n",
        "      y.iloc[i] = question_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_question.word_index['<pad>'] = 0\n",
        "    self.tokenizer_question.index_word[0] = '<pad>'\n",
        "\n",
        "    return y, self.tokenizer_question.word_index\n",
        "\n",
        "  def extract_answer(self):\n",
        "    df = self.squad_df.copy()\n",
        "    start_end = self.__answer_start_end(df)\n",
        "    context = list(df.context)\n",
        "    \n",
        "    selected_sentences = []\n",
        "    for i, par in enumerate(context):\n",
        "      sentences = sent_tokenize(par)\n",
        "      start = start_end.iloc[i].start\n",
        "      end = start_end.iloc[i].end      \n",
        "      right_sentence = \"\"\n",
        "      context_characters = 0\n",
        "\n",
        "      for j, sen in enumerate(sentences):\n",
        "        sen += ' '\n",
        "        context_characters += len(sen)\n",
        "        # If the answer is completely in the current sentence\n",
        "        if(start < context_characters and end <= context_characters):\n",
        "          right_sentence = sen\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break\n",
        "        # the answer is in both the current and the next sentence\n",
        "        if(start < context_characters and end > context_characters):\n",
        "          right_sentence = sen + sentences[j+1]\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break \n",
        "\n",
        "    self.squad_df.context = selected_sentences\n",
        "\n",
        "  def to_tensor(self, X, y, train=True):\n",
        "    X = X.context.copy()\n",
        "    y = y.copy()\n",
        "\n",
        "    # Reference:- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(list(X), tf.int64), \n",
        "         tf.cast(list(y), tf.int64)))\n",
        "    if train: \n",
        "      dataset = dataset.shuffle(self.buffer_size).batch(self.batch_size, drop_remainder=True)\n",
        "    else:\n",
        "      dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTXVdOGcZSCT"
      },
      "source": [
        "By calling the `SQuAD` constructor we create a dataset handling object which will be useful for future operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "RfCYdZofJ866"
      },
      "outputs": [],
      "source": [
        "dataset_creator = SQuAD()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgeJckqZIufT"
      },
      "source": [
        "## 1.1 Preprocessed untokenized split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "TJFUuu2Y5hc-"
      },
      "outputs": [],
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                                                                       num_words=None,\n",
        "#                                                                       BUFFER_SIZE=32000,\n",
        "#                                                                       BATCH_SIZE=64,\n",
        "#                                                                       random_seed=RANDOM_SEED,\n",
        "#                                                                       tokenized=False)\n",
        "\n",
        "# print(f'Set target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')\n",
        "\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator(**dataset_config, **path, tokenized=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dndR7_CNI1jq"
      },
      "source": [
        "## 1.2 Tokenized split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvU7n1LboA6g"
      },
      "source": [
        "### 1.2.1 Tensor Ready\n",
        "\n",
        "This is the data produced that we are most interested in. As we can see we will have:\n",
        "- a data structure `dataset` containing the training, validation and test set;\n",
        "- a tuple containing the word-to-token mappings for the training, validation and test set respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax999y7aI75Y",
        "outputId": "7df64825-dc51-4513-d292-3c1efda7fada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists! Loading from .pkl...\n",
            "\n",
            "Sentences max lenght: 85\n",
            "Questions max lenght: 26\n",
            "CPU times: user 519 ms, sys: 29.8 ms, total: 549 ms\n",
            "Wall time: 542 ms\n"
          ]
        }
      ],
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "%%time\n",
        "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(**dataset_config, \n",
        "                                                                     training_json_path=path['training_json_path'], \n",
        "                                                                     save_pkl_path=path['save_pkl_path'], \n",
        "                                                                     tokenized=True)\n",
        "\n",
        "max_length_context = dataset.train.element_spec[0].shape[1]\n",
        "max_length_question = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "print(f'Sentences max lenght: {max_length_context}')\n",
        "print(f'Questions max lenght: {max_length_question}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7hARM_R2Kod"
      },
      "source": [
        "Accessing such `NamedTuple` data structure (cfr `dataset`) is pretty simple, namely in a:\n",
        "1. tuple-way by accessing it like a list, e.g. `train = dataset[0]`,\n",
        "2. object-way by calling the instance parameters, e.g. `train = dataset.train`.\n",
        "\n",
        "The other two returned values are the word to index mappings for the context and question words respectively. In order to refer to a specific split simply call:\n",
        "1. for the training dataset,\n",
        "2. for the validation dataset,\n",
        "3. for the test dataset,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaRYgawxyXp",
        "outputId": "a7ef62f8-38cc-44e6-8e00-a3d14c29a649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size for the context: 3839\n",
            "Training vocab size for the question: 1557\n",
            "\n",
            "Validation vocab size for the context: 4071\n",
            "Validation vocab size for the question: 1699\n",
            "\n",
            "Test vocab size for the context: 4353\n",
            "Test vocab size for the question: 1796\n"
          ]
        }
      ],
      "source": [
        "print(f'Training vocab size for the context: {len(word_to_idx_context[0])}')\n",
        "print(f'Training vocab size for the question: {len(word_to_idx_question[0])}')\n",
        "print()\n",
        "print(f'Validation vocab size for the context: {len(word_to_idx_context[1])}')\n",
        "print(f'Validation vocab size for the question: {len(word_to_idx_question[1])}')\n",
        "print()\n",
        "print(f'Test vocab size for the context: {len(word_to_idx_context[2])}')\n",
        "print(f'Test vocab size for the question: {len(word_to_idx_question[2])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlpy-ayWoEHa"
      },
      "source": [
        "### 1.2.2 Standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "vJFBk-r8eIcj"
      },
      "outputs": [],
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                      BUFFER_SIZE=32000,\n",
        "#                      BATCH_SIZE=64,\n",
        "#                      random_seed=RANDOM_SEED,\n",
        "#                      tokenized=True,\n",
        "#                      tensor_type=False)\n",
        "\n",
        "# print(f'\\nSet target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qxjGzKJM2w"
      },
      "source": [
        "## 1.3 Original SQuAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "9qhTAc5NErOk"
      },
      "outputs": [],
      "source": [
        "# Original dataset\n",
        "# squad_df = dataset_creator.squad_df\n",
        "# print(f'[Info] SQuAD target: {list(squad_df.columns.values)}')\n",
        "# print(f'[Info] Shape: {squad_df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2f7Nn_4en9"
      },
      "source": [
        "# 2. GloVe and embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "TRJ1NpSMqaJL"
      },
      "outputs": [],
      "source": [
        "class GloVe:\n",
        "  def __init__(self, embedding_dimension):\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "\n",
        "    try:\n",
        "      self.embedding_model = KeyedVectors.load(f'./data/glove_model_{self.embedding_dimension}')\n",
        "    except FileNotFoundError:\n",
        "      print('[Warning] Model not found in local folder, please wait...')\n",
        "      self.embedding_model = self.load_glove()\n",
        "      self.embedding_model.save(f'./data/glove_model_{self.embedding_dimension}')  \n",
        "      print('Download finished. Model loaded!')\n",
        "\n",
        "  def load_glove(self):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained GloVe embedding model via gensim library.\n",
        "\n",
        "    We have a matrix that associate words to a vector of a user-defined dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(self.embedding_dimension)\n",
        "\n",
        "    try:\n",
        "      emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "      print(\"Generic error when loading GloVe\")\n",
        "      print(\"Check embedding dimension\")\n",
        "      raise e\n",
        "\n",
        "    emb_model = gloader.load(download_path)\n",
        "    return emb_model\n",
        "\n",
        "  def build_embedding_matrix(self, word_to_idx, vocab_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the \n",
        "        dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, self.embedding_dimension), dtype=np.float32)\n",
        "    oov_count = 0\n",
        "    oov_words = []\n",
        "\n",
        "    # For each word which is not present in the vocabulary we assign a random vector, otherwise we take the GloVe embedding\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      try:\n",
        "        embedding_vector = self.embedding_model[word]\n",
        "      except (KeyError, TypeError):\n",
        "        oov_count += 1\n",
        "        oov_words.append(word)\n",
        "        embedding_vector = np.random.uniform(low=-0.5, \n",
        "                                             high=0.5, \n",
        "                                             size=self.embedding_dimension)\n",
        "\n",
        "      embedding_matrix[idx] = embedding_vector\n",
        "    \n",
        "    print(f'\\n[Debug] {oov_count} OOV words found!\\n')\n",
        "    return embedding_matrix, oov_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk-z8A5y3cpI"
      },
      "source": [
        "The next step is to initialize the handler with the desidered `embedding_dimension`. Then to build the embedding matrix with the pre-trained GloVe embeddings simply call the `build_embedding_matrix` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUmvdCWGavSR",
        "outputId": "06375f18-e5f0-468c-ae16-39491778735b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4071/4071 [00:00<00:00, 220118.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 107 OOV words found!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1699/1699 [00:00<00:00, 156552.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 43 OOV words found!\n",
            "\n",
            "CPU times: user 814 ms, sys: 473 ms, total: 1.29 s\n",
            "Wall time: 1.62 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Initalize the handler for GloVe\n",
        "glove_handler = GloVe(encoder_config['embedding_dimension'])\n",
        "\n",
        "# We will create the matrix by using only the words present in the training and validation set\n",
        "embedding_matrix_context, oov_words_context = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_context[1], \n",
        "    len(word_to_idx_context[1])+1)\n",
        "\n",
        "embedding_matrix_question, oov_words_question = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_question[1], \n",
        "    len(word_to_idx_question[1])+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdLOk0pQu3Iu"
      },
      "source": [
        "Convert both of them into tensor, but it is fine to also treat them as `numpy` array, still it is better to use the `tensorflow` fundamentals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "EX6PvKTBsdSW"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_context = tf.convert_to_tensor(embedding_matrix_context)\n",
        "embedding_matrix_question = tf.convert_to_tensor(embedding_matrix_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF5Rtd4uqa_k"
      },
      "source": [
        "# 3. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "GjdRysIvPoLc"
      },
      "outputs": [],
      "source": [
        "example_context_batch, example_question_batch = next(iter(dataset.train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjVfZgIIf1RV"
      },
      "source": [
        "## 3.1 Encoder\n",
        "We will use a bidirectional LSTM to encode the sentence,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\overrightarrow{b_t} &= \\overrightarrow{\\text{LSTM}}(x_t, \\overrightarrow{b_{t-1}})\\\\\n",
        "\\overleftarrow{b_t} &= \\overleftarrow{\\text{LSTM}}(x_t, \\overleftarrow{b_{t+1}})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\overrightarrow{b_t}$ is the hidden state at time step $t$ for the forward pass LSTM and $\\overleftarrow{b_t}$ for the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "GK6Kd1XvqK22"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               context_vocab_size, \n",
        "               embedding_matrix,\n",
        "               embedding_dimension, \n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_context,\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.batch_size = batch_size\n",
        "    self.max_length_context = max_length_context\n",
        "\n",
        "    # Input node\n",
        "    self.input_layer = Input(shape=(self.max_length_context,), \n",
        "                             batch_size = batch_size, \n",
        "                             dtype=tf.int32)\n",
        "    \n",
        "    self.embedding = Embedding(input_dim=context_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=max_length_context,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,\n",
        "                               mask_zero=True,\n",
        "                               name='encoder_embedding_layer') \n",
        "    \n",
        "    self.bi_lstm = Bidirectional(LSTM(units//2, return_sequences=True, return_state=True), \n",
        "                                  name='encoder_bi_lstm',\n",
        "                                  merge_mode='concat')\n",
        "\n",
        "    # self.dropout = Dropout(.3)\n",
        "\n",
        "    self.concatenate = tf.keras.layers.Concatenate(axis=1) \n",
        "\n",
        "    self.output_layer = self.call(self.input_layer)\n",
        "\n",
        "  def call(self, inputs, state=None, training=True):\n",
        "    # 1. The input is a tokenized and padded sentence containing the answer from the context\n",
        "    \n",
        "    # 2. The embedding layer looks up for the embedding for each token\n",
        "    vectors = self.embedding(inputs)\n",
        "\n",
        "    # 3. The Bi-LSTM processes the embedding sequence forward and backward:\n",
        "    #     output shape: ('batch', 'max_length_context', 'units')\n",
        "    #     hidden state shape: fw ('batch', 'units//2'), bw ('batch', 'units//2')\n",
        "    #     cell state shape: fw ('batch', 'units//2'), bw ('batch', 'units//2')\n",
        "    output, forward_h, forward_c, backward_h, backward_c = self.bi_lstm(vectors, initial_state=state, training=training)\n",
        "\n",
        "    # 4. Concatenate the forward and the backward states\n",
        "    h = self.concatenate([forward_h, backward_h])\n",
        "    c = self.concatenate([forward_c, backward_c])\n",
        "    encoder_state = [h, c]\n",
        "\n",
        "    # 5. Return the new sequence processed by the encoder and its state\n",
        "    return output, encoder_state\n",
        "\n",
        "  # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_model(self):\n",
        "    x = Input(shape=(self.max_length_context,), batch_size=self.batch_size)\n",
        "    return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_model(), \n",
        "        # to_file='encoder.png', dpi=96,              \n",
        "        show_shapes=True, show_layer_names=True,  \n",
        "        expand_nested=True                       \n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbjSxPGcFud_"
      },
      "source": [
        "### 3.1.1 Test the encoder stack\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ffteDMQyzmD",
        "outputId": "443f7c54-d755-441e-809a-da862bee361c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch_size, max_length_context, units): (16, 85, 600)\n",
            "Hidden state shape: (batch_size, units): (16, 600)\n",
            "Cell state shape: (batch_size, units): (16, 600)\n"
          ]
        }
      ],
      "source": [
        "encoder_config['context_vocab_size'] = len(word_to_idx_context[1])\n",
        "encoder_config['max_length_context'] = dataset.train.element_spec[0].shape[1]\n",
        "\n",
        "encoder = Encoder(**encoder_config, embedding_matrix=embedding_matrix_context)\n",
        "encoder_outputs, encoder_state = encoder(inputs=example_context_batch)\n",
        "\n",
        "hidden_state, cell_state = encoder_state\n",
        "\n",
        "print(f'Encoder output shape: (batch_size, max_length_context, units): {encoder_outputs.shape}')\n",
        "print(f'Hidden state shape: (batch_size, units): {hidden_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, units): {cell_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng97TFpLRym2",
        "outputId": "204cdcee-0d5d-4bf5-f62d-7c6a027a8d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_embedding_layer (Em  (16, 85, 300)            1221600   \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " encoder_bi_lstm (Bidirectio  [(16, 85, 600),          1442400   \n",
            " nal)                         (16, 300),                         \n",
            "                              (16, 300),                         \n",
            "                              (16, 300),                         \n",
            "                              (16, 300)]                         \n",
            "                                                                 \n",
            " concatenate_13 (Concatenate  (16, 600)                0         \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,664,000\n",
            "Trainable params: 1,442,400\n",
            "Non-trainable params: 1,221,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "aaJFf-ELdIZo",
        "outputId": "3591943d-af9d-4c81-a042-9aee18804abf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "encoder.plot_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Attention Head"
      ],
      "metadata": {
        "id": "fauWdCYGMmUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(Layer):\n",
        "  # Still it is possible to use Luang's attention as an alternative\n",
        "  # Reference:- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.W1 = Dense(units, use_bias=False, name='Wb1_attention_weights')\n",
        "    self.W2 = Dense(units, use_bias=False, name='Wb2_attention_weights')\n",
        "\n",
        "    self.attention = AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    \"\"\"\n",
        "    This layer takes 3 inputs:\n",
        "      - the query; this will be generated by the decoder, later,\n",
        "      - the value: the output of the encoder,\n",
        "      - the mask: to exclude the padding, i.e., context_batch != 0.\n",
        "    \"\"\"\n",
        "    #W1@ht\n",
        "    w1_query = self.W1(query)\n",
        "    #W2@hs\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask = [query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    \n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "qPPqdUTsMqyM"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer takes 3 inputs:\n",
        "- the `query`: this will be generated by the decoder, later.\n",
        "- the `value`: this will be the output of the encoder.\n",
        "- the `mask`: to exclude the padding.\n",
        "\n",
        "The attention results will be of shape:\n",
        "1. *context vector* shape: `(batch, query_seq_length, units)`,\n",
        "2. *attention weights* shape: `(batch, query_seq_length, value_seq_length)`. They should sum to `1.0` for each sequence `t`."
      ],
      "metadata": {
        "id": "e3O7TCSnM-7A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtM9nOQrf3jq"
      },
      "source": [
        "## 3.3 Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "73wq7CTiTQpX"
      },
      "outputs": [],
      "source": [
        "# Container classes\n",
        "# Reference :- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "class DecoderInput(NamedTuple):\n",
        "  new_token: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "V_-Lef2CqUW2"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               question_vocab_size, \n",
        "               embedding_matrix, \n",
        "               embedding_dimension,\n",
        "               units, \n",
        "               batch_size,\n",
        "               max_length_question,\n",
        "               **kwargs):\n",
        "    \n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "    self.units = units\n",
        "\n",
        "    self.input_layer = Input(shape=(None,), batch_size=batch_size)\n",
        "                        \n",
        "    # Embedding for the questions\n",
        "    self.embedding = Embedding(input_dim=question_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                              #  input_length=self.max_length_question,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,  #?\n",
        "                               mask_zero=True,\n",
        "                               name='decoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM layer\n",
        "    self.lstm_layer = LSTM(units, \n",
        "                           return_sequences=True,\n",
        "                           return_state=True,\n",
        "                           name='decoder_lstm_layer')\n",
        "    \n",
        "    self.attention = BahdanauAttention(units)\n",
        "\n",
        "    self.Wt = Dense(units, activation=tf.math.tanh, use_bias=False, name='decoder_Wt_weights')\n",
        "\n",
        "    # For the word probabilities\n",
        "    # self.Ws = Dense(question_vocab_size, activation=tf.nn.softmax, use_bias=False, name='decoder_Ws_weights')\n",
        "    self.Ws = Dense(question_vocab_size, name='logits')\n",
        "\n",
        "    self.concatenate = tf.keras.layers.Concatenate(axis=-1)\n",
        "    # self.dropout_layer = Dropout(.3)\n",
        "\n",
        "  def call(self, \n",
        "           inputs: DecoderInput, \n",
        "           state=None,\n",
        "           training=True) -> Tuple[DecoderOutput, Tuple[tf.Tensor]]:\n",
        "\n",
        "    # See issue with nested structures: https://github.com/tensorflow/tensorflow/issues/37061\n",
        "    # self.input_layer = tf.nest.map_structure(lambda x: tf.keras.layers.Input(tf.shape(x)[1:]), inputs)\n",
        "    # inputs = tf.nest.map_structure(lambda x: tf.keras.layers.Input(tf.shape(x)[1:]), inputs)\n",
        "\n",
        "    # 2. The embedding layer looks up for the embedding for each token\n",
        "    # vectors shape: (batch_size, 1, embedding_dimension)\n",
        "    vectors = self.embedding(inputs.new_token)\n",
        "    if tf.shape(vectors).shape == 2: vectors = tf.expand_dims(vectors, axis=1)\n",
        "\n",
        "    # 3. Process one step with the LSTM\n",
        "    # LSTM expects inputs of shape: (batch_size, timestep, feature)\n",
        "    output, h, c = self.lstm_layer(vectors, initial_state=state)\n",
        "\n",
        "    # cell_output = self.dropout_layer(cell_output, training=training)\n",
        "    # cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(cell_output, initial_state=(hidden_dec_state, cell_dec_state), training=training)\n",
        "    \n",
        "    # 4. Use the LSTM cell output as the query for the attention over the encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=output, \n",
        "        value=inputs.enc_output, \n",
        "        mask=inputs.mask)\n",
        "\n",
        "    # 5. Join the context_vector and cell output [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    output_and_context_vector = self.concatenate([context_vector, output])\n",
        "\n",
        "    # at = tanh(Wt@[ht, ct])\n",
        "    attention_vector = self.Wt(output_and_context_vector)\n",
        "\n",
        "    # logits = softmax(Ws@at), it produces unscaled logits\n",
        "    logits = self.Ws(attention_vector)\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), (h, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF5J42g1l-be"
      },
      "source": [
        "### 3.3.1 Test the decoder stack\n",
        "\n",
        "The decoder will take as input:\n",
        "1. `new_tokens`: the last token generated of shape `(batch_size, 1)`, namely the token obrained in the previous time step of the decoder (we will initialize the decoder with the `\"<sos>\"` token);\n",
        "2. `enc_output`: this is the representation produced by the `Encoder` of shape `(batch_size, max_length_context, enc_units)`;\n",
        "3. `mask`: this is the mask, that is a boolean tensor, indicating which tokens will be considered in the decoding of shape `(batch_size, max_length_context)`; \n",
        "4. `decoder_state`: the previous state of the decoder, namely the internal state of the decoder's LSTM (the paper suggests to input the hidden and cell state produced by the Bi-LSTM). The shape is `[(batch_size, enc_units), (batch_size, enc_units)]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "kS0UBnMzTbie"
      },
      "outputs": [],
      "source": [
        "decoder_config['question_vocab_size'] = len(word_to_idx_question[1])\n",
        "decoder_config['max_length_question'] = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "decoder = Decoder(**decoder_config, embedding_matrix=embedding_matrix_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "KeMvqDnrTkf0"
      },
      "outputs": [],
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "start_tag_index = word_to_idx_question[2]['<sos>']\n",
        "first_token = tf.constant([[start_tag_index]] * decoder_config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF6PWsNYfmUf",
        "outputId": "1b7f0bda-bdbc-42ff-82d8-98cb1ef65f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (batch_size, t, output_vocab_size) (16, 1, 1699)\n",
            "Attention weights shape: (batch_size, t, max_length_context) (16, 1, 85)\n",
            "Hidden state shape: (batch_size, dec_units) (16, 600)\n",
            "Cell state shape: (batch_size, dec_units) (16, 600)\n"
          ]
        }
      ],
      "source": [
        "decoder_result, decoder_state = decoder(\n",
        "    inputs = DecoderInput(first_token, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = encoder_state\n",
        ")\n",
        "\n",
        "hidden_dec_state, cell_dec_state = decoder_state\n",
        "\n",
        "print(f'Logits shape: (batch_size, t, output_vocab_size) {decoder_result.logits.shape}')\n",
        "print(f'Attention weights shape: (batch_size, t, max_length_context) {decoder_result.attention_weights.shape}')\n",
        "print(f'Hidden state shape: (batch_size, dec_units) {hidden_dec_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, dec_units) {cell_dec_state.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9FZVz2QyLkw"
      },
      "source": [
        "In this case we cannot provide a detailed summary or a handy plot due to the fact that we pass to the decoder model a structured input which is not preferred by tensorflow. Still, if you are curious you can uncomment the line:\n",
        "\n",
        "`inputs = tf.nest.map_structure(lambda x: tf.keras.layers.Input(tf.shape(x)[1:]), inputs)`\n",
        "\n",
        "NB: this will cause issues with the tensorflow graph execution and it should be only allowed to check the shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AJWwcDacEj",
        "outputId": "1a80810a-91a1-46a4-a437-3e6bcb9dc362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_embedding_layer (Em  multiple                 510000    \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " decoder_lstm_layer (LSTM)   multiple                  2162400   \n",
            "                                                                 \n",
            " bahdanau_attention_11 (Bahd  multiple                 720600    \n",
            " anauAttention)                                                  \n",
            "                                                                 \n",
            " decoder_Wt_weights (Dense)  multiple                  720000    \n",
            "                                                                 \n",
            " logits (Dense)              multiple                  1021099   \n",
            "                                                                 \n",
            " concatenate_14 (Concatenate  multiple                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,134,099\n",
            "Trainable params: 4,624,099\n",
            "Non-trainable params: 510,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBIQDE0Sl6k8"
      },
      "source": [
        "Moving on: this means that the decoder will produce a vector of unnormalized log probabilities (**logits**) associated to each vocabulary word. That is, a vector of logits $l_b \\in \\mathbb{R}^{\\mathcal{V}}$ for each element $b$ in the batch, namely indicating the next probable token for a given sentence. \n",
        "\n",
        "Now we sample a token according to the logits computed by the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGGwivobvx_W",
        "outputId": "916d39ad-4b82-405f-a0e0-59303af79eb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['featuring', 'position', 'october', 'princess', 'increase']"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :],\n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "vocab = np.array(list(word_to_idx_question[1].keys()))\n",
        "\n",
        "first_word = list(vocab[tf.squeeze(sampled_tokens, axis=-1).numpy()])\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2ixRaJZn271",
        "outputId": "1b083407-4cc9-4f24-f97d-93d2e3a14127"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['effects', 'piece', 'recording', 'society', 'describe']"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ],
      "source": [
        "decoder_result, _ = decoder(\n",
        "    inputs = DecoderInput(sampled_tokens, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = decoder_state\n",
        ")\n",
        "\n",
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :], \n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "sampled_tokens = tf.squeeze(sampled_tokens, axis=-1).numpy()\n",
        "\n",
        "first_word = list(vocab[sampled_tokens])\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the weights."
      ],
      "metadata": {
        "id": "U8xwVgulXUab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(decoder_result.attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_context_batch != 0)\n",
        "plt.title('Mask')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "JtJrZ7UeWac-",
        "outputId": "4e5dad2d-131a-4d82-b78e-d34529cd422c"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the weights for only one sequence."
      ],
      "metadata": {
        "id": "wjc7zQkDY19Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_slice = decoder_result.attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]\n",
        "\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "XwTgLXw0YjQ1",
        "outputId": "69f573b6-f9af-46bf-be48-5871bb97af85"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIoySQKuIGlc"
      },
      "source": [
        "# 4. Training for QG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6uDOVpginU0"
      },
      "source": [
        "## 4.1 Training checkpoints\n",
        "\n",
        "See [Manual Checkpointing](https://www.tensorflow.org/guide/checkpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "5tNJR_fRi7qd"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(path['checkpoint_dir'], \"tf_ckpt\")\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_iterator = iter(dataset.train) \n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "manager = tf.train.CheckpointManager(checkpoint,\n",
        "                                     checkpoint_prefix,\n",
        "                                     max_to_keep=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyRA2RxZNsx4"
      },
      "source": [
        "## 4.2 Loss\n",
        "\n",
        "The **QG** task is defined as finding $\\hat{y}$ such that:\n",
        "$$\n",
        "\\hat{y} = \\arg{\\max_y P(y|x)}  \n",
        "$$\n",
        "where $P(y|x)$ is the conditional log-likelihood of the predicted question sentence $y$ given the input $x$. Du et al. shown that the conditional probability could be factorized in:\n",
        "$$\n",
        "P(y|x) = \\prod_{t=1}^{|y|} P(y_t|x, y_{<t})\n",
        "$$\n",
        "where the probability of each $y_t$ is predicted based on all the words that have been generated upon time $t$, namely $y_{<t}$.\n",
        "\n",
        "This means that given a training corpus of sentence-question pairs $\\mathcal{S} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the objective is to minimize the negative log-likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L} &= - \\sum_{i=1}^N \\log P(y^{(i)}|x^{(i)}; \\theta)\\\\\n",
        "            &=  - \\sum_{i=1}^N \\sum_{j=1}^{|y^{(i)}|} \\log P (y_j^{(i)}|x^{(i)}, y_{<j}^{(i)}; \\theta)\n",
        "\\end{align*}\n",
        "$$\n",
        "We parameterize the probability of decoding each word $y_j$ by using an RNN:\n",
        "$$\n",
        "P(y_j|y_{<j}, s) = \\text{softmax}(g(h_j))\n",
        "$$\n",
        "where $g(.)$ is a transition function that outputs a vocabulary-sized vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "IP_UunM3MUtF"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'conditional_ll_loss'\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch\n",
        "    # Shape of labels = (batch_size, )\n",
        "    # Shape of logits = (batch_size, 1, vocab_size)\n",
        "    y_pred = tf.squeeze(y_pred, axis=1)\n",
        "\n",
        "    # We will use this loss function since we are working with logits\n",
        "    # Reference :- https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\n",
        "    # loss shape (batch_size, 1)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
        "    \n",
        "    # Mask of the losses on the padding\n",
        "    mask = tf.math.not_equal(y_true, 0)\n",
        "    loss = tf.boolean_mask(loss, mask)\n",
        "\n",
        "    # Sum over all the predictions in the batch\n",
        "    loss = tf.reduce_sum(loss)\n",
        "\n",
        "    # Return the total\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOVZj966EPP"
      },
      "source": [
        "## 4.3 Metrics\n",
        "\n",
        "The metrics used during training will be:\n",
        "1. perplexity,\n",
        "2. masked accuracy,\n",
        "3. masked f1 score.\n",
        "\n",
        "They will be implemented by exploiting the `Metric` object in `tensorflow`. See [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "xbK72c5r6LWP"
      },
      "outputs": [],
      "source": [
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='perplexity', **kwargs):\n",
        "    super(Perplexity, self).__init__(name=name, **kwargs)\n",
        "    self.scores = self.add_weight(name='perplexity_scores', initializer='zeros')\n",
        "\n",
        "  def update_state(self, loss):\n",
        "    \"\"\"\n",
        "    Reference :- https://www.surgehq.ai/blog/how-good-is-your-chatbot-an-introduction-to-perplexity-in-nlp\n",
        "    \"\"\"\n",
        "    self.scores.assign(tf.exp(loss))\n",
        "\n",
        "  def result(self): return self.scores\n",
        "  def reset_state(self): self.scores.assign(0)\n",
        "\n",
        "# Also the accuracy should mask the padding\n",
        "class MaskedAccuracy(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='masked_accuracy',**kwargs):\n",
        "    super(MaskedAccuracy, self).__init__(name=name, **kwargs)\n",
        "    self.scores = self.add_weight(name='accuracy_scores', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # We mask since we are not interested in the final accuracy score\n",
        "    mask = tf.cast(tf.math.greater(y_true, 0), dtype=tf.float32)\n",
        "\n",
        "    correct = tf.cast(tf.math.equal(y_true, y_pred), dtype=tf.float32)\n",
        "    correct = tf.math.reduce_sum(mask * correct)\n",
        "    total_legit = tf.math.reduce_sum(mask)\n",
        "\n",
        "    self.scores.assign(correct / total_legit)\n",
        "\n",
        "  def result(self): return self.scores\n",
        "  def reset_state(self): self.scores.assign(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLiPsCyNuor"
      },
      "source": [
        "## 4.3 QG model and training step implementation\n",
        "\n",
        "The training step should:\n",
        "1. Run the encoder on the `input_tokens` to get the `encoder_outputs`, `hidden_state` and `cell_state`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "cellView": "code",
        "id": "8xSn_StMq9cx"
      },
      "outputs": [],
      "source": [
        "class QGeneratorTrainer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               context_vocab_size,\n",
        "               question_vocab_size,\n",
        "               embedding_dimension,\n",
        "               embedding_matrix_context,\n",
        "               embedding_matrix_question,\n",
        "               units,\n",
        "               batch_size,\n",
        "               max_length_context,\n",
        "               max_length_question,\n",
        "               use_tf_function=True,\n",
        "               **kwargs):\n",
        "    \"\"\"\n",
        "    Prepare the model for the training. It builds the both the encoder and the decoder.\n",
        "    Also it defines a wrapper to use the tf.function compilation for the tensorflow computational\n",
        "    graph.\n",
        "    \"\"\"\n",
        "    super(QGeneratorTrainer, self).__init__(**kwargs)\n",
        "    self.encoder = Encoder(\n",
        "        context_vocab_size,\n",
        "        embedding_matrix_context,\n",
        "        embedding_dimension,\n",
        "        units,\n",
        "        batch_size,\n",
        "        max_length_context)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        question_vocab_size,\n",
        "        embedding_matrix_question,\n",
        "        embedding_dimension,\n",
        "        units, \n",
        "        batch_size,\n",
        "        max_length_question)\n",
        "\n",
        "    self.max_length_question = max_length_question\n",
        "    self.batch_size = batch_size\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "    self.perplexity_metric = Perplexity()\n",
        "    self.accuracy_metric = MaskedAccuracy()\n",
        "\n",
        "    self.loss_tracker = []\n",
        "\n",
        "  def call(self, inputs, training=True, masks=None):\n",
        "    \"\"\"\n",
        "    It performs a forward pass. Calls the model on new inputs and returns the outputs as tensors.\n",
        "    \"\"\"\n",
        "    context = inputs[0]\n",
        "    question = inputs[1]\n",
        "\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    # Encode the input\n",
        "    encoder_output, encoder_state = self.encoder(context, training=True)\n",
        "\n",
        "    # The decoder should be initialized with the encoder last state \n",
        "    decoder_state = encoder_state \n",
        "\n",
        "    # We collect the question predicted by the decoder, the first character is the starting token\n",
        "    pred_question = tf.fill([self.batch_size, 1], question[0][0])\n",
        "    \n",
        "    # Keep a loss tracking value\n",
        "    loss = tf.constant(0.0)\n",
        "    t = 0\n",
        "\n",
        "    # Reference :- https://www.tensorflow.org/guide/function\n",
        "    # We have to run the decoder for all the length of the question \n",
        "    while t < (self.max_length_question - 1):\n",
        "      # We have to pass two tokens:\n",
        "      #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "      #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "      new_token = tf.gather(question, t, axis=1)\n",
        "      target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "      # Here we call the decoder in order to produce the token at time step t+1, it returns,\n",
        "      #   1. the partial loss for the predicted token,\n",
        "      #   2. the new decoder state,\n",
        "      #   3. the predicted token at time step t+1\n",
        "      step_loss, decoder_state, pred_token = self.step_decoder(\n",
        "          (new_token, target_token),\n",
        "          context_mask,\n",
        "          encoder_output,\n",
        "          decoder_state, \n",
        "          training=True)\n",
        "\n",
        "      pred_question = tf.concat([pred_question, pred_token], axis=1)\n",
        "\n",
        "      loss = loss + step_loss\n",
        "      t = t + 1\n",
        "\n",
        "    return pred_question, loss\n",
        "\n",
        "  @tf.function\n",
        "  def step_decoder(self, \n",
        "                   tokens,\n",
        "                   context_mask,\n",
        "                   encoder_output,\n",
        "                   decoder_state,\n",
        "                   training):\n",
        "    \"\"\"\n",
        "    Run a single iteration of the decoder and computers the incremental loss between the\n",
        "    produced token and the token in the target input.\n",
        "    \"\"\"\n",
        "    new_token, target_token = tokens\n",
        "    \n",
        "    # Run the decoder one time\n",
        "    decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=context_mask),\n",
        "        state = decoder_state, \n",
        "        training = training)\n",
        "  \n",
        "    y_true = target_token\n",
        "    y_pred = decoder_result.logits\n",
        "\n",
        "    step_loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    y_pred = tf.cast(tf.math.argmax(y_pred, axis=-1), dtype=tf.int64)\n",
        "    return step_loss, decoder_state, y_pred\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    # We list our `Metric` objects here so that `reset_states()` can be called \n",
        "    # automatically at the start of each epoch or at the start of `evaluate()`.\n",
        "    # If you don't implement this property, you have to call # `reset_states()` \n",
        "    # yourself at the time of your choosing.\n",
        "    return [self.perplexity_metric, self.accuracy_metric]\n",
        "\n",
        "  def _train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Optimization step for a batch.\n",
        "    \"\"\"\n",
        "    context, question = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      pred_question, loss = self(inputs, training=True)\n",
        "\n",
        "      # Average the loss for all the legit tokens\n",
        "      # avg_loss = loss / tf.math.reduce_sum(tf.cast(self.__get_mask(question), dtype=loss.dtype))\n",
        "\n",
        "      avg_loss = tf.math.divide_no_nan(loss, tf.math.reduce_sum(tf.cast(self.__get_mask(question), dtype=loss.dtype)))\n",
        "      self.loss_tracker.append(avg_loss)\n",
        "\n",
        "    # Compute gradients\n",
        "    tr_variables = self.trainable_variables\n",
        "    grads = tape.gradient(avg_loss, tr_variables)\n",
        "\n",
        "    # Apply some clipping (by norm) as done in the paper and update the weights\n",
        "    # grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
        "    # grads = [tf.math.pow(g, 20) for g in grads]\n",
        "    self.optimizer.apply_gradients(zip(grads, tr_variables))\n",
        "\n",
        "    # Compute metrics\n",
        "    self.perplexity_metric.update_state(avg_loss)\n",
        "    self.accuracy_metric.update_state(question, pred_question)\n",
        "\n",
        "    return {'batch_loss': avg_loss, 'perplexity': self.perplexity_metric.result(), 'accuracy': self.accuracy_metric.result()}\n",
        "    \n",
        "  def __get_mask(self, tokens):\n",
        "    \"\"\"\n",
        "    Generate a boolean mask for those elements which are not <pad>.\n",
        "    \"\"\" \n",
        "    return tf.math.not_equal(tokens, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "zwKc0rvrIWkg"
      },
      "outputs": [],
      "source": [
        "train_model = QGeneratorTrainer(**encoder_config,\n",
        "                             question_vocab_size=decoder_config['question_vocab_size'],\n",
        "                             max_length_question=decoder_config['max_length_question'],\n",
        "                             embedding_matrix_context=embedding_matrix_context,\n",
        "                             embedding_matrix_question=embedding_matrix_question,\n",
        "                             use_tf_function=False)\n",
        "\n",
        "# We do not pass any metric here since they are already in the model \n",
        "# Reference :- https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
        "train_model.compile(\n",
        "    # optimizer=trainer_config['optimizer'],\n",
        "    # optimizer=tf.optimizers.Nadam(learning_rate=1e-1),\n",
        "    optimizer=tf.optimizers.Adam(1e-2),\n",
        "    loss=MaskedLoss(),\n",
        "    # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPSDqU3_3nOg"
      },
      "source": [
        "### 4.3.2 Simple Training\n",
        "The first call with `use_tf_function=True` will be slow since it has to trace the function. So be patient or try `use_tf_function=False` 😀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf5JzL9T47Ha",
        "outputId": "0880ed6d-0000-43c5-f8cd-8eea79d59df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.00055817 0.00064728 0.00051111 ... 0.00054398 0.00060229 0.0005977 ], shape=(1699,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.06530612>,\n",
              " 'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.963035>,\n",
              " 'perplexity': <tf.Tensor: shape=(), dtype=float32, numpy=1056.8363>}"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ],
      "source": [
        "batch = next(iter(dataset.train))\n",
        "train_model.train_step(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0imUvBAg14Ep"
      },
      "source": [
        "## 4.4 Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8BfykX5faH"
      },
      "source": [
        "## 4.5 Train the model\n",
        "\n",
        "First we define some useful callbacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "uhSLln405mfy"
      },
      "outputs": [],
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key) -> None:\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_ends(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 8:\n",
        "    return lr\n",
        "  elif epoch == 8:\n",
        "    return lr / 2\n",
        "  else: \n",
        "    return lr\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss as tf.Variable? \n",
        "Instead of using masking we compute it also on padding"
      ],
      "metadata": {
        "id": "CuKbg9yAR50K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys97OwVn61UT",
        "outputId": "002a6d9a-f1b6-48ba-d70c-7257e48b5399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method QGeneratorTrainer.call of <__main__.QGeneratorTrainer object at 0x7f60f8db2850>> and will run it as-is.\n",
            "Cause: mangled names are not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method QGeneratorTrainer.call of <__main__.QGeneratorTrainer object at 0x7f60f8db2850>> and will run it as-is.\n",
            "Cause: mangled names are not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Tensor(\"AddN_11:0\", shape=(1699,), dtype=float32)\n",
            "26/26 [==============================] - 16s 339ms/step - batch_loss: 7.9912 - perplexity: 3376.5024 - accuracy: 0.1321 - lr: 0.0100\n",
            "Epoch 2/15\n",
            "26/26 [==============================] - 9s 337ms/step - batch_loss: 7.5302 - perplexity: 980.3446 - accuracy: 0.2120 - lr: 0.0100\n",
            "Epoch 3/15\n",
            "26/26 [==============================] - 9s 338ms/step - batch_loss: 7.0001 - perplexity: 1133.7777 - accuracy: 0.2363 - lr: 0.0100\n",
            "Epoch 4/15\n",
            "26/26 [==============================] - 9s 336ms/step - batch_loss: 6.5766 - perplexity: 686.4571 - accuracy: 0.2422 - lr: 0.0100\n",
            "Epoch 5/15\n",
            "26/26 [==============================] - 9s 335ms/step - batch_loss: 6.5452 - perplexity: 709.2155 - accuracy: 0.2352 - lr: 0.0100\n",
            "Epoch 6/15\n",
            "26/26 [==============================] - 9s 334ms/step - batch_loss: 6.4820 - perplexity: 680.4648 - accuracy: 0.2467 - lr: 0.0100\n",
            "Epoch 7/15\n",
            "26/26 [==============================] - 9s 331ms/step - batch_loss: 6.3199 - perplexity: 348.1832 - accuracy: 0.2517 - lr: 0.0100\n",
            "Epoch 8/15\n",
            "26/26 [==============================] - 10s 378ms/step - batch_loss: 6.6698 - perplexity: 2078.2722 - accuracy: 0.2683 - lr: 0.0100\n",
            "Epoch 9/15\n",
            "26/26 [==============================] - 9s 331ms/step - batch_loss: 5.8694 - perplexity: 165.8130 - accuracy: 0.2783 - lr: 0.0050\n",
            "Epoch 10/15\n",
            "26/26 [==============================] - 9s 333ms/step - batch_loss: 5.5759 - perplexity: 363.4424 - accuracy: 0.2881 - lr: 0.0050\n",
            "Epoch 11/15\n",
            "26/26 [==============================] - 9s 329ms/step - batch_loss: 5.4905 - perplexity: 260.5166 - accuracy: 0.2997 - lr: 0.0050\n",
            "Epoch 12/15\n",
            "26/26 [==============================] - 9s 355ms/step - batch_loss: 5.1081 - perplexity: 157.9795 - accuracy: 0.3051 - lr: 0.0050\n",
            "Epoch 13/15\n",
            "26/26 [==============================] - 14s 534ms/step - batch_loss: 4.9898 - perplexity: 188.5990 - accuracy: 0.3121 - lr: 0.0050\n",
            "Epoch 14/15\n",
            "26/26 [==============================] - 15s 581ms/step - batch_loss: 4.6745 - perplexity: 150.5335 - accuracy: 0.3159 - lr: 0.0050\n",
            "Epoch 15/15\n",
            "26/26 [==============================] - 9s 329ms/step - batch_loss: 4.4763 - perplexity: 140.6900 - accuracy: 0.3213 - lr: 0.0050\n"
          ]
        }
      ],
      "source": [
        "batch_loss = BatchLogs('batch_loss')\n",
        "perplexity = BatchLogs('perplexity')\n",
        "accuracy = BatchLogs('accuracy')\n",
        "history = train_model.fit(\n",
        "    dataset.train, \n",
        "    batch_size=dataset_config['batch_size'],\n",
        "    # epochs=trainer_config['epochs'],\n",
        "    epochs=15, \n",
        "    callbacks=[batch_loss, perplexity, accuracy, lr_scheduler],\n",
        "    # validation_data=dataset.val, \n",
        "    # validation_batch_size=dataset_config['batch_size'],\n",
        "    verbose='auto',\n",
        "    use_multiprocessing = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC-NjKElfqNf"
      },
      "source": [
        "#5. Evaluation for QG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqHNOZo8Kld7"
      },
      "outputs": [],
      "source": [
        "class QGeneratorEvaluator(tf.Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def test_step(self, inputs):\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    encoder_output, encoder_state = self.encoder(context, training=False)\n",
        "    decoder_state = encoder_state\n",
        "    loss = tf.constant(0.0)\n",
        "    t = 0\n",
        "\n",
        "    # Reference :- https://www.tensorflow.org/guide/function\n",
        "    # We have to run the decoder for all the length of the question \n",
        "    while t < (self.max_length_question - 1):\n",
        "      # We have to pass two tokens:\n",
        "      #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "      #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "      new_token = tf.gather(question, t, axis=1)\n",
        "      target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "      step_loss, metric_value, decoder_state = self.step_decoder(\n",
        "          (new_token, target_token),\n",
        "          context_mask,\n",
        "          encoder_output,\n",
        "          decoder_state, \n",
        "          training=True)\n",
        "\n",
        "      loss = loss + step_loss\n",
        "      # self.custom_metric_mean.update_state(metric_value)\n",
        "      t = t + 1\n",
        "\n",
        "    # Average the loss for all the legit tokens\n",
        "    avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "    # return {f'batch_loss': avg_loss, f'batch_perplexity': self.custom_metric_mean.result()}\n",
        "    return {f'batch_loss': avg_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezgR7c68_0nv"
      },
      "source": [
        "# 6. Inference for QG\n",
        "In this section we will provide the class and the methods for the inference part. More specifically, both auxiliary and inferencing methods:\n",
        "1. `token_to_string()`:\n",
        "2. `string_to_token()`:\n",
        "3. `create_mask()`:\n",
        "4. `temperature_sampling()`:\n",
        "5. `generate_question()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn2NTxAq_2sy"
      },
      "outputs": [],
      "source": [
        "class QGeneratorInference(tf.Module):\n",
        "  def __init__(self, encoder, decoder, tokenizer, word_to_idx, use_tf_function):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_idx = word_to_idx\n",
        "   \n",
        "    self.result_tokens = None\n",
        "    self.result_text = None\n",
        "    self.token_mask = self.create_mask()\n",
        "\n",
        "    self.start_idx = word_to_idx['<sos>']\n",
        "    self.end_idx = word_to_idx['<eos>']\n",
        "    self.use_tf_function = False\n",
        "\n",
        "  def token_to_string(self, result_tokens: tf.Tensor):  \n",
        "    \"\"\"\n",
        "    This method converts token IDs to text by using a given mapping.\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    list_text = self.tokenizer.sequences_to_texts(list_tokens)\n",
        "    list_text = tf.convert_to_tensor([list_text])\n",
        "    result_text = tf.strings.reduce_join(list_text, axis=0, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    \n",
        "    self.result_tokens = result_tokens\n",
        "    self.result_text = result_text\n",
        "    return result_text\n",
        "\n",
        "  def string_to_token(self, result_str: tf.Tensor):\n",
        "    \"\"\"\n",
        "    This method converts texts to token IDs by using a given mapping.\n",
        "    \"\"\"  \n",
        "    list_str = [s.decode(\"utf-8\") for s in result_str.numpy().tolist()]\n",
        "    list_tokens = self.tokenizer.texts_to_sequences(list_str)\n",
        "    list_tokens = tf.convert_to_tensor(list_tokens, dtype=tf.int64)\n",
        "    result_tokens = tf.squeeze(tf.split(list_tokens, num_or_size_splits=list_tokens.shape[0], axis=0), axis=1)\n",
        "\n",
        "    return result_tokens\n",
        "  \n",
        "  def create_mask(self):\n",
        "    \"\"\"\n",
        "    This method creates a mask for the padding, the unknwon words and the start/ending tokens.\n",
        "    \"\"\"\n",
        "    masked_words = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "    token_mask_ids = [self.tokenizer.word_index[mask] for mask in masked_words]\n",
        "\n",
        "    token_mask = np.zeros(shape=(len(self.word_to_idx),), dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    return token_mask\n",
        "\n",
        "  # evaluate or predict?\n",
        "  def evaluate(self, inputs, max_length, return_attention, mode='greedy', temperature=0.5):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if mode == 'greedy':\n",
        "      if self.use_tf_function:\n",
        "        return self._tf_generate_greedy(inputs, max_length, temperature, return_attention)\n",
        "      else:\n",
        "        return self._generate_greedy(inputs, max_length, temperature, return_attention)\n",
        "    elif mode == 'beam':\n",
        "      return self._generate_beam(inputs, max_length, return_attention)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_generate_greedy(self, inputs, max_length, temperature, return_attention):\n",
        "    return self._generate_question_greedy(inputs, max_length, return_attention, temperature)\n",
        "  \n",
        "  def _generate_greedy(self, \n",
        "                        inputs,\n",
        "                        max_length,\n",
        "                        return_attention,\n",
        "                        temperature):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    # Similarly for what it has been done in the train step\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    # Generate the first token of each sentence, that is the <sos> token\n",
        "    new_token = tf.fill([batch_size, 1], self.start_idx)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    timestep = 0\n",
        "    \n",
        "    while timestep < max_length:\n",
        "      # Decode the token at the next timestep\n",
        "      decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=(inputs != 0)),\n",
        "        state = decoder_state)\n",
        "      \n",
        "      attention.append(decoder_result.attention_weights)\n",
        "\n",
        "      # Sample the new token accordingly to the distribution produced by the decoder\n",
        "      new_token = self.temperature_sampling(decoder_result.logits, temperature)\n",
        "\n",
        "      # if a sequence has reached <eos> set it as done\n",
        "      # MISSING PART\n",
        "\n",
        "      result_tokens.append(new_token)\n",
        "\n",
        "      timestep = timestep + 1\n",
        "    \n",
        "    # MISSING\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.token_to_string(result_tokens)\n",
        "\n",
        "    attention_stack = tf.concat(attention, axis=-1)\n",
        "\n",
        "    # HANDLING UNK WORDS\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}\n",
        "\n",
        "  def temperature_sampling(self, logits, temperature):\n",
        "    \"\"\"\n",
        "\n",
        "    For the temperature choice see here:\n",
        "      Reference :- https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "    \"\"\"\n",
        "    # First of all we use broadcast the generated mask to the expected logits' shape\n",
        "    # token_mask shape: (batch_size, timestep, vocab_size)\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # The logits for all the tokens that have to not be used are set top -1.0\n",
        "    logits = tf.where(token_mask, -1.0, logits)\n",
        "\n",
        "    # Freezing function\n",
        "    # Higher temperature -> greater variety\n",
        "    # Lower temperature -> grammatically correct\n",
        "    if temperature == 0.0:\n",
        "      # the freezing function is the argmax, behaving like a greedy search\n",
        "      new_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      # the freezing function now scales the logits.\n",
        "      # for temperature == 1.0 is the identity function\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_token = tf.random.categorical(logits / temperature, num_samples=1)\n",
        "    return new_token\n",
        "\n",
        "  # def _beam_search(self, k, max_length, encoder_output, encoder_state, mask):\n",
        "  #   decoder_states = list()\n",
        "    \n",
        "  #   for i in range(k):\n",
        "  #     decoder_states.append(encoder_state)\n",
        "\n",
        "  #   print(decoder_states[0])\n",
        "  #   # decoders = []\n",
        "    \n",
        "  #   sequences = [[list(), 0.0, 0]]\n",
        "  #   timestep = 0\n",
        "  #   new_tokens = tf.fill([1, k, 1], self.start_idx)\n",
        "  #   all_candidates = list()\n",
        "    \n",
        "  #   while timestep < max_length:\n",
        "  #     timestep = timestep+1\n",
        "      \n",
        "  #     for i in range(len(sequences)):\n",
        "  #       seq, score, _ = sequences[i]\n",
        "\n",
        "  #       decoder_result, decoder_state = self.decoder(\n",
        "  #         inputs = DecoderInput(\n",
        "  #             new_token=new_tokens[i],\n",
        "  #             enc_output=encoder_output,\n",
        "  #             mask=mask),\n",
        "  #         state = decoder_states[i])\n",
        "\n",
        "  #       decoder_states[i] = decoder_state\n",
        "        \n",
        "  #       for j in range(decoder.logits.shape[-1]):\n",
        "  #         candidate = [seq + [j], score - np.log(decoder.logits[0][j]), i]\n",
        "  #         all_candidates.append(candidate)\n",
        "  #       # order all candidates by score\n",
        "  #     ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "  #     sequences = ordered[:k]\n",
        "  #     new_tokens = list()\n",
        "  #     # new_decoders = []\n",
        "  #     new_decoder_states = []\n",
        "  #     for k in range(k):\n",
        "  #       new_tokens.append(all_candidates[k][0][-1])\n",
        "  #       # new_decoders.append(decoders[all_candidates[k][2]])\n",
        "  #       new_decoder_states.append(decoder_states[all_candidates[k][2]])\n",
        "  #     # decoders = new_decoders\n",
        "  #     decoder_states = new_decoder_states\n",
        "  #   return sequences\n",
        "\n",
        "  # def _generate_question_beam_search(self, inputs, max_length, return_attention=True):\n",
        "  #   batch_size = tf.shape(inputs)[0]\n",
        "  #   encoder_output, encoder_state = self.encoder(inputs)\n",
        "\n",
        "  #   hidden_state, cell_state = encoder_state\n",
        "  #   batch_sequences = []\n",
        "  #   for i in range(batch_size):\n",
        "  #     encoder_state_batch = (hidden_state[i, :], cell_state[i, :])\n",
        "  #     sequences = self._beam_search(3, \n",
        "  #                                   max_length, \n",
        "  #                                   encoder_output[i, :, :], \n",
        "  #                                   encoder_state_batch, \n",
        "  #                                   (inputs[i,] != 0))\n",
        "  #     batch_sequences.append(sequences)\n",
        "  #   return batch_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q2pWJ5YDfqi"
      },
      "outputs": [],
      "source": [
        "qg_generator = QGeneratorInference(train_model.encoder, \n",
        "                                   train_model.decoder, \n",
        "                                   dataset_creator.tokenizer_question, \n",
        "                                   word_to_idx=word_to_idx_question[1], \n",
        "                                   use_tf_function=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fklYEd1e6XOb"
      },
      "outputs": [],
      "source": [
        "example_logits = tf.random.normal([5, 1, len(word_to_idx_question[1])])\n",
        "example_output_tokens = qg_generator.temperature_sampling(example_logits, temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RO-7QCL_MSc"
      },
      "outputs": [],
      "source": [
        "qg_generator.evaluate(example_output_tokens, max_length=10, return_attention=False, mode='greedy')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sFBKCIE3Jxf2",
        "ZgeJckqZIufT",
        "dndR7_CNI1jq",
        "MvU7n1LboA6g",
        "hlpy-ayWoEHa",
        "r7qxjGzKJM2w",
        "kx2f7Nn_4en9",
        "fauWdCYGMmUy",
        "qIoySQKuIGlc",
        "x6uDOVpginU0",
        "ZC-NjKElfqNf",
        "ezgR7c68_0nv"
      ],
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}