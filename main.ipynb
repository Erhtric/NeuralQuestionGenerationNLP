{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sFBKCIE3Jxf2",
        "ZgeJckqZIufT",
        "hlpy-ayWoEHa",
        "r7qxjGzKJM2w",
        "kx2f7Nn_4en9",
        "wjVfZgIIf1RV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erhtric/NeuralQuestionGenerationNLP/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main file: its purpouse is to collect all the code coming from the coding pipeline."
      ],
      "metadata": {
        "id": "8OU-wpGL18xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorflow-addons\n",
        "#!pip install -q \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "id": "Htbwd0W_Y9IC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "import re\n",
        "import os\n",
        "import typing\n",
        "from typing import Any, Tuple, List, NamedTuple\n",
        "import spacy\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "import seaborn as sns\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from keras.layers import (\n",
        "    Layer, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    LSTMCell,\n",
        "    Dense, \n",
        "    Bidirectional, \n",
        "    Input, \n",
        "    AdditiveAttention)\n",
        "\n",
        "import nltk\n",
        "from nltk import punkt, pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvONYDvKAHz",
        "outputId": "54318eb3-012a-4358-92af-7aed53d0b1ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CIZdy1hp14x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e093b91-096b-415b-8ae2-6f1d32f2724a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commands to prepare the folder to accomodate data."
      ],
      "metadata": {
        "id": "UqKVWPel_ybt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP/Project/Testing folder/Eric\n",
        "%pwd\n",
        "%mkdir data\n",
        "%mkdir training_checkpoints\n",
        "\n",
        "# disable chained assignments to avoid annoying warning\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKVGy7JPJCoi",
        "outputId": "bd7a97e4-a936-4992-c965-e8d52a4dbdc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cVw6eUwM-dRL9BhqtXULyOqeXDrYkwmH/NLP/Project/Testing folder/Eric\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘training_checkpoints’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print({tf.__version__})"
      ],
      "metadata": {
        "id": "-ePFW3UcrVtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8c65c2-8829-47ed-f407-c309fc094925"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'2.8.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for the `configuration.json` file: "
      ],
      "metadata": {
        "id": "2hyzKyj_-GjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "units = 600\n",
        "\n",
        "dataset_config = {\n",
        "    # 'num_examples': 18896,\n",
        "    'num_examples': 9000,\n",
        "    'num_words_context': 45000,\n",
        "    'num_words_question': 28000,\n",
        "    'buffer_size': 32000,\n",
        "    'batch_size': batch_size,\n",
        "    'random_seed': 13,\n",
        "}\n",
        "\n",
        "encoder_config = {\n",
        "    'context_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_context': None\n",
        "}\n",
        "\n",
        "decoder_config = {\n",
        "    'question_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_question': None,\n",
        "}\n",
        "\n",
        "trainer_config = {\n",
        "    'epochs': 3,\n",
        "    'optimizer': tf.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "}\n",
        "\n",
        "path = {\n",
        "    'training_json_path': \"./data/training_set.json\",\n",
        "    'save_pkl_path': \"./data/squadv2.pkl\"\n",
        "}"
      ],
      "metadata": {
        "id": "5bS3uLkE-Mvf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data handling and Pre-processing\n",
        "\n",
        "\n",
        "Things to do:\n",
        "1. Add to each sentence $x$ a start of sequence `<SOS>` tag and end of sequence `<EOS>` tag,\n",
        "2. Clean the sentences by removing special chars,\n",
        "3. Perform other preprocessing steps,\n",
        "4. Create a **vocabulary** with a word-to-index and index-to-word mappings by using a **tokenizer**, \n",
        "5. Extract the sentences that contain an answer and use them as input features, whereas the question will be our target\n",
        "6. Pad each context to maximum length.\n",
        "\n",
        "The resulting data that will be used hereinafter will be of type `tf.data.Dataset`. "
      ],
      "metadata": {
        "id": "sFBKCIE3Jxf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(NamedTuple):\n",
        "  \"\"\"\n",
        "  This class represent a a 3-way split processed dataset. \n",
        "  \"\"\"\n",
        "  # Reference :- https://github.com/topper-123/Articles/blob/master/New-interesting-data-types-in-Python3.rst\n",
        "  train: tf.data.Dataset\n",
        "  val: tf.data.Dataset\n",
        "  test: tf.data.Dataset\n",
        "\n",
        "class SQuAD:\n",
        "  def __init__(self):\n",
        "    self.random_seed = None\n",
        "    self.squad_df = None\n",
        "    self.preproc_squad_df = None\n",
        "    self.tokenizer = None\n",
        "    self.buffer_size = 0\n",
        "    self.batch_size = 0\n",
        "\n",
        "  def __call__(self,\n",
        "           num_examples, \n",
        "           buffer_size, \n",
        "           batch_size, \n",
        "           random_seed,\n",
        "           training_json_path,\n",
        "           save_pkl_path,\n",
        "           num_words_context=None,\n",
        "           num_words_question=None,\n",
        "           tokenized=True,\n",
        "           pos_ner_tag=True,\n",
        "           tensor_type=True):\n",
        "    \"\"\"The call() method loads the SQuAD dataset, preprocess it and optionally it returns \n",
        "    it tokenized. Moreover it also perform a 3-way split.\n",
        "\n",
        "    Args:\n",
        "        num_examples (int): number of examples to be taken from the original SQuAD dataset\n",
        "        num_words (int): the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. \n",
        "        buffer_size (int): buffer size for the shuffling operation\n",
        "        batch_size (int): size of the batches\n",
        "        tokenized (boolean): specifies if the context and question data should be both tokenized\n",
        "        pos_ner_tag (boolean):\n",
        "        tensro_type (boolean): \n",
        "\n",
        "    Returns (depending on the input parameters):\n",
        "        pd.DataFrame: training dataset\n",
        "        pd.DataFrame: validation dataset\n",
        "        pd.DataFrame: testing dataset\n",
        "          OR\n",
        "        NamedTuple: dataset, (dict, dict, dict)\n",
        "    \"\"\"\n",
        "    self.random_seed = random_seed\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "    self.training_json_path = training_json_path\n",
        "    self.save_pkl_path = save_pkl_path\n",
        "    self.pos_ner_tag = pos_ner_tag\n",
        "\n",
        "    # Load dataset from file\n",
        "    self.load_dataset(num_examples)\n",
        "    # Extract answer\n",
        "    self.extract_answer()\n",
        "    # Preprocess context and question\n",
        "    self.preprocess()\n",
        "    \n",
        "    # Perform splitting\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.split_train_val(self.preproc_squad_df)\n",
        "    \n",
        "    if self.pos_ner_tag: \n",
        "      pass\n",
        "\n",
        "    # Initialize Tokenizer for the source: in our case the context phrases\n",
        "    # alternatively TextVectorization \n",
        "    self.tokenizer_context = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_context)\n",
        "    # initialize also for the target, namely the question phrases\n",
        "    self.tokenizer_question = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_question)\n",
        "\n",
        "    if tokenized:\n",
        "      X_train_tokenized, word_to_idx_train_context = self.__tokenize_context(X_train)\n",
        "      y_train_tokenized, word_to_idx_train_question = self.__tokenize_question(y_train)\n",
        "\n",
        "      X_val_tokenized, word_to_idx_val_context = self.__tokenize_context(X_val)\n",
        "      y_val_tokenized, word_to_idx_val_question = self.__tokenize_question(y_val)\n",
        "\n",
        "      X_test_tokenized, word_to_idx_test_context = self.__tokenize_context(X_test)\n",
        "      y_test_tokenized, word_to_idx_test_question = self.__tokenize_question(y_test)\n",
        "\n",
        "      word_to_idx_context = (word_to_idx_train_context, word_to_idx_val_context, word_to_idx_test_context)\n",
        "      word_to_idx_question = (word_to_idx_train_question, word_to_idx_val_question, word_to_idx_test_question)\n",
        "      \n",
        "      if tensor_type:\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "        # Returns tf.Data.Dataset objects (tokenized)\n",
        "        train_dataset = self.to_tensor(X_train_tokenized, y_train_tokenized)\n",
        "        val_dataset = self.to_tensor(X_val_tokenized, y_val_tokenized)\n",
        "        test_dataset = self.to_tensor(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "        # Configure the dataset for performance\n",
        "        train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        dataset = Dataset(\n",
        "            train=train_dataset, \n",
        "            val=val_dataset,\n",
        "            test=test_dataset)\n",
        "\n",
        "        return dataset, word_to_idx_context, word_to_idx_question\n",
        "      else:\n",
        "        # Returns pd.DataFrame objects (tokenized)\n",
        "        return X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized\n",
        "    else:\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def load_dataset(self, num_examples):\n",
        "    \"\"\"\n",
        "    Extract the dataset from the json file. Already grouped by title.\n",
        "\n",
        "    :param path: [Optional] specifies the local path where the training_set.json file is located\n",
        "\n",
        "    :return\n",
        "        - the extracted dataset in a dataframe format\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.save_pkl_path):\n",
        "      print('File already exists! Loading from .pkl...\\n')\n",
        "      self.squad_df = pd.read_pickle(self.save_pkl_path)\n",
        "      self.squad_df = self.squad_df[:num_examples]\n",
        "    else:\n",
        "      print('Loading from .json...\\n')\n",
        "      with open(self.training_json_path) as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      df_array = []\n",
        "      for current_subject in data['data']:\n",
        "          title = current_subject['title']\n",
        "\n",
        "          for current_context in current_subject['paragraphs']:\n",
        "              context = current_context['context']\n",
        "\n",
        "              for current_question in current_context['qas']:\n",
        "                  question = current_question['question']\n",
        "                  id = current_question['id']\n",
        "\n",
        "              for answer_text in current_question['answers']:\n",
        "                    answer = answer_text['text']\n",
        "                    answer_start = answer_text['answer_start']\n",
        "                    record = { \"id\": id,\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"answer_start\": answer_start,\n",
        "                                \"answer\": answer\n",
        "                                }\n",
        "\n",
        "              df_array.append(record)\n",
        "      \n",
        "      # Save file\n",
        "      pd.to_pickle(pd.DataFrame(df_array), self.save_pkl_path)\n",
        "      self.squad_df = pd.DataFrame(df_array)[:num_examples]\n",
        "\n",
        "  def preprocess(self):\n",
        "    df = self.squad_df.copy()\n",
        "\n",
        "    # Pre-processing context\n",
        "    context = list(df.context)\n",
        "    preproc_context = []\n",
        "\n",
        "    for c in context:\n",
        "      c = self.__preprocess_sentence(c, question=False)\n",
        "      preproc_context.append(c)\n",
        "    \n",
        "    df.context = preproc_context\n",
        "\n",
        "    # Pre-processing questions\n",
        "    question = list(df.question)\n",
        "    preproc_question = []\n",
        "\n",
        "    for q in question:\n",
        "      q = self.__preprocess_sentence(q, question=True)\n",
        "      preproc_question.append(q)\n",
        "    \n",
        "    df.question = preproc_question\n",
        "\n",
        "    # Remove features that are not useful\n",
        "    df = df.drop(['id'], axis=1)\n",
        "    self.preproc_squad_df = df\n",
        "\n",
        "  def __preprocess_sentence(self, sen, question):\n",
        "    # Creating a space between a word and the punctuation following it\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    sen = re.sub(r\"([?.!,¿])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sen = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", sen)\n",
        "\n",
        "    sen = sen.strip()\n",
        "\n",
        "    # Adding a start and an end token to the sentence so that the model know when to \n",
        "    # start and stop predicting.\n",
        "    # if not question: sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    return sen\n",
        "\n",
        "  def __answer_start_end(self, df):\n",
        "    \"\"\"\n",
        "    Creates a list of starting indexes and ending indexes for the answers.\n",
        "\n",
        "    :param df: the target Dataframe\n",
        "\n",
        "    :return: a dataframe containing the start and the end indexes foreach answer (ending index is excluded).\n",
        "\n",
        "    \"\"\"\n",
        "    start_idx = df.answer_start\n",
        "    end_idx = [start + len(list(answer)) for start, answer in zip(list(start_idx), list(df.answer))]\n",
        "    return pd.DataFrame(list(zip(start_idx, end_idx)), columns=['start', 'end'])\n",
        "\n",
        "  def split_train_val(self, df, train_size=0.8):\n",
        "    \"\"\"\n",
        "    This method splits the dataframe in training and test sets, or eventually, in training, validation and test sets.\n",
        "\n",
        "    Args\n",
        "        :param df: the target Dataframe\n",
        "        :param random_seed: random seed used in the splits\n",
        "        :param train_size: represents the absolute number of train samples\n",
        "        :param val: boolean for choosing between a 3-way split or 2-way one.\n",
        "\n",
        "    Returns:\n",
        "        - Data and labels for training, validation and test sets if val is True \n",
        "        - Data and labels for training and test sets if val is False \n",
        "\n",
        "    \"\"\"\n",
        "    # Maybe we have also to return the index for the starting answer\n",
        "    X = df.drop(['answer_start', 'question', 'answer'], axis=1).copy()\n",
        "    idx = self.__answer_start_end(df)\n",
        "    X['start'] = idx['start']\n",
        "    X['end'] = idx['end']\n",
        "    y = df['question']\n",
        "\n",
        "    # In the first step we will split the data in training and remaining dataset\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X, groups=X['title'])\n",
        "    train_idx, rem_idx = next(split)\n",
        "\n",
        "    X_train = X.iloc[train_idx]\n",
        "    y_train = y.iloc[train_idx]\n",
        "    X_rem = X.iloc[rem_idx]\n",
        "    y_rem = y.iloc[rem_idx]\n",
        "\n",
        "\n",
        "    # Val and test test accounts for 10% of the total data. Both 5%.\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X_rem, groups=X_rem['title'])\n",
        "    val_idx, test_idx = next(split)\n",
        "\n",
        "    X_val = X_rem.iloc[val_idx]\n",
        "    y_val = y_rem.iloc[val_idx]\n",
        "\n",
        "    X_test = X_rem.iloc[test_idx]\n",
        "    y_test = y_rem.iloc[test_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def __tokenize_context(self, X):\n",
        "    context = X.context\n",
        "    self.tokenizer_context.fit_on_texts(context)\n",
        "    context_tf = self.tokenizer_context.texts_to_sequences(context)\n",
        "\n",
        "    # context_lengths = [len(seq) for seq in context_tf]\n",
        "    # sns.boxplot(context_lengths)\n",
        "\n",
        "    context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(context):\n",
        "      X['context'].iloc[i] = context_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_context.word_index['<pad>'] = 0\n",
        "    self.tokenizer_context.index_word[0] = '<pad>'\n",
        "\n",
        "    return X, self.tokenizer_context.word_index\n",
        "\n",
        "  def __tokenize_question(self, y):\n",
        "    question = y\n",
        "    self.tokenizer_question.fit_on_texts(question)\n",
        "    question_tf = self.tokenizer_question.texts_to_sequences(question)\n",
        "\n",
        "    # question_lengths = [len(seq) for seq in question_tf]\n",
        "    # sns.boxplot(question_lengths)\n",
        "    \n",
        "    # See also tf.data.Dataset.padding_batch\n",
        "    question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(question):\n",
        "      y.iloc[i] = question_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_question.word_index['<pad>'] = 0\n",
        "    self.tokenizer_question.index_word[0] = '<pad>'\n",
        "\n",
        "    return y, self.tokenizer_question.word_index\n",
        "\n",
        "  def extract_answer(self):\n",
        "    df = self.squad_df.copy()\n",
        "    start_end = self.__answer_start_end(df)\n",
        "    context = list(df.context)\n",
        "    \n",
        "    selected_sentences = []\n",
        "    for i, par in enumerate(context):\n",
        "      sentences = sent_tokenize(par)\n",
        "      start = start_end.iloc[i].start\n",
        "      end = start_end.iloc[i].end      \n",
        "      right_sentence = \"\"\n",
        "      context_characters = 0\n",
        "\n",
        "      for j, sen in enumerate(sentences):\n",
        "        sen += ' '\n",
        "        context_characters += len(sen)\n",
        "        # If the answer is completely in the current sentence\n",
        "        if(start < context_characters and end <= context_characters):\n",
        "          right_sentence = sen\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break\n",
        "        # the answer is in both the current and the next sentence\n",
        "        if(start < context_characters and end > context_characters):\n",
        "          right_sentence = sen + sentences[j+1]\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break \n",
        "\n",
        "    self.squad_df.context = selected_sentences\n",
        "\n",
        "  def to_tensor(self, X, y):\n",
        "    X = X.context.copy()\n",
        "    y = y.copy()\n",
        "\n",
        "    # Reference:- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(list(X), tf.int64), \n",
        "         tf.cast(list(y), tf.int64)))\n",
        "    dataset = dataset.shuffle(self.buffer_size).batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "XyCKxqwRZelj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the `SQuAD` constructor we create a dataset handling object which will be useful for future operations."
      ],
      "metadata": {
        "id": "iTXVdOGcZSCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_creator = SQuAD()"
      ],
      "metadata": {
        "id": "RfCYdZofJ866"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preprocessed untokenized split"
      ],
      "metadata": {
        "id": "ZgeJckqZIufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                                                                       num_words=None,\n",
        "#                                                                       BUFFER_SIZE=32000,\n",
        "#                                                                       BATCH_SIZE=64,\n",
        "#                                                                       random_seed=RANDOM_SEED,\n",
        "#                                                                       tokenized=False)\n",
        "\n",
        "# print(f'Set target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')\n",
        "\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator(**dataset_config, **path, tokenized=False)"
      ],
      "metadata": {
        "id": "TJFUuu2Y5hc-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Tokenized split"
      ],
      "metadata": {
        "id": "dndR7_CNI1jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Tensor Ready\n",
        "\n",
        "This is the data produced that we are most interested in. As we can see we will have:\n",
        "- a data structure `dataset` containing the training, validation and test set;\n",
        "- a tuple containing the word-to-token mappings for the training, validation and test set respectively."
      ],
      "metadata": {
        "id": "MvU7n1LboA6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "%%time\n",
        "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(**dataset_config, **path, tokenized=True)\n",
        "\n",
        "max_length_context = dataset.train.element_spec[0].shape[1]\n",
        "max_length_question = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "print(f'Sentences max lenght: {max_length_context}')\n",
        "print(f'Questions max lenght: {max_length_question}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax999y7aI75Y",
        "outputId": "0fd30016-9e4c-44f3-c3a6-176f1317bfad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists! Loading from .pkl...\n",
            "\n",
            "Sentences max lenght: 389\n",
            "Questions max lenght: 40\n",
            "CPU times: user 9.48 s, sys: 907 ms, total: 10.4 s\n",
            "Wall time: 12.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing such `NamedTuple` data structure (cfr `dataset`) is pretty simple, namely in a:\n",
        "1. tuple-way by accessing it like a list, e.g. `train = dataset[0]`,\n",
        "2. object-way by calling the instance parameters, e.g. `train = dataset.train`.\n",
        "\n",
        "The other two returned values are the word to index mappings for the context and question words respectively. In order to refer to a specific split simply call:\n",
        "1. for the training dataset,\n",
        "2. for the validation dataset,\n",
        "3. for the test dataset,"
      ],
      "metadata": {
        "id": "W7hARM_R2Kod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training vocab size for the context: {len(word_to_idx_context[0])}')\n",
        "print(f'Training vocab size for the question: {len(word_to_idx_question[0])}')\n",
        "print()\n",
        "print(f'Validation vocab size for the context: {len(word_to_idx_context[1])}')\n",
        "print(f'Validation vocab size for the question: {len(word_to_idx_question[1])}')\n",
        "print()\n",
        "print(f'Test vocab size for the context: {len(word_to_idx_context[2])}')\n",
        "print(f'Test vocab size for the question: {len(word_to_idx_question[2])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaRYgawxyXp",
        "outputId": "86e8eb34-eac4-4a05-faf3-bbbd8d09bf9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size for the context: 23465\n",
            "Training vocab size for the question: 11342\n",
            "\n",
            "Validation vocab size for the context: 26136\n",
            "Validation vocab size for the question: 12672\n",
            "\n",
            "Test vocab size for the context: 26567\n",
            "Test vocab size for the question: 12892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Standard"
      ],
      "metadata": {
        "id": "hlpy-ayWoEHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                      BUFFER_SIZE=32000,\n",
        "#                      BATCH_SIZE=64,\n",
        "#                      random_seed=RANDOM_SEED,\n",
        "#                      tokenized=True,\n",
        "#                      tensor_type=False)\n",
        "\n",
        "# print(f'\\nSet target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "vJFBk-r8eIcj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Original SQuAD dataset"
      ],
      "metadata": {
        "id": "r7qxjGzKJM2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original dataset\n",
        "# squad_df = dataset_creator.squad_df\n",
        "# print(f'[Info] SQuAD target: {list(squad_df.columns.values)}')\n",
        "# print(f'[Info] Shape: {squad_df.shape}')"
      ],
      "metadata": {
        "id": "9qhTAc5NErOk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. GloVe and embedding matrix"
      ],
      "metadata": {
        "id": "kx2f7Nn_4en9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVe:\n",
        "  def __init__(self, embedding_dimension):\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "\n",
        "    try:\n",
        "      self.embedding_model = KeyedVectors.load(f'./data/glove_model_{self.embedding_dimension}')\n",
        "    except FileNotFoundError:\n",
        "      print('[Warning] Model not found in local folder, please wait...')\n",
        "      self.embedding_model = self.load_glove()\n",
        "      self.embedding_model.save(f'./data/glove_model_{self.embedding_dimension}')  \n",
        "      print('Download finished. Model loaded!')\n",
        "\n",
        "  def load_glove(self):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained GloVe embedding model via gensim library.\n",
        "\n",
        "    We have a matrix that associate words to a vector of a user-defined dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(self.embedding_dimension)\n",
        "\n",
        "    try:\n",
        "      emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "      print(\"Generic error when loading GloVe\")\n",
        "      print(\"Check embedding dimension\")\n",
        "      raise e\n",
        "\n",
        "    emb_model = gloader.load(download_path)\n",
        "    return emb_model\n",
        "\n",
        "  def build_embedding_matrix(self, word_to_idx, vocab_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the \n",
        "        dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, self.embedding_dimension), dtype=np.float32)\n",
        "    oov_count = 0\n",
        "    oov_words = []\n",
        "\n",
        "    # For each word which is not present in the vocabulary we assign a random vector, otherwise we take the GloVe embedding\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      try:\n",
        "        embedding_vector = self.embedding_model[word]\n",
        "      except (KeyError, TypeError):\n",
        "        oov_count += 1\n",
        "        oov_words.append(word)\n",
        "        embedding_vector = np.random.uniform(low=-0.05, \n",
        "                                             high=0.05, \n",
        "                                             size=self.embedding_dimension)\n",
        "\n",
        "      embedding_matrix[idx] = embedding_vector\n",
        "    \n",
        "    print(f'\\n[Debug] {oov_count} OOV words found!\\n')\n",
        "    return embedding_matrix, oov_words"
      ],
      "metadata": {
        "id": "TRJ1NpSMqaJL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initialize the handler with the desidered `embedding_dimension`. Then to build the embedding matrix with the pre-trained GloVe embeddings simply call the `build_embedding_matrix` method."
      ],
      "metadata": {
        "id": "gk-z8A5y3cpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Initalize the handler for GloVe\n",
        "glove_handler = GloVe(encoder_config['embedding_dimension'])\n",
        "\n",
        "# We will create the matrix by using only the words present in the training and validation set\n",
        "embedding_matrix_context, oov_words_context = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_context[1], \n",
        "    len(word_to_idx_context[1])+1)\n",
        "\n",
        "embedding_matrix_question, oov_words_question = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_question[1], \n",
        "    len(word_to_idx_question[1])+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUmvdCWGavSR",
        "outputId": "2fe82e67-26ba-4a77-ec5f-14ea0e26c0f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26136/26136 [00:00<00:00, 218226.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 1706 OOV words found!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12672/12672 [00:00<00:00, 252339.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 652 OOV words found!\n",
            "\n",
            "CPU times: user 1.16 s, sys: 451 ms, total: 1.61 s\n",
            "Wall time: 6.72 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert both of them into tensor, but it is fine to also treat them as `numpy` array, still it is better to use the `tensorflow` fundamentals."
      ],
      "metadata": {
        "id": "qdLOk0pQu3Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_context = tf.convert_to_tensor(embedding_matrix_context)\n",
        "embedding_matrix_question = tf.convert_to_tensor(embedding_matrix_question)"
      ],
      "metadata": {
        "id": "EX6PvKTBsdSW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Definition"
      ],
      "metadata": {
        "id": "FF5Rtd4uqa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_context_batch, example_question_batch = next(iter(dataset.train))"
      ],
      "metadata": {
        "id": "GjdRysIvPoLc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Encoder\n",
        "We will use a bidirectional LSTM to encode the sentence,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\overrightarrow{b_t} &= \\overrightarrow{\\text{LSTM}}(x_t, \\overrightarrow{b_{t-1}})\\\\\n",
        "\\overleftarrow{b_t} &= \\overleftarrow{\\text{LSTM}}(x_t, \\overleftarrow{b_{t+1}})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\overrightarrow{b_t}$ is the hidden state at time step $t$ for the forward pass LSTM and $\\overleftarrow{b_t}$ for the backward pass."
      ],
      "metadata": {
        "id": "wjVfZgIIf1RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               context_vocab_size, \n",
        "               embedding_matrix,\n",
        "               embedding_dimension, \n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_context,\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_context = tf.constant(max_length_context)\n",
        "\n",
        "    # Layers\n",
        "    self.inputs = Input(shape=(self.max_length_context,), name='encoder_input')\n",
        "    \n",
        "    self.embedding = Embedding(input_dim=context_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_context,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,\n",
        "                               mask_zero=False,\n",
        "                               name='encoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM forward pass\n",
        "    self.forward_lstm_layer = LSTM(units//2,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform',\n",
        "                                  use_bias=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "    \n",
        "    # The LSTM backward pass\n",
        "    self.backward_lstm_layer = LSTM(units//2,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  go_backwards=True,\n",
        "                                  recurrent_initializer='glorot_uniform',\n",
        "                                  use_bias=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "    \n",
        "    # The Bidirectional wrapper\n",
        "    self.bidirectional_lstm = Bidirectional(self.forward_lstm_layer, \n",
        "                                            backward_layer=self.backward_lstm_layer, \n",
        "                                            name='encoder_bi_lstm',\n",
        "                                            merge_mode='concat')\n",
        "    \n",
        "  def call(self, inputs, state=None, training=True):\n",
        "    # x shape = (batch_size, max_length_context, embedding_dimension)\n",
        "    x = self.embedding(inputs)\n",
        "\n",
        "    # encoder_outputs shape = (batch_size, max_length_context, units)\n",
        "    # forward_h shape = (batch_size, units//2)\n",
        "    # forward_c shape = (batch_size, units//2)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm(x, \n",
        "                                                                                            initial_state=state)\n",
        "    # op:concat shape = (batch_size, units)\n",
        "    h_concat = tf.concat([forward_h, backward_h], axis=1, name='hidden_concat')\n",
        "    c_concat = tf.concat([forward_c, backward_c], axis=1, name='cell_concat')\n",
        "    return encoder_outputs, (h_concat, c_concat)\n",
        "\n",
        "  # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    x = Input(shape=(self.max_length_context,), batch_size=self.batch_size)\n",
        "    return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(), \n",
        "        to_file='encoder.png', dpi=96,              \n",
        "        show_shapes=True, show_layer_names=True,  \n",
        "        expand_nested=False                       \n",
        "    )"
      ],
      "metadata": {
        "id": "GK6Kd1XvqK22"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Test the encoder stack\n",
        "\n"
      ],
      "metadata": {
        "id": "fbjSxPGcFud_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_config['context_vocab_size'] = len(word_to_idx_context[1])\n",
        "encoder_config['max_length_context'] = dataset.train.element_spec[0].shape[1]\n",
        "\n",
        "encoder = Encoder(**encoder_config, embedding_matrix=embedding_matrix_context)\n",
        "encoder_outputs, encoder_state = encoder(inputs=example_context_batch)\n",
        "\n",
        "hidden_state, cell_state = encoder_state\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, max_length_context, units): {encoder_outputs.shape}')\n",
        "print(f'Hidden state shape: (batch_size, units): {hidden_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, units): {cell_state.shape}')"
      ],
      "metadata": {
        "id": "_ffteDMQyzmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e338c8-aaa1-40a7-c50b-070f3fe2b1ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, max_length_context, units): (64, 389, 600)\n",
            "Hidden state shape: (batch_size, units): (64, 600)\n",
            "Cell state shape: (batch_size, units): (64, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.build_graph().summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng97TFpLRym2",
        "outputId": "65c9f155-ed56-4635-d7df-efe9cbf5353c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(64, 389)]          0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding_layer (Embed  (64, 389, 300)      7841100     ['input_1[0][0]']                \n",
            " ding)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_bi_lstm (Bidirectional  [(64, 389, 600),    1442400     ['encoder_embedding_layer[0][0]']\n",
            " )                               (64, 300),                                                       \n",
            "                                 (64, 300),                                                       \n",
            "                                 (64, 300),                                                       \n",
            "                                 (64, 300)]                                                       \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (64, 600)            0           ['encoder_bi_lstm[0][1]',        \n",
            "                                                                  'encoder_bi_lstm[0][3]']        \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)       (64, 600)            0           ['encoder_bi_lstm[0][2]',        \n",
            "                                                                  'encoder_bi_lstm[0][4]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,283,500\n",
            "Trainable params: 1,442,400\n",
            "Non-trainable params: 7,841,100\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "aaJFf-ELdIZo",
        "outputId": "8f4508d6-07ad-4915-b389-a9549e6fe7cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Decoder"
      ],
      "metadata": {
        "id": "dtM9nOQrf3jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Container classes\n",
        "# Reference :- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "class DecoderInput(NamedTuple):\n",
        "  new_token: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "73wq7CTiTQpX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(Layer):\n",
        "  # Still it is possible to use Luang's attention as an alternative\n",
        "  # Reference:- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.W1 = Dense(units, use_bias=False, name='Wb1_attention_weights')\n",
        "    self.W2 = Dense(units, use_bias=False, name='Wb2_attention_weights')\n",
        "\n",
        "    self.attention = AdditiveAttention(use_scale=True)\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    \"\"\"\n",
        "    This layer takes 3 inputs:\n",
        "      - the query; this will be generated by the decoder, later,\n",
        "      - the value: the output of the encoder,\n",
        "      - the mask: to exclude the padding, i.e., context_batch != 0.\n",
        "    \"\"\"\n",
        "    #W1@ht\n",
        "    w1_query = self.W1(query)\n",
        "    #W2@hs\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask = [query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    \n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               question_vocab_size, \n",
        "               embedding_matrix, \n",
        "               embedding_dimension,\n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_question,\n",
        "               **kwargs):\n",
        "    \n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_question = tf.constant(max_length_question)\n",
        "    self.embedding_dimension = tf.constant(embedding_dimension)\n",
        "    self.units = tf.constant(units)\n",
        "\n",
        "    # Layers definition\n",
        "    self.inputs = Input(shape=(None,), batch_size=self.batch_size)\n",
        "                        \n",
        "    # Embedding for the questions\n",
        "    self.embedding = Embedding(input_dim=question_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_question,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,  #?\n",
        "                               mask_zero=False,\n",
        "                               name='decoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM layer\n",
        "    self.lstm_layer = LSTM(units,\n",
        "                          return_sequences=True,\n",
        "                          return_state=True,\n",
        "                          recurrent_initializer='glorot_uniform',\n",
        "                          use_bias=True,\n",
        "                          input_shape=(self.max_length_question, embedding_dimension),\n",
        "                          name='decoder_lstm_layer')\n",
        "    \n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(units)\n",
        "\n",
        "    # Parameters to be learned\n",
        "    self.Wt = Dense(units, activation=tf.math.tanh, use_bias=False, name='decoder_Wt_weights')\n",
        "\n",
        "    # For the word probabilities\n",
        "    # self.Ws = Dense(self.dec_units, activation=tf.nn.softmax, use_bias=False)\n",
        "    self.Ws = Dense(question_vocab_size, activation=tf.nn.softmax, use_bias=False, name='decoder_Ws_weights')\n",
        "\n",
        "  def call(self, \n",
        "            inputs: DecoderInput, \n",
        "            state=None) -> Tuple[DecoderOutput, Tuple[tf.Tensor]]:\n",
        "\n",
        "    # Lookup the embeddings for the questions\n",
        "    x = self.embedding(inputs.new_token)\n",
        "    # embedded_tensor shape: (batch_size, 1, embedding_dimension)\n",
        "    if tf.shape(x).shape == 2: x = tf.expand_dims(x, axis=1)\n",
        "\n",
        "    # Process one step with the RNN\n",
        "    # LSTM expects inputs of shape: (batch_size, timestep, feature)\n",
        "    cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(x, initial_state=state)\n",
        "\n",
        "    # Use the LSTM cell output as the query for the attention over the encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=cell_output, \n",
        "        value=inputs.enc_output, \n",
        "        mask=inputs.mask)\n",
        "\n",
        "    # Join the context_vector and cell outpyt [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    cell_output_and_context_vector = tf.concat([cell_output, context_vector], axis=-1)\n",
        "\n",
        "    # at = tanh(Wt@[ht, ct])\n",
        "    attention_vector = self.Wt(cell_output_and_context_vector)\n",
        "\n",
        "    # logits = softmax(Ws@at)\n",
        "    logits = self.Ws(attention_vector)\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), (hidden_dec_state, cell_dec_state)"
      ],
      "metadata": {
        "id": "V_-Lef2CqUW2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Test the decoder stack\n",
        "\n",
        "The decoder will take as input:\n",
        "1. `new_tokens`: the last token generated of shape `(batch_size, 1)`, namely the token obrained in the previous time step of the decoder (we will initialize the decoder with the `\"<sos>\"` token);\n",
        "2. `enc_output`: this is the representation produced by the `Encoder` of shape `(batch_size, max_length_context, enc_units)`;\n",
        "3. `mask`: this is the mask, that is a boolean tensor, indicating which tokens will be considered in the decoding of shape `(batch_size, max_length_context)`; \n",
        "4. `decoder_state`: the previous state of the decoder, namely the internal state of the decoder's LSTM (the paper suggests to input the hidden and cell state produced by the Bi-LSTM). The shape is `[(batch_size, enc_units), (batch_size, enc_units)]`.\n",
        "\n"
      ],
      "metadata": {
        "id": "IF5J42g1l-be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_config['question_vocab_size'] = len(word_to_idx_question[1])\n",
        "decoder_config['max_length_question'] = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "decoder = Decoder(**decoder_config, embedding_matrix=embedding_matrix_question)"
      ],
      "metadata": {
        "id": "kS0UBnMzTbie"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "start_tag_index = word_to_idx_question[2]['<sos>']\n",
        "first_token = tf.squeeze(tf.constant([[start_tag_index]] * decoder_config['batch_size']), axis=1)"
      ],
      "metadata": {
        "id": "KeMvqDnrTkf0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, decoder_state = decoder(\n",
        "    inputs = DecoderInput(first_token, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = encoder_state\n",
        ")\n",
        "\n",
        "hidden_dec_state, cell_dec_state = decoder_state\n",
        "\n",
        "print(f'Logits shape: (batch_size, t, output_vocab_size) {decoder_result.logits.shape}')\n",
        "print(f'Hidden state shape: (batch_size, dec_units) {hidden_dec_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, dec_units) {cell_dec_state.shape}')"
      ],
      "metadata": {
        "id": "BF6PWsNYfmUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0825f1-b47d-46d4-899c-2a385c68b968"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (batch_size, t, output_vocab_size) (64, 1, 12672)\n",
            "Hidden state shape: (batch_size, dec_units) (64, 600)\n",
            "Cell state shape: (batch_size, dec_units) (64, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we cannot provide a detailed summary or a handy plot due to the fact that we pass to the decoder model a structured input which is not preferred by tensorflow."
      ],
      "metadata": {
        "id": "T9FZVz2QyLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AJWwcDacEj",
        "outputId": "72eed81d-3dc5-40cc-80a3-b72296bfcfc4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_embedding_layer (Em  multiple                 3801900   \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " decoder_lstm_layer (LSTM)   multiple                  2162400   \n",
            "                                                                 \n",
            " bahdanau_attention (Bahdana  multiple                 720600    \n",
            " uAttention)                                                     \n",
            "                                                                 \n",
            " decoder_Wt_weights (Dense)  multiple                  720000    \n",
            "                                                                 \n",
            " decoder_Ws_weights (Dense)  multiple                  7603200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,008,100\n",
            "Trainable params: 11,206,200\n",
            "Non-trainable params: 3,801,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on: this means that the decoder will produce a vector of probabilities associated to each vocabulary word. That is, a vector of logits $l_b \\in \\mathbb{R}^{\\mathcal{V}}$ for each element $b$ in the batch, namely indicating the next probable token for a given sentence. Since they are logits they should sum up to `1.0`, evenutally a number really close to it. "
      ],
      "metadata": {
        "id": "BBIQDE0Sl6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result.logits[0, 0, :].numpy().sum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO-C7ELdlv1u",
        "outputId": "0b0a3a0c-9359-4665-bf92-d986a140505d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we sample a token according to the logits computed by the decoder."
      ],
      "metadata": {
        "id": "xrN_dTRtGdQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :],\n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "vocab = np.array(list(word_to_idx_question[1].keys()))\n",
        "\n",
        "first_word = list(vocab[tf.squeeze(sampled_tokens, axis=-1).numpy()])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "kGGwivobvx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc623876-70cc-4035-8786-cab6d3519e90"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['indentured', 'mutliracial', 'providing', 'requiring', 'hisham']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, _ = decoder(\n",
        "    inputs = DecoderInput(sampled_tokens, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = decoder_state\n",
        ")\n",
        "\n",
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :], \n",
        " \n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "sampled_tokens = tf.squeeze(sampled_tokens, axis=-1).numpy()\n",
        "\n",
        "first_word = list(vocab[sampled_tokens])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "Y2ixRaJZn271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04400f05-c818-4cd0-bed3-8d3eb9a1d24a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['complaining', 'pcs', 'older', 'cameron', 'dukkha']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training for QG"
      ],
      "metadata": {
        "id": "qIoySQKuIGlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Loss\n",
        "\n",
        "The **QG** task is defined as finding $\\hat{y}$ such that:\n",
        "$$\n",
        "\\hat{y} = \\arg{\\max_y P(y|x)}  \n",
        "$$\n",
        "where $P(y|x)$ is the conditional log-likelihood of the predicted question sentence $y$ given the input $x$. Du et al. shown that the conditional probability could be factorized in:\n",
        "$$\n",
        "P(y|x) = \\prod_{t=1}^{|y|} P(y_t|x, y_{<t})\n",
        "$$\n",
        "where the probability of each $y_t$ is predicted based on all the words that have been generated upon time $t$, namely $y_{<t}$.\n",
        "\n",
        "This means that given a training corpus of sentence-question pairs $\\mathcal{S} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the objective is to minimize the negative log-likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L} &= - \\sum_{i=1}^N \\log P(y^{(i)}|x^{(i)}; \\theta)\\\\\n",
        "            &=  - \\sum_{i=1}^N \\sum_{j=1}^{|y^{(i)}|} \\log P (y_j^{(i)}|x^{(i)}, y_{<j}^{(i)}; \\theta)\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "qyRA2RxZNsx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'conditional_ll_loss'\n",
        "\n",
        "    # The loss needs to work with logits since the decoder is outputting the most probable token\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True,\n",
        "        reduction='none'\n",
        "    )\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch\n",
        "    # Shape of y_true = (batch_size, max_length_question)\n",
        "    # Shape of y_pred = (batch_size, max_length_question, vocab_size)\n",
        "    loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    \n",
        "    # Mask of the losses on the padding\n",
        "    mask = tf.math.not_equal(y_true, 0)\n",
        "    loss = tf.boolean_mask(loss, mask)\n",
        "    loss = tf.reduce_sum(loss)\n",
        "\n",
        "    # Return the total\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IP_UunM3MUtF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 QG model and training step implementation\n",
        "\n",
        "The training step should:\n",
        "1. Run the encoder on the `input_tokens` to get the `encoder_outputs`, `hidden_state` and `cell_state`. "
      ],
      "metadata": {
        "id": "iwLiPsCyNuor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorTrainer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               context_vocab_size,\n",
        "               question_vocab_size,\n",
        "               embedding_dimension,\n",
        "               embedding_matrix_context,\n",
        "               embedding_matrix_question,\n",
        "               units,\n",
        "               batch_size,\n",
        "               max_length_context,\n",
        "               max_length_question,\n",
        "               use_tf_function=True):\n",
        "    \"\"\"\n",
        "    Prepare the model for the training. It builds the both the encoder and the decoder.\n",
        "    Also it defines a wrapper to use the tf.function compilation for the tensorflow computational\n",
        "    graph.\n",
        "    \"\"\"\n",
        "    self.max_length_question = max_length_question\n",
        "\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(\n",
        "        context_vocab_size,\n",
        "        embedding_matrix_context,\n",
        "        embedding_dimension,\n",
        "        units,\n",
        "        batch_size,\n",
        "        max_length_context)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        question_vocab_size,\n",
        "        embedding_matrix_question,\n",
        "        embedding_dimension,\n",
        "        units, \n",
        "        batch_size,\n",
        "        max_length_question)\n",
        "    \n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self.tf_train_step(inputs)\n",
        "    else:\n",
        "      return self.train_step(inputs)\n",
        "\n",
        "  @tf.function\n",
        "  def tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # Extract context and target\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      encoder_output, encoder_state = self.encoder(context)\n",
        "\n",
        "      # The decoder should be initialized with the encoder last state \n",
        "      decoder_state = encoder_state\n",
        "      loss = tf.constant(0.0)\n",
        "      t = 0\n",
        "\n",
        "      # Reference :- https://www.tensorflow.org/guide/function\n",
        "      # We have to run the decoder for all the length of the question \n",
        "      while t < (self.max_length_question - 1):\n",
        "        # We have to pass two tokens:\n",
        "        #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "        #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "        new_token = tf.gather(question, t, axis=1)\n",
        "        target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "        step_loss, decoder_state = self.step_decoder(\n",
        "            (new_token, target_token),\n",
        "            context_mask,\n",
        "            encoder_output,\n",
        "            decoder_state)\n",
        "\n",
        "        loss = loss + step_loss\n",
        "        t = t + 1\n",
        "\n",
        "      # Average the loss for all the legit tokens\n",
        "      avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "\n",
        "    # Apply an optimization step\n",
        "    tr_variables = self.trainable_variables\n",
        "    grads = tape.gradient(avg_loss, tr_variables)\n",
        "\n",
        "    n_vars = len(tr_variables)\n",
        "    \n",
        "    # Apply some clipping (by norm) as written in the paper\n",
        "    grads = [tf.clip_by_norm(grads[i], 5.0) for i in range(n_vars)]\n",
        "    self.optimizer.apply_gradients(zip(grads, tr_variables))\n",
        "\n",
        "    return {f'batch_loss': avg_loss}\n",
        "\n",
        "  @tf.function\n",
        "  def step_decoder(self, \n",
        "                    tokens, \n",
        "                    context_mask, \n",
        "                    encoder_output, \n",
        "                    decoder_state):\n",
        "    \"\"\"\n",
        "    Run a single iteration of the decoder and computers the incremental loss between the\n",
        "    produced token and the token in the target input.\n",
        "\n",
        "    \"\"\"\n",
        "    new_token, target_token = tokens\n",
        "    \n",
        "    # Run the decoder one time\n",
        "    decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=context_mask),\n",
        "        state = decoder_state)\n",
        "  \n",
        "    y_true = target_token\n",
        "    y_pred = decoder_result.logits\n",
        "\n",
        "    step_loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    return step_loss, decoder_state\n",
        "\n",
        "  def __get_mask(self, tokens): return tf.math.not_equal(tokens, 0)"
      ],
      "metadata": {
        "id": "8xSn_StMq9cx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_model = QGeneratorTrainer(**encoder_config,\n",
        "                             question_vocab_size=decoder_config['question_vocab_size'],\n",
        "                             max_length_question=decoder_config['max_length_question'],\n",
        "                             embedding_matrix_context=embedding_matrix_context,\n",
        "                             embedding_matrix_question=embedding_matrix_question,\n",
        "                             use_tf_function=True)\n",
        "\n",
        "trainer_config['loss'] = MaskedLoss()\n",
        "\n",
        "qg_model.compile(\n",
        "    optimizer=trainer_config['optimizer'],\n",
        "    loss=trainer_config['loss']\n",
        ")"
      ],
      "metadata": {
        "id": "zwKc0rvrIWkg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 Simple Training\n",
        "The first call with `use_tf_function=True` will be slow since it has to trace the function. So be patient or try `use_tf_function=False` 😀"
      ],
      "metadata": {
        "id": "SPSDqU3_3nOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataset.train))\n",
        "qg_model.train_step(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf5JzL9T47Ha",
        "outputId": "e1386c04-acd2-4d0a-d31d-e2482cbb74be"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.800503>}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "losses = []\n",
        "for n in tqdm(range(10)):\n",
        "  # print('.', end='')\n",
        "  logs = qg_model.train_step(next(iter(dataset.train)))\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)\n",
        "print()\n",
        "print(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "E0ZatZ4g-5iq",
        "outputId": "b04995af-6b23-41f8-b3c6-999ceb24aead"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:09<00:00,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[8.773858, 8.758519, 8.785645, 8.805987, 8.810041, 8.796323, 8.806666, 8.789956, 8.826391, 8.786362]\n",
            "CPU times: user 4.8 s, sys: 391 ms, total: 5.19 s\n",
            "Wall time: 9.39 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Checkpoints\n",
        "\n",
        "See [Manual Checkpointing](https://www.tensorflow.org/guide/checkpoint)."
      ],
      "metadata": {
        "id": "x6uDOVpginU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_DIR = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"tf_ckpt\")"
      ],
      "metadata": {
        "id": "gEMKO8azio4k"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_iterator = iter(dataset.train) \n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "manager = tf.train.CheckpointManager(checkpoint,\n",
        "                                     checkpoint_prefix,\n",
        "                                     max_to_keep=3)"
      ],
      "metadata": {
        "id": "5tNJR_fRi7qd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Train the model"
      ],
      "metadata": {
        "id": "fF8BfykX5faH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key) -> None:\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_ends(self, n, logs):\n",
        "    self.logs.append(logs[self.key])"
      ],
      "metadata": {
        "id": "uhSLln405mfy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_loss = BatchLogs('batch_loss')\n",
        "#history = qg_model.fit(dataset.train, epochs=trainer_config['epochs'], callbacks=[batch_loss], verbose='auto')"
      ],
      "metadata": {
        "id": "ys97OwVn61UT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Inference for QG\n",
        "In this section we will provide the class and the methods for the inference part. More specifically, both auxiliary and inferencing methods:\n",
        "1. `token_to_string()`:\n",
        "2. `string_to_token()`:\n",
        "3. `create_mask()`:\n",
        "4. `temperature_sampling()`:\n",
        "5. `generate_question()`:"
      ],
      "metadata": {
        "id": "ezgR7c68_0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorInference(tf.Module):\n",
        "  def __init__(self, \n",
        "               encoder, \n",
        "               decoder, \n",
        "               tokenizer_question, \n",
        "               word_to_idx, \n",
        "               use_tf_function):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.tokenizer_question = tokenizer_question\n",
        "    self.word_to_idx_question = word_to_idx\n",
        "   \n",
        "    self.result_tokens = None\n",
        "    self.result_text = None\n",
        "    self.token_mask = self.create_mask()\n",
        "\n",
        "    self.start_idx = self.word_to_idx_question['<sos>']\n",
        "    self.end_idx = self.word_to_idx_question['<eos>']\n",
        "    self.use_tf_function = False\n",
        "\n",
        "  def token_to_string(self, result_tokens: tf.Tensor):  \n",
        "    \"\"\"\n",
        "    This method converts token IDs to text by using a given mapping.\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    list_text = self.tokenizer_question.sequences_to_texts(list_tokens)\n",
        "    list_text = tf.convert_to_tensor([list_text])\n",
        "    result_text = tf.strings.reduce_join(list_text, axis=0, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    \n",
        "    self.result_tokens = result_tokens\n",
        "    self.result_text = result_text\n",
        "    return result_text\n",
        "\n",
        "  def string_to_token(self, result_str: tf.Tensor):\n",
        "    \"\"\"\n",
        "    This method converts texts to token IDs by using a given mapping.\n",
        "    \"\"\"  \n",
        "    list_str = [s.decode(\"utf-8\") for s in result_str.numpy().tolist()]\n",
        "    list_tokens = self.tokenizer_question.texts_to_sequences(list_str)\n",
        "    list_tokens = tf.convert_to_tensor(list_tokens, dtype=tf.int64)\n",
        "    result_tokens = tf.squeeze(tf.split(list_tokens, num_or_size_splits=list_tokens.shape[0], axis=0), axis=1)\n",
        "\n",
        "    return result_tokens\n",
        "  \n",
        "  def create_mask(self):\n",
        "    \"\"\"\n",
        "    This method creates a mask for the padding, the unknwon words and the start/ending tokens.\n",
        "    \"\"\"\n",
        "    masked_words = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "    token_mask_ids = [self.word_to_idx_question[mask] for mask in masked_words]\n",
        "\n",
        "    token_mask = np.zeros(shape=(len(self.word_to_idx_question),), dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    return token_mask\n",
        "\n",
        "  def temperature_sampling(self, logits, temperature=0.5):\n",
        "    \"\"\"\n",
        "\n",
        "    For the temperature choice see here:\n",
        "      Reference :- https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "    \"\"\"\n",
        "    # First of all we use broadcast the generated mask to the expected logits' shape\n",
        "    # token_mask shape: (batch_size, timestep, vocab_size)\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # The logits for all the tokens that have to not be used are set top -1.0\n",
        "    logits = tf.where(token_mask, -1.0, logits)\n",
        "\n",
        "    # Freezing function\n",
        "    # Higher temperature -> greater variety\n",
        "    # Lower temperature -> grammatically correct\n",
        "    if temperature == 0.0:\n",
        "      # the freezing function is the argmax\n",
        "      new_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      # the freezing function now scales the logits.\n",
        "      # for temperature == 1.0 is the identity function\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_token = tf.random.categorical(logits/temperature, num_samples=1)\n",
        "    return new_token\n",
        "\n",
        "  def predict(self, inputs, max_length, return_attention):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_generate_question(inputs, max_length, return_attention)\n",
        "    else:\n",
        "      return self._generate_question(inputs, max_length, return_attention)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_generate_question(self, inputs):\n",
        "    return self._generate_question(inputs)\n",
        "\n",
        "  \"\"\"\n",
        "  def _beam_search(self, k, max_length, encoder_output, encoder_state):\n",
        "    decoder_states = list()\n",
        "    for i in range(k):\n",
        "      decoder_states.append(encoder_state)\n",
        "    decoders = []\n",
        "    for i in range(k):\n",
        "      decoder = copy.deepcopy(self.decoder)\n",
        "      decoders.append(decoder)\n",
        "    sequences = [[list(), 0.0]]\n",
        "    timestep = 0\n",
        "    first_token = True\n",
        "    new_token = [self.start_idx]*k\n",
        "    while timestep < max_length:\n",
        "      timestep = timestep+1\n",
        "      all_candidates = list()\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        decoder_result, decoder_state = decoders[i](\n",
        "          inputs = DecoderInput(\n",
        "              new_token=new_tokens[i],\n",
        "              enc_output=encoder_output,\n",
        "              mask=(inputs != 0)),\n",
        "          state = decoder_states[i]))\n",
        "        decoder_states[i] = decoder_state\n",
        "        for j in range(decoder.logits.shape[-1])\n",
        "          candidate = [seq + [j], score - np.log(decoder.logits[0][j])]\n",
        "          all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "\t\t  ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      for i in range(k):\n",
        "        new_tokens.append(all_candidates[i][0][-1])\n",
        "        # select k best\n",
        "\t\t  sequences = ordered[:k]\n",
        "\n",
        "  def _generate_question_beam_search(self, inputs, max_length, return_attention=True):\n",
        "    \n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    new_token = tf.fill([batch_size, 1], self.start_idx)\n",
        "    batch_sequences = []\n",
        "    for i in range(batch_size):\n",
        "      sequences = self._beam_search(3, max_length, encoder_output[i], encoder_state[i])\n",
        "      batch_sequences.append(sequences)\n",
        "    return batch_sequences\n",
        "  \"\"\"  \n",
        "  def _generate_question(self, \n",
        "                        inputs,\n",
        "                        max_length,\n",
        "                        return_attention=True,\n",
        "                        temperature=0.5):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    # Similarly for what it has been done in the train step\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    # Generate the first token of each sentence, that is the <sos> token\n",
        "    new_token = tf.fill([batch_size, 1], self.start_idx)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    timestep = 0\n",
        "    \n",
        "    while timestep < max_length:\n",
        "      timestep = timestep + 1\n",
        "      \n",
        "      # Decode the token at the next timestep\n",
        "      decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=(inputs != 0)),\n",
        "        state = decoder_state)\n",
        "      \n",
        "      attention.append(decoder_result.attention_weights)\n",
        "\n",
        "      # Sample the new token accordingly to the distribution produced by the decoder\n",
        "      new_token = self.temperature_sampling(decoder_result.logits, temperature)\n",
        "\n",
        "      # if a sequence has reached <eos> set it as done\n",
        "      \n",
        "      result_tokens.append(new_token)\n",
        "    \n",
        "    #\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.token_to_string(result_tokens)\n",
        "\n",
        "    attention_stack = tf.concat(attention, axis=-1)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}"
      ],
      "metadata": {
        "id": "gn2NTxAq_2sy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator = QGeneratorInference(qg_model.encoder, \n",
        "                                   qg_model.decoder, \n",
        "                                   dataset_creator.tokenizer_question, \n",
        "                                   word_to_idx=word_to_idx_question[1], \n",
        "                                   use_tf_function=True)"
      ],
      "metadata": {
        "id": "1Q2pWJ5YDfqi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_logits = tf.random.normal([5, 1, len(word_to_idx_question[1])])\n",
        "example_output_tokens = qg_generator.temperature_sampling(example_logits, temperature=1.0)"
      ],
      "metadata": {
        "id": "fklYEd1e6XOb"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator.predict(example_output_tokens, max_length=10, return_attention=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RO-7QCL_MSc",
        "outputId": "e3f4e74f-13e1-4b90-b3fd-8f6023f42ad7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
              " array([b'bombycilla defaulted beheading incentives multitasking natives zhiping maturity trace monument',\n",
              "        b'democratic primordialist orbital humvee chicago jerry conference meets lacquer harper',\n",
              "        b'assigned reappointed ii platonic reclamation bond touch begun hostelries ingaas',\n",
              "        b'travels sills regulations limit vibhushan justifications napoleon francisco geograhical centric',\n",
              "        b'regulating scanner roles mountain criteria accountability mentioned cavities neuron promoted'],\n",
              "       dtype=object)>}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}