{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wjVfZgIIf1RV",
        "x6uDOVpginU0",
        "fF8BfykX5faH",
        "ezgR7c68_0nv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erhtric/NeuralQuestionGenerationNLP/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main file: its purpouse is to collect all the code coming from the coding pipeline."
      ],
      "metadata": {
        "id": "8OU-wpGL18xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorflow-addons\n",
        "#!pip install -q \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "id": "Htbwd0W_Y9IC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "import re\n",
        "import os\n",
        "import typing\n",
        "from typing import Any, Tuple, List, NamedTuple\n",
        "import spacy\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "import seaborn as sns\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from keras.layers import (\n",
        "    Layer, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    LSTMCell,\n",
        "    Dense, \n",
        "    Bidirectional, \n",
        "    Input, \n",
        "    AdditiveAttention)\n",
        "\n",
        "import nltk\n",
        "from nltk import punkt, pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvONYDvKAHz",
        "outputId": "0be706c4-292e-4ddb-f0e8-9bdbb83dfdf9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "CIZdy1hp14x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63057ba1-8f68-42d3-95f2-5efc007ec47a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commands to prepare the folder to accomodate data."
      ],
      "metadata": {
        "id": "UqKVWPel_ybt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP/Project/Testing folder/Eric\n",
        "%pwd\n",
        "%mkdir data\n",
        "%mkdir training_checkpoints\n",
        "\n",
        "# disable chained assignments to avoid annoying warning\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKVGy7JPJCoi",
        "outputId": "dca6944c-9edc-41f4-a46c-f240045363eb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cVw6eUwM-dRL9BhqtXULyOqeXDrYkwmH/NLP/Project/Testing folder/Eric\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘training_checkpoints’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print({tf.__version__})"
      ],
      "metadata": {
        "id": "-ePFW3UcrVtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9308256c-a01b-4411-f441-9576834caa2c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'2.8.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for the `configuration.json` file: "
      ],
      "metadata": {
        "id": "2hyzKyj_-GjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "units = 600\n",
        "\n",
        "dataset_config = {\n",
        "    # 'num_examples': 18896,\n",
        "    'num_examples': 9000,\n",
        "    'num_words_context': 45000,\n",
        "    'num_words_question': 28000,\n",
        "    'buffer_size': 32000,\n",
        "    'batch_size': batch_size,\n",
        "    'random_seed': 13,\n",
        "}\n",
        "\n",
        "encoder_config = {\n",
        "    'context_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_context': None\n",
        "}\n",
        "\n",
        "decoder_config = {\n",
        "    'question_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_question': None,\n",
        "}\n",
        "\n",
        "trainer_config = {\n",
        "    'epochs': 3,\n",
        "    'optimizer': tf.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "}\n",
        "\n",
        "path = {\n",
        "    'training_json_path': \"./data/training_set.json\",\n",
        "    'save_pkl_path': \"./data/squadv2.pkl\"\n",
        "}"
      ],
      "metadata": {
        "id": "5bS3uLkE-Mvf"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data handling and Pre-processing\n",
        "\n",
        "\n",
        "Things to do:\n",
        "1. Add to each sentence $x$ a start of sequence `<SOS>` tag and end of sequence `<EOS>` tag,\n",
        "2. Clean the sentences by removing special chars,\n",
        "3. Perform other preprocessing steps,\n",
        "4. Create a **vocabulary** with a word-to-index and index-to-word mappings by using a **tokenizer**, \n",
        "5. Extract the sentences that contain an answer and use them as input features, whereas the question will be our target\n",
        "6. Pad each context to maximum length.\n",
        "\n",
        "The resulting data that will be used hereinafter will be of type `tf.data.Dataset`. "
      ],
      "metadata": {
        "id": "sFBKCIE3Jxf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(NamedTuple):\n",
        "  \"\"\"\n",
        "  This class represent a a 3-way split processed dataset. \n",
        "  \"\"\"\n",
        "  # Reference :- https://github.com/topper-123/Articles/blob/master/New-interesting-data-types-in-Python3.rst\n",
        "  train: tf.data.Dataset\n",
        "  val: tf.data.Dataset\n",
        "  test: tf.data.Dataset\n",
        "\n",
        "class SQuAD:\n",
        "  def __init__(self):\n",
        "    self.random_seed = None\n",
        "    self.squad_df = None\n",
        "    self.preproc_squad_df = None\n",
        "    self.tokenizer = None\n",
        "    self.buffer_size = 0\n",
        "    self.batch_size = 0\n",
        "\n",
        "  def __call__(self,\n",
        "           num_examples, \n",
        "           buffer_size, \n",
        "           batch_size, \n",
        "           random_seed,\n",
        "           training_json_path,\n",
        "           save_pkl_path,\n",
        "           num_words_context=None,\n",
        "           num_words_question=None,\n",
        "           tokenized=True,\n",
        "           pos_ner_tag=True,\n",
        "           tensor_type=True):\n",
        "    \"\"\"The call() method loads the SQuAD dataset, preprocess it and optionally it returns \n",
        "    it tokenized. Moreover it also perform a 3-way split.\n",
        "\n",
        "    Args:\n",
        "        num_examples (int): number of examples to be taken from the original SQuAD dataset\n",
        "        num_words (int): the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. \n",
        "        buffer_size (int): buffer size for the shuffling operation\n",
        "        batch_size (int): size of the batches\n",
        "        tokenized (boolean): specifies if the context and question data should be both tokenized\n",
        "        pos_ner_tag (boolean):\n",
        "        tensro_type (boolean): \n",
        "\n",
        "    Returns (depending on the input parameters):\n",
        "        pd.DataFrame: training dataset\n",
        "        pd.DataFrame: validation dataset\n",
        "        pd.DataFrame: testing dataset\n",
        "          OR\n",
        "        NamedTuple: dataset, (dict, dict, dict)\n",
        "    \"\"\"\n",
        "    self.random_seed = random_seed\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "    self.training_json_path = training_json_path\n",
        "    self.save_pkl_path = save_pkl_path\n",
        "    self.pos_ner_tag = pos_ner_tag\n",
        "\n",
        "    # Load dataset from file\n",
        "    self.load_dataset(num_examples)\n",
        "    # Extract answer\n",
        "    self.extract_answer()\n",
        "    # Preprocess context and question\n",
        "    self.preprocess()\n",
        "    \n",
        "    # Perform splitting\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.split_train_val(self.preproc_squad_df)\n",
        "    \n",
        "    if self.pos_ner_tag: \n",
        "      pass\n",
        "\n",
        "    # Initialize Tokenizer for the source: in our case the context phrases\n",
        "    # alternatively TextVectorization \n",
        "    self.tokenizer_context = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_context)\n",
        "    # initialize also for the target, namely the question phrases\n",
        "    self.tokenizer_question = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_question)\n",
        "\n",
        "    if tokenized:\n",
        "      X_train_tokenized, word_to_idx_train_context = self.__tokenize_context(X_train)\n",
        "      y_train_tokenized, word_to_idx_train_question = self.__tokenize_question(y_train)\n",
        "\n",
        "      X_val_tokenized, word_to_idx_val_context = self.__tokenize_context(X_val)\n",
        "      y_val_tokenized, word_to_idx_val_question = self.__tokenize_question(y_val)\n",
        "\n",
        "      X_test_tokenized, word_to_idx_test_context = self.__tokenize_context(X_test)\n",
        "      y_test_tokenized, word_to_idx_test_question = self.__tokenize_question(y_test)\n",
        "\n",
        "      word_to_idx_context = (word_to_idx_train_context, word_to_idx_val_context, word_to_idx_test_context)\n",
        "      word_to_idx_question = (word_to_idx_train_question, word_to_idx_val_question, word_to_idx_test_question)\n",
        "      \n",
        "      if tensor_type:\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "        # Returns tf.Data.Dataset objects (tokenized)\n",
        "        train_dataset = self.to_tensor(X_train_tokenized, y_train_tokenized)\n",
        "        val_dataset = self.to_tensor(X_val_tokenized, y_val_tokenized)\n",
        "        test_dataset = self.to_tensor(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "        # Configure the dataset for performance\n",
        "        train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        dataset = Dataset(\n",
        "            train=train_dataset, \n",
        "            val=val_dataset,\n",
        "            test=test_dataset)\n",
        "\n",
        "        return dataset, word_to_idx_context, word_to_idx_question\n",
        "      else:\n",
        "        # Returns pd.DataFrame objects (tokenized)\n",
        "        return X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized\n",
        "    else:\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def load_dataset(self, num_examples):\n",
        "    \"\"\"\n",
        "    Extract the dataset from the json file. Already grouped by title.\n",
        "\n",
        "    :param path: [Optional] specifies the local path where the training_set.json file is located\n",
        "\n",
        "    :return\n",
        "        - the extracted dataset in a dataframe format\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.save_pkl_path):\n",
        "      print('File already exists! Loading from .pkl...\\n')\n",
        "      self.squad_df = pd.read_pickle(self.save_pkl_path)\n",
        "      self.squad_df = self.squad_df[:num_examples]\n",
        "    else:\n",
        "      print('Loading from .json...\\n')\n",
        "      with open(self.training_json_path) as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      df_array = []\n",
        "      for current_subject in data['data']:\n",
        "          title = current_subject['title']\n",
        "\n",
        "          for current_context in current_subject['paragraphs']:\n",
        "              context = current_context['context']\n",
        "\n",
        "              for current_question in current_context['qas']:\n",
        "                  question = current_question['question']\n",
        "                  id = current_question['id']\n",
        "\n",
        "              for answer_text in current_question['answers']:\n",
        "                    answer = answer_text['text']\n",
        "                    answer_start = answer_text['answer_start']\n",
        "                    record = { \"id\": id,\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"answer_start\": answer_start,\n",
        "                                \"answer\": answer\n",
        "                                }\n",
        "\n",
        "              df_array.append(record)\n",
        "      \n",
        "      # Save file\n",
        "      pd.to_pickle(pd.DataFrame(df_array), self.save_pkl_path)\n",
        "      self.squad_df = pd.DataFrame(df_array)[:num_examples]\n",
        "\n",
        "  def preprocess(self):\n",
        "    df = self.squad_df.copy()\n",
        "\n",
        "    # Pre-processing context\n",
        "    context = list(df.context)\n",
        "    preproc_context = []\n",
        "\n",
        "    for c in context:\n",
        "      c = self.__preprocess_sentence(c, question=False)\n",
        "      preproc_context.append(c)\n",
        "    \n",
        "    df.context = preproc_context\n",
        "\n",
        "    # Pre-processing questions\n",
        "    question = list(df.question)\n",
        "    preproc_question = []\n",
        "\n",
        "    for q in question:\n",
        "      q = self.__preprocess_sentence(q, question=True)\n",
        "      preproc_question.append(q)\n",
        "    \n",
        "    df.question = preproc_question\n",
        "\n",
        "    # Remove features that are not useful\n",
        "    df = df.drop(['id'], axis=1)\n",
        "    self.preproc_squad_df = df\n",
        "\n",
        "  def __preprocess_sentence(self, sen, question):\n",
        "    # Creating a space between a word and the punctuation following it\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    sen = re.sub(r\"([?.!,¿])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sen = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", sen)\n",
        "\n",
        "    sen = sen.strip()\n",
        "\n",
        "    # Adding a start and an end token to the sentence so that the model know when to \n",
        "    # start and stop predicting.\n",
        "    # if not question: sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    return sen\n",
        "\n",
        "  def __answer_start_end(self, df):\n",
        "    \"\"\"\n",
        "    Creates a list of starting indexes and ending indexes for the answers.\n",
        "\n",
        "    :param df: the target Dataframe\n",
        "\n",
        "    :return: a dataframe containing the start and the end indexes foreach answer (ending index is excluded).\n",
        "\n",
        "    \"\"\"\n",
        "    start_idx = df.answer_start\n",
        "    end_idx = [start + len(list(answer)) for start, answer in zip(list(start_idx), list(df.answer))]\n",
        "    return pd.DataFrame(list(zip(start_idx, end_idx)), columns=['start', 'end'])\n",
        "\n",
        "  def split_train_val(self, df, train_size=0.8):\n",
        "    \"\"\"\n",
        "    This method splits the dataframe in training and test sets, or eventually, in training, validation and test sets.\n",
        "\n",
        "    Args\n",
        "        :param df: the target Dataframe\n",
        "        :param random_seed: random seed used in the splits\n",
        "        :param train_size: represents the absolute number of train samples\n",
        "        :param val: boolean for choosing between a 3-way split or 2-way one.\n",
        "\n",
        "    Returns:\n",
        "        - Data and labels for training, validation and test sets if val is True \n",
        "        - Data and labels for training and test sets if val is False \n",
        "\n",
        "    \"\"\"\n",
        "    # Maybe we have also to return the index for the starting answer\n",
        "    X = df.drop(['answer_start', 'question', 'answer'], axis=1).copy()\n",
        "    idx = self.__answer_start_end(df)\n",
        "    X['start'] = idx['start']\n",
        "    X['end'] = idx['end']\n",
        "    y = df['question']\n",
        "\n",
        "    # In the first step we will split the data in training and remaining dataset\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X, groups=X['title'])\n",
        "    train_idx, rem_idx = next(split)\n",
        "\n",
        "    X_train = X.iloc[train_idx]\n",
        "    y_train = y.iloc[train_idx]\n",
        "    X_rem = X.iloc[rem_idx]\n",
        "    y_rem = y.iloc[rem_idx]\n",
        "\n",
        "\n",
        "    # Val and test test accounts for 10% of the total data. Both 5%.\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X_rem, groups=X_rem['title'])\n",
        "    val_idx, test_idx = next(split)\n",
        "\n",
        "    X_val = X_rem.iloc[val_idx]\n",
        "    y_val = y_rem.iloc[val_idx]\n",
        "\n",
        "    X_test = X_rem.iloc[test_idx]\n",
        "    y_test = y_rem.iloc[test_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def __tokenize_context(self, X):\n",
        "    context = X.context\n",
        "    self.tokenizer_context.fit_on_texts(context)\n",
        "    context_tf = self.tokenizer_context.texts_to_sequences(context)\n",
        "\n",
        "    # context_lengths = [len(seq) for seq in context_tf]\n",
        "    # sns.boxplot(context_lengths)\n",
        "\n",
        "    context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(context):\n",
        "      X['context'].iloc[i] = context_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_context.word_index['<pad>'] = 0\n",
        "    self.tokenizer_context.index_word[0] = '<pad>'\n",
        "\n",
        "    return X, self.tokenizer_context.word_index\n",
        "\n",
        "  def __tokenize_question(self, y):\n",
        "    question = y\n",
        "    self.tokenizer_question.fit_on_texts(question)\n",
        "    question_tf = self.tokenizer_question.texts_to_sequences(question)\n",
        "\n",
        "    # question_lengths = [len(seq) for seq in question_tf]\n",
        "    # sns.boxplot(question_lengths)\n",
        "    \n",
        "    # See also tf.data.Dataset.padding_batch\n",
        "    question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(question):\n",
        "      y.iloc[i] = question_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_question.word_index['<pad>'] = 0\n",
        "    self.tokenizer_question.index_word[0] = '<pad>'\n",
        "\n",
        "    return y, self.tokenizer_question.word_index\n",
        "\n",
        "  def extract_answer(self):\n",
        "    df = self.squad_df.copy()\n",
        "    start_end = self.__answer_start_end(df)\n",
        "    context = list(df.context)\n",
        "    \n",
        "    selected_sentences = []\n",
        "    for i, par in enumerate(context):\n",
        "      sentences = sent_tokenize(par)\n",
        "      start = start_end.iloc[i].start\n",
        "      end = start_end.iloc[i].end      \n",
        "      right_sentence = \"\"\n",
        "      context_characters = 0\n",
        "\n",
        "      for j, sen in enumerate(sentences):\n",
        "        sen += ' '\n",
        "        context_characters += len(sen)\n",
        "        # If the answer is completely in the current sentence\n",
        "        if(start < context_characters and end <= context_characters):\n",
        "          right_sentence = sen\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break\n",
        "        # the answer is in both the current and the next sentence\n",
        "        if(start < context_characters and end > context_characters):\n",
        "          right_sentence = sen + sentences[j+1]\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break \n",
        "\n",
        "    self.squad_df.context = selected_sentences\n",
        "\n",
        "  def to_tensor(self, X, y):\n",
        "    X = X.context.copy()\n",
        "    y = y.copy()\n",
        "\n",
        "    # Reference:- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(list(X), tf.int64), \n",
        "         tf.cast(list(y), tf.int64)))\n",
        "    dataset = dataset.shuffle(self.buffer_size).batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "XyCKxqwRZelj"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the `SQuAD` constructor we create a dataset handling object which will be useful for future operations."
      ],
      "metadata": {
        "id": "iTXVdOGcZSCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_creator = SQuAD()"
      ],
      "metadata": {
        "id": "RfCYdZofJ866"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preprocessed untokenized split"
      ],
      "metadata": {
        "id": "ZgeJckqZIufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                                                                       num_words=None,\n",
        "#                                                                       BUFFER_SIZE=32000,\n",
        "#                                                                       BATCH_SIZE=64,\n",
        "#                                                                       random_seed=RANDOM_SEED,\n",
        "#                                                                       tokenized=False)\n",
        "\n",
        "# print(f'Set target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')\n",
        "\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator(**dataset_config, **path, tokenized=False)"
      ],
      "metadata": {
        "id": "TJFUuu2Y5hc-"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Tokenized split"
      ],
      "metadata": {
        "id": "dndR7_CNI1jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Tensor Ready\n",
        "\n",
        "This is the data produced that we are most interested in. As we can see we will have:\n",
        "- a data structure `dataset` containing the training, validation and test set;\n",
        "- a tuple containing the word-to-token mappings for the training, validation and test set respectively."
      ],
      "metadata": {
        "id": "MvU7n1LboA6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "%%time\n",
        "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(**dataset_config, **path, tokenized=True)\n",
        "\n",
        "max_length_context = dataset.train.element_spec[0].shape[1]\n",
        "max_length_question = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "print(f'Sentences max lenght: {max_length_context}')\n",
        "print(f'Questions max lenght: {max_length_question}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax999y7aI75Y",
        "outputId": "0bc97833-c988-4ce9-8d19-91c64cbb2c62"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists! Loading from .pkl...\n",
            "\n",
            "Sentences max lenght: 389\n",
            "Questions max lenght: 40\n",
            "CPU times: user 10.9 s, sys: 434 ms, total: 11.3 s\n",
            "Wall time: 10.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing such `NamedTuple` data structure (cfr `dataset`) is pretty simple, namely in a:\n",
        "1. tuple-way by accessing it like a list, e.g. `train = dataset[0]`,\n",
        "2. object-way by calling the instance parameters, e.g. `train = dataset.train`.\n",
        "\n",
        "The other two returned values are the word to index mappings for the context and question words respectively. In order to refer to a specific split simply call:\n",
        "1. for the training dataset,\n",
        "2. for the validation dataset,\n",
        "3. for the test dataset,"
      ],
      "metadata": {
        "id": "W7hARM_R2Kod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training vocab size for the context: {len(word_to_idx_context[0])}')\n",
        "print(f'Training vocab size for the question: {len(word_to_idx_question[0])}')\n",
        "print()\n",
        "print(f'Validation vocab size for the context: {len(word_to_idx_context[1])}')\n",
        "print(f'Validation vocab size for the question: {len(word_to_idx_question[1])}')\n",
        "print()\n",
        "print(f'Test vocab size for the context: {len(word_to_idx_context[2])}')\n",
        "print(f'Test vocab size for the question: {len(word_to_idx_question[2])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaRYgawxyXp",
        "outputId": "1d771be6-57ca-4a7c-9f94-23309266e148"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size for the context: 23465\n",
            "Training vocab size for the question: 11342\n",
            "\n",
            "Validation vocab size for the context: 26136\n",
            "Validation vocab size for the question: 12672\n",
            "\n",
            "Test vocab size for the context: 26567\n",
            "Test vocab size for the question: 12892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Standard"
      ],
      "metadata": {
        "id": "hlpy-ayWoEHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                      BUFFER_SIZE=32000,\n",
        "#                      BATCH_SIZE=64,\n",
        "#                      random_seed=RANDOM_SEED,\n",
        "#                      tokenized=True,\n",
        "#                      tensor_type=False)\n",
        "\n",
        "# print(f'\\nSet target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "vJFBk-r8eIcj"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Original SQuAD dataset"
      ],
      "metadata": {
        "id": "r7qxjGzKJM2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original dataset\n",
        "# squad_df = dataset_creator.squad_df\n",
        "# print(f'[Info] SQuAD target: {list(squad_df.columns.values)}')\n",
        "# print(f'[Info] Shape: {squad_df.shape}')"
      ],
      "metadata": {
        "id": "9qhTAc5NErOk"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. GloVe and embedding matrix"
      ],
      "metadata": {
        "id": "kx2f7Nn_4en9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVe:\n",
        "  def __init__(self, embedding_dimension):\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "\n",
        "    try:\n",
        "      self.embedding_model = KeyedVectors.load(f'./data/glove_model_{self.embedding_dimension}')\n",
        "    except FileNotFoundError:\n",
        "      print('[Warning] Model not found in local folder, please wait...')\n",
        "      self.embedding_model = self.load_glove()\n",
        "      self.embedding_model.save(f'./data/glove_model_{self.embedding_dimension}')  \n",
        "      print('Download finished. Model loaded!')\n",
        "\n",
        "  def load_glove(self):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained GloVe embedding model via gensim library.\n",
        "\n",
        "    We have a matrix that associate words to a vector of a user-defined dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(self.embedding_dimension)\n",
        "\n",
        "    try:\n",
        "      emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "      print(\"Generic error when loading GloVe\")\n",
        "      print(\"Check embedding dimension\")\n",
        "      raise e\n",
        "\n",
        "    emb_model = gloader.load(download_path)\n",
        "    return emb_model\n",
        "\n",
        "  def build_embedding_matrix(self, word_to_idx, vocab_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the \n",
        "        dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, self.embedding_dimension), dtype=np.float32)\n",
        "    oov_count = 0\n",
        "    oov_words = []\n",
        "\n",
        "    # For each word which is not present in the vocabulary we assign a random vector, otherwise we take the GloVe embedding\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      try:\n",
        "        embedding_vector = self.embedding_model[word]\n",
        "      except (KeyError, TypeError):\n",
        "        oov_count += 1\n",
        "        oov_words.append(word)\n",
        "        embedding_vector = np.random.uniform(low=-0.05, \n",
        "                                             high=0.05, \n",
        "                                             size=self.embedding_dimension)\n",
        "\n",
        "      embedding_matrix[idx] = embedding_vector\n",
        "    \n",
        "    print(f'\\n[Debug] {oov_count} OOV words found!\\n')\n",
        "    return embedding_matrix, oov_words"
      ],
      "metadata": {
        "id": "TRJ1NpSMqaJL"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initialize the handler with the desidered `embedding_dimension`. Then to build the embedding matrix with the pre-trained GloVe embeddings simply call the `build_embedding_matrix` method."
      ],
      "metadata": {
        "id": "gk-z8A5y3cpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Initalize the handler for GloVe\n",
        "glove_handler = GloVe(encoder_config['embedding_dimension'])\n",
        "\n",
        "# We will create the matrix by using only the words present in the training and validation set\n",
        "embedding_matrix_context, oov_words_context = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_context[1], \n",
        "    len(word_to_idx_context[1])+1)\n",
        "\n",
        "embedding_matrix_question, oov_words_question = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_question[1], \n",
        "    len(word_to_idx_question[1])+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUmvdCWGavSR",
        "outputId": "7e65ea43-6e45-47b1-a4ec-951b65e369a2"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26136/26136 [00:00<00:00, 131563.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 1706 OOV words found!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12672/12672 [00:00<00:00, 241876.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 652 OOV words found!\n",
            "\n",
            "CPU times: user 1.11 s, sys: 1.27 s, total: 2.38 s\n",
            "Wall time: 2.52 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert both of them into tensor, but it is fine to also treat them as `numpy` array, still it is better to use the `tensorflow` fundamentals."
      ],
      "metadata": {
        "id": "qdLOk0pQu3Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_context = tf.convert_to_tensor(embedding_matrix_context)\n",
        "embedding_matrix_question = tf.convert_to_tensor(embedding_matrix_question)"
      ],
      "metadata": {
        "id": "EX6PvKTBsdSW"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Definition"
      ],
      "metadata": {
        "id": "FF5Rtd4uqa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_context_batch, example_question_batch = next(iter(dataset.train))"
      ],
      "metadata": {
        "id": "GjdRysIvPoLc"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Encoder\n",
        "We will use a bidirectional LSTM to encode the sentence,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\overrightarrow{b_t} &= \\overrightarrow{\\text{LSTM}}(x_t, \\overrightarrow{b_{t-1}})\\\\\n",
        "\\overleftarrow{b_t} &= \\overleftarrow{\\text{LSTM}}(x_t, \\overleftarrow{b_{t+1}})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\overrightarrow{b_t}$ is the hidden state at time step $t$ for the forward pass LSTM and $\\overleftarrow{b_t}$ for the backward pass."
      ],
      "metadata": {
        "id": "wjVfZgIIf1RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               context_vocab_size, \n",
        "               embedding_matrix,\n",
        "               embedding_dimension, \n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_context,\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_context = tf.constant(max_length_context)\n",
        "\n",
        "    # Layers\n",
        "    self.inputs = Input(shape=(self.max_length_context,), name='encoder_input')\n",
        "    \n",
        "    self.embedding = Embedding(input_dim=context_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_context,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,\n",
        "                               mask_zero=False,\n",
        "                               name='encoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM forward pass\n",
        "    self.forward_lstm_layer = LSTM(units//2,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform',\n",
        "                                  use_bias=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "    \n",
        "    # The LSTM backward pass\n",
        "    self.backward_lstm_layer = LSTM(units//2,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  go_backwards=True,\n",
        "                                  recurrent_initializer='glorot_uniform',\n",
        "                                  use_bias=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "    \n",
        "    # The Bidirectional wrapper\n",
        "    self.bidirectional_lstm = Bidirectional(self.forward_lstm_layer, \n",
        "                                            backward_layer=self.backward_lstm_layer, \n",
        "                                            name='encoder_bi_lstm',\n",
        "                                            merge_mode='concat')\n",
        "    \n",
        "  def call(self, inputs, state=None, training=True):\n",
        "    # x shape = (batch_size, max_length_context, embedding_dimension)\n",
        "    x = self.embedding(inputs)\n",
        "\n",
        "    # encoder_outputs shape = (batch_size, max_length_context, units)\n",
        "    # forward_h shape = (batch_size, units//2)\n",
        "    # forward_c shape = (batch_size, units//2)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm(x, \n",
        "                                                                                            initial_state=state)\n",
        "    # op:concat shape = (batch_size, units)\n",
        "    h_concat = tf.concat([forward_h, backward_h], axis=1, name='hidden_concat')\n",
        "    c_concat = tf.concat([forward_c, backward_c], axis=1, name='cell_concat')\n",
        "    return encoder_outputs, (h_concat, c_concat)\n",
        "\n",
        "  # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    x = Input(shape=(self.max_length_context,), batch_size=self.batch_size)\n",
        "    return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(), \n",
        "        to_file='encoder.png', dpi=96,              \n",
        "        show_shapes=True, show_layer_names=True,  \n",
        "        expand_nested=False                       \n",
        "    )"
      ],
      "metadata": {
        "id": "GK6Kd1XvqK22"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Test the encoder stack\n",
        "\n"
      ],
      "metadata": {
        "id": "fbjSxPGcFud_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_config['context_vocab_size'] = len(word_to_idx_context[1])\n",
        "encoder_config['max_length_context'] = dataset.train.element_spec[0].shape[1]\n",
        "\n",
        "encoder = Encoder(**encoder_config, embedding_matrix=embedding_matrix_context)\n",
        "encoder_outputs, encoder_state = encoder(inputs=example_context_batch)\n",
        "\n",
        "hidden_state, cell_state = encoder_state\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, max_length_context, units): {encoder_outputs.shape}')\n",
        "print(f'Hidden state shape: (batch_size, units): {hidden_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, units): {cell_state.shape}')"
      ],
      "metadata": {
        "id": "_ffteDMQyzmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6b4ff6-243c-434b-de50-00830ffe8fee"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, max_length_context, units): (64, 389, 600)\n",
            "Hidden state shape: (batch_size, units): (64, 600)\n",
            "Cell state shape: (batch_size, units): (64, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.build_graph().summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng97TFpLRym2",
        "outputId": "88b42712-17de-44f4-ac1b-69be4188feb4"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_14 (InputLayer)          [(64, 389)]          0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding_layer (Embed  (64, 389, 300)      7841100     ['input_14[0][0]']               \n",
            " ding)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_bi_lstm (Bidirectional  [(64, 389, 600),    1442400     ['encoder_embedding_layer[0][0]']\n",
            " )                               (64, 300),                                                       \n",
            "                                 (64, 300),                                                       \n",
            "                                 (64, 300),                                                       \n",
            "                                 (64, 300)]                                                       \n",
            "                                                                                                  \n",
            " tf.concat_22 (TFOpLambda)      (64, 600)            0           ['encoder_bi_lstm[0][1]',        \n",
            "                                                                  'encoder_bi_lstm[0][3]']        \n",
            "                                                                                                  \n",
            " tf.concat_23 (TFOpLambda)      (64, 600)            0           ['encoder_bi_lstm[0][2]',        \n",
            "                                                                  'encoder_bi_lstm[0][4]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,283,500\n",
            "Trainable params: 1,442,400\n",
            "Non-trainable params: 7,841,100\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "aaJFf-ELdIZo",
        "outputId": "6f5ae0d2-0424-4630-8d1b-be302f9fbced"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Decoder"
      ],
      "metadata": {
        "id": "dtM9nOQrf3jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Container classes\n",
        "# Reference :- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "class DecoderInput(NamedTuple):\n",
        "  new_token: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "73wq7CTiTQpX"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(Layer):\n",
        "  # Still it is possible to use Luang's attention as an alternative\n",
        "  # Reference:- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.W1 = Dense(units, use_bias=False, name='Wb1_attention_weights')\n",
        "    self.W2 = Dense(units, use_bias=False, name='Wb2_attention_weights')\n",
        "\n",
        "    self.attention = AdditiveAttention(use_scale=True)\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    \"\"\"\n",
        "    This layer takes 3 inputs:\n",
        "      - the query; this will be generated by the decoder, later,\n",
        "      - the value: the output of the encoder,\n",
        "      - the mask: to exclude the padding, i.e., context_batch != 0.\n",
        "    \"\"\"\n",
        "    #W1@ht\n",
        "    w1_query = self.W1(query)\n",
        "    #W2@hs\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask = [query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    \n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               question_vocab_size, \n",
        "               embedding_matrix, \n",
        "               embedding_dimension,\n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_question,\n",
        "               **kwargs):\n",
        "    \n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_question = tf.constant(max_length_question)\n",
        "    self.embedding_dimension = tf.constant(embedding_dimension)\n",
        "    self.units = tf.constant(units)\n",
        "\n",
        "    # Layers definition\n",
        "    self.inputs = Input(shape=(None,), batch_size=self.batch_size)\n",
        "                        \n",
        "    # Embedding for the questions\n",
        "    self.embedding = Embedding(input_dim=question_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_question,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,  #?\n",
        "                               mask_zero=False,\n",
        "                               name='decoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM layer\n",
        "    self.lstm_layer = LSTM(units,\n",
        "                          return_sequences=True,\n",
        "                          return_state=True,\n",
        "                          recurrent_initializer='glorot_uniform',\n",
        "                          use_bias=True,\n",
        "                          input_shape=(self.max_length_question, embedding_dimension),\n",
        "                          name='decoder_lstm_layer')\n",
        "    \n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(units)\n",
        "\n",
        "    # Parameters to be learned\n",
        "    self.Wt = Dense(units, activation=tf.math.tanh, use_bias=False, name='decoder_Wt_weights')\n",
        "\n",
        "    # For the word probabilities\n",
        "    # self.Ws = Dense(self.dec_units, activation=tf.nn.softmax, use_bias=False)\n",
        "    self.Ws = Dense(question_vocab_size, activation=tf.nn.softmax, use_bias=False, name='decoder_Ws_weights')\n",
        "\n",
        "  def call(self, \n",
        "            inputs: DecoderInput, \n",
        "            state=None) -> Tuple[DecoderOutput, Tuple[tf.Tensor]]:\n",
        "\n",
        "    # Lookup the embeddings for the questions\n",
        "    x = self.embedding(inputs.new_token)\n",
        "    # embedded_tensor shape: (batch_size, 1, embedding_dimension)\n",
        "    if tf.shape(x).shape == 2: x = tf.expand_dims(x, axis=1)\n",
        "\n",
        "    # Process one step with the RNN\n",
        "    # LSTM expects inputs of shape: (batch_size, timestep, feature)\n",
        "    cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(x, initial_state=state)\n",
        "\n",
        "    # Use the LSTM cell output as the query for the attention over the encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=cell_output, \n",
        "        value=inputs.enc_output, \n",
        "        mask=inputs.mask)\n",
        "\n",
        "    # Join the context_vector and cell outpyt [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    cell_output_and_context_vector = tf.concat([cell_output, context_vector], axis=-1)\n",
        "\n",
        "    # at = tanh(Wt@[ht, ct])\n",
        "    attention_vector = self.Wt(cell_output_and_context_vector)\n",
        "\n",
        "    # logits = softmax(Ws@at)\n",
        "    logits = self.Ws(attention_vector)\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), (hidden_dec_state, cell_dec_state)"
      ],
      "metadata": {
        "id": "V_-Lef2CqUW2"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Test the decoder stack\n",
        "\n",
        "The decoder will take as input:\n",
        "1. `new_tokens`: the last token generated of shape `(batch_size, 1)`, namely the token obrained in the previous time step of the decoder (we will initialize the decoder with the `\"<sos>\"` token);\n",
        "2. `enc_output`: this is the representation produced by the `Encoder` of shape `(batch_size, max_length_context, enc_units)`;\n",
        "3. `mask`: this is the mask, that is a boolean tensor, indicating which tokens will be considered in the decoding of shape `(batch_size, max_length_context)`; \n",
        "4. `decoder_state`: the previous state of the decoder, namely the internal state of the decoder's LSTM (the paper suggests to input the hidden and cell state produced by the Bi-LSTM). The shape is `[(batch_size, enc_units), (batch_size, enc_units)]`.\n",
        "\n"
      ],
      "metadata": {
        "id": "IF5J42g1l-be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_config['question_vocab_size'] = len(word_to_idx_question[1])\n",
        "decoder_config['max_length_question'] = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "decoder = Decoder(**decoder_config, embedding_matrix=embedding_matrix_question)"
      ],
      "metadata": {
        "id": "kS0UBnMzTbie"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "start_tag_index = word_to_idx_question[2]['<sos>']\n",
        "first_token = tf.squeeze(tf.constant([[start_tag_index]] * decoder_config['batch_size']), axis=1)"
      ],
      "metadata": {
        "id": "KeMvqDnrTkf0"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, decoder_state = decoder(\n",
        "    inputs = DecoderInput(first_token, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = encoder_state\n",
        ")\n",
        "\n",
        "hidden_dec_state, cell_dec_state = decoder_state\n",
        "\n",
        "print(f'Logits shape: (batch_size, t, output_vocab_size) {decoder_result.logits.shape}')\n",
        "print(f'Hidden state shape: (batch_size, dec_units) {hidden_dec_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, dec_units) {cell_dec_state.shape}')"
      ],
      "metadata": {
        "id": "BF6PWsNYfmUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31746e8d-40d9-49d4-d850-35be83a3988c"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (batch_size, t, output_vocab_size) (64, 1, 12672)\n",
            "Hidden state shape: (batch_size, dec_units) (64, 600)\n",
            "Cell state shape: (batch_size, dec_units) (64, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we cannot provide a detailed summary or a handy plot due to the fact that we pass to the decoder model a structured input which is not preferred by tensorflow."
      ],
      "metadata": {
        "id": "T9FZVz2QyLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AJWwcDacEj",
        "outputId": "8be3ca8d-f31f-4f20-d2a4-747d9dc631f7"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_embedding_layer (Em  multiple                 3801900   \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " decoder_lstm_layer (LSTM)   multiple                  2162400   \n",
            "                                                                 \n",
            " bahdanau_attention_2 (Bahda  multiple                 720600    \n",
            " nauAttention)                                                   \n",
            "                                                                 \n",
            " decoder_Wt_weights (Dense)  multiple                  720000    \n",
            "                                                                 \n",
            " decoder_Ws_weights (Dense)  multiple                  7603200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,008,100\n",
            "Trainable params: 11,206,200\n",
            "Non-trainable params: 3,801,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on: this means that the decoder will produce a vector of probabilities associated to each vocabulary word. That is, a vector of logits $l_b \\in \\mathbb{R}^{\\mathcal{V}}$ for each element $b$ in the batch, namely indicating the next probable token for a given sentence. Since they are logits they should sum up to `1.0`, evenutally a number really close to it. "
      ],
      "metadata": {
        "id": "BBIQDE0Sl6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result.logits[0, 0, :].numpy().sum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO-C7ELdlv1u",
        "outputId": "f89004b3-a221-41b5-83b3-c339f4a53cc1"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we sample a token according to the logits computed by the decoder."
      ],
      "metadata": {
        "id": "xrN_dTRtGdQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :],\n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "vocab = np.array(list(word_to_idx_question[1].keys()))\n",
        "\n",
        "first_word = list(vocab[tf.squeeze(sampled_tokens, axis=-1).numpy()])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "kGGwivobvx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f1397b-5a0d-4c71-e908-f625fe730aeb"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sell', 'april', 'inexpensive', 'inspiration', 'summerwood']"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, _ = decoder(\n",
        "    inputs = DecoderInput(sampled_tokens, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = decoder_state\n",
        ")\n",
        "\n",
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :], \n",
        " \n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "sampled_tokens = tf.squeeze(sampled_tokens, axis=-1).numpy()\n",
        "\n",
        "first_word = list(vocab[sampled_tokens])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "Y2ixRaJZn271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9f08be-b5ba-4fb2-bf4f-5c408a99d649"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greenhouse', 'closests', 'shield', 'jinrong', 'paintings']"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training for QG"
      ],
      "metadata": {
        "id": "qIoySQKuIGlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Loss\n",
        "\n",
        "The **QG** task is defined as finding $\\hat{y}$ such that:\n",
        "$$\n",
        "\\hat{y} = \\arg{\\max_y P(y|x)}  \n",
        "$$\n",
        "where $P(y|x)$ is the conditional log-likelihood of the predicted question sentence $y$ given the input $x$. Du et al. shown that the conditional probability could be factorized in:\n",
        "$$\n",
        "P(y|x) = \\prod_{t=1}^{|y|} P(y_t|x, y_{<t})\n",
        "$$\n",
        "where the probability of each $y_t$ is predicted based on all the words that have been generated upon time $t$, namely $y_{<t}$.\n",
        "\n",
        "This means that given a training corpus of sentence-question pairs $\\mathcal{S} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the objective is to minimize the negative log-likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L} &= - \\sum_{i=1}^N \\log P(y^{(i)}|x^{(i)}; \\theta)\\\\\n",
        "            &=  - \\sum_{i=1}^N \\sum_{j=1}^{|y^{(i)}|} \\log P (y_j^{(i)}|x^{(i)}, y_{<j}^{(i)}; \\theta)\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "qyRA2RxZNsx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'conditional_ll_loss'\n",
        "\n",
        "    # The loss needs to work with logits since the decoder is outputting the most probable token\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True,\n",
        "        reduction='none'\n",
        "    )\n",
        "\n",
        "  def __call__(self, y_true, y_pred, previous_true=None, previous_pred=None):\n",
        "    # Calculate the loss for each item in the batch\n",
        "    # Shape of y_true = (batch_size, max_length_question)\n",
        "    # Shape of y_pred = (batch_size, max_length_question, vocab_size)\n",
        "    loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    \n",
        "    # Mask of the losses on the padding\n",
        "    mask = tf.math.not_equal(y_true, 0)\n",
        "    loss = tf.boolean_mask(loss, mask)\n",
        "    loss = tf.reduce_sum(loss)\n",
        "\n",
        "    # Return the total\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IP_UunM3MUtF"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 QG model and training step implementation\n",
        "\n",
        "The training step should:\n",
        "1. Run the encoder on the `input_tokens` to get the `encoder_outputs`, `hidden_state` and `cell_state`. "
      ],
      "metadata": {
        "id": "iwLiPsCyNuor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorTrainer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               context_vocab_size,\n",
        "               question_vocab_size,\n",
        "               embedding_dimension,\n",
        "               embedding_matrix_context,\n",
        "               embedding_matrix_question,\n",
        "               units,\n",
        "               batch_size,\n",
        "               max_length_context,\n",
        "               max_length_question,\n",
        "               use_tf_function=True):\n",
        "    \"\"\"\n",
        "    Prepare the model for the training. It builds the both the encoder and the decoder.\n",
        "    Also it defines a wrapper to use the tf.function compilation for the tensorflow computational\n",
        "    graph.\n",
        "    \"\"\"\n",
        "    self.max_length_question = max_length_question\n",
        "\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(\n",
        "        context_vocab_size,\n",
        "        embedding_matrix_context,\n",
        "        embedding_dimension,\n",
        "        units,\n",
        "        batch_size,\n",
        "        max_length_context)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        question_vocab_size,\n",
        "        embedding_matrix_question,\n",
        "        embedding_dimension,\n",
        "        units, \n",
        "        batch_size,\n",
        "        max_length_question)\n",
        "    \n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self.tf_train_step(inputs)\n",
        "    else:\n",
        "      return self.train_step(inputs)\n",
        "\n",
        "  @tf.function\n",
        "  def tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # Extract context and target\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      encoder_output, encoder_state = self.encoder(context)\n",
        "\n",
        "      # The decoder should be initialized with the encoder last state \n",
        "      decoder_state = encoder_state\n",
        "      loss = tf.constant(0.0)\n",
        "      t = 0\n",
        "\n",
        "      # Reference :- https://www.tensorflow.org/guide/function\n",
        "      # We have to run the decoder for all the length of the question \n",
        "      while t < self.max_length_question - 1:\n",
        "        # We have to pass two tokens:\n",
        "        #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "        #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "        new_token = tf.gather(question, t, axis=1)\n",
        "        target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "        step_loss, decoder_state = self.step_decoder(\n",
        "            (new_token, target_token),\n",
        "            context_mask,\n",
        "            encoder_output,\n",
        "            decoder_state)\n",
        "\n",
        "        loss = loss + step_loss\n",
        "        t = t + 1\n",
        "\n",
        "      # Average the loss for all the legit tokens\n",
        "      avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "\n",
        "    # Apply an optimization step\n",
        "    tr_variables = self.trainable_variables\n",
        "    grads = tape.gradient(avg_loss, tr_variables)\n",
        "\n",
        "    n_vars = len(tr_variables)\n",
        "    \n",
        "    # Apply some clipping (by norm) as written in the paper\n",
        "    grads = [tf.clip_by_norm(grads[i.numpy()], 5.0) for i in tf.range(n_vars)]\n",
        "    self.optimizer.apply_gradients(zip(grads, tr_variables))\n",
        "\n",
        "    return {f'batch_loss': avg_loss}\n",
        "\n",
        "  @tf.function\n",
        "  def step_decoder(self, \n",
        "                    tokens, \n",
        "                    context_mask, \n",
        "                    encoder_output, \n",
        "                    decoder_state):\n",
        "    \"\"\"\n",
        "    Run a single iteration of the decoder and computers the incremental loss between the\n",
        "    produced token and the token in the target input.\n",
        "\n",
        "    \"\"\"\n",
        "    new_token, target_token = tokens\n",
        "    \n",
        "    # Run the decoder one time\n",
        "    decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=context_mask),\n",
        "        state = decoder_state)\n",
        "  \n",
        "    y_true = target_token\n",
        "    y_pred = decoder_result.logits\n",
        "\n",
        "    step_loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    return step_loss, decoder_state\n",
        "\n",
        "  def __get_mask(self, tokens): return tf.math.not_equal(tokens, 0)"
      ],
      "metadata": {
        "id": "8xSn_StMq9cx"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_model = QGeneratorTrainer(**encoder_config,\n",
        "                             question_vocab_size=decoder_config['question_vocab_size'],\n",
        "                             max_length_question=decoder_config['max_length_question'],\n",
        "                             embedding_matrix_context=embedding_matrix_context,\n",
        "                             embedding_matrix_question=embedding_matrix_question,\n",
        "                             use_tf_function=True)\n",
        "\n",
        "trainer_config['loss'] = MaskedLoss()\n",
        "\n",
        "qg_model.compile(\n",
        "    optimizer=trainer_config['optimizer'],\n",
        "    loss=trainer_config['loss']\n",
        ")"
      ],
      "metadata": {
        "id": "zwKc0rvrIWkg"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 Simple Training\n",
        "The first call with `use_tf_function=True` will be slow since it has to trace the function. So be patient or try `use_tf_function=False` 😀"
      ],
      "metadata": {
        "id": "SPSDqU3_3nOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataset.train))\n",
        "qg_model.train_step(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf5JzL9T47Ha",
        "outputId": "191416b4-5096-4fef-b886-5502d0ed4a16"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.759303>}"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "losses = []\n",
        "for n in tqdm(range(10)):\n",
        "  # print('.', end='')\n",
        "  logs = qg_model.train_step(next(iter(dataset.train)))\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)\n",
        "print()\n",
        "print(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "E0ZatZ4g-5iq",
        "outputId": "25e6a6b3-517a-49e7-b8a7-a798016d125d"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [01:35<14:19, 95.50s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-243-d48aea0747eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"losses = []\\nfor n in tqdm(range(10)):\\n  # print('.', end='')\\n  logs = qg_model.train_step(next(iter(dataset.train)))\\n  losses.append(logs['batch_loss'].numpy())\\n\\nprint()\\nplt.plot(losses)\\nprint()\\nprint(losses)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-e03e686edcdc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Apply an optimization step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtr_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mn_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1206\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m       return backward._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m-> 1208\u001b[0;31m           processed_args, remapped_captures)\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Checkpoints\n",
        "\n",
        "See [Manual Checkpointing](https://www.tensorflow.org/guide/checkpoint)."
      ],
      "metadata": {
        "id": "x6uDOVpginU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_DIR = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"tf_ckpt\")"
      ],
      "metadata": {
        "id": "gEMKO8azio4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_iterator = iter(dataset.train) \n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "manager = tf.train.CheckpointManager(checkpoint,\n",
        "                                     checkpoint_prefix,\n",
        "                                     max_to_keep=3)"
      ],
      "metadata": {
        "id": "5tNJR_fRi7qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Train the model"
      ],
      "metadata": {
        "id": "fF8BfykX5faH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key) -> None:\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_ends(self, n, logs):\n",
        "    self.logs.append(logs[self.key])"
      ],
      "metadata": {
        "id": "uhSLln405mfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_loss = BatchLogs('batch_loss')\n",
        "history = qg_model.fit(dataset.train, epochs=trainer_config['epochs'], callbacks=[batch_loss], verbose='auto')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys97OwVn61UT",
        "outputId": "49e920fe-7c25-45f9-f545-6245fd44e58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "112/112 [==============================] - 124s 1s/step - batch_loss: 9.4343\n",
            "Epoch 2/3\n",
            "112/112 [==============================] - 119s 1s/step - batch_loss: 9.3608\n",
            "Epoch 3/3\n",
            "112/112 [==============================] - 119s 1s/step - batch_loss: 9.3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Inference for QG\n",
        "In this section we will provide the class and the methods for the inference part. More specifically, both auxiliary and inferencing methods:\n",
        "1. `token_to_string()`:\n",
        "2. `string_to_token()`:\n",
        "3. `create_mask()`:\n",
        "4. `temperature_sampling()`:\n",
        "5. `generate_question()`:"
      ],
      "metadata": {
        "id": "ezgR7c68_0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorInference(tf.Module):\n",
        "  def __init__(self, encoder, decoder, tokenizer, word_to_idx, use_tf_function):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_idx = word_to_idx\n",
        "   \n",
        "    self.result_tokens = None\n",
        "    self.result_text = None\n",
        "    self.token_mask = self.create_mask()\n",
        "\n",
        "    self.start_idx = word_to_idx['<sos>']\n",
        "    self.end_idx = word_to_idx['<eos>']\n",
        "    self.use_tf_function = False\n",
        "\n",
        "  def token_to_string(self, result_tokens: tf.Tensor):  \n",
        "    \"\"\"\n",
        "    This method converts token IDs to text by using a given mapping.\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    list_text = self.tokenizer.sequences_to_texts(list_tokens)\n",
        "    list_text = tf.convert_to_tensor([list_text])\n",
        "    result_text = tf.strings.reduce_join(list_text, axis=0, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    \n",
        "    self.result_tokens = result_tokens\n",
        "    self.result_text = result_text\n",
        "    return result_text\n",
        "\n",
        "  def string_to_token(self, result_str: tf.Tensor):\n",
        "    \"\"\"\n",
        "    This method converts texts to token IDs by using a given mapping.\n",
        "    \"\"\"  \n",
        "    list_str = [s.decode(\"utf-8\") for s in result_str.numpy().tolist()]\n",
        "    list_tokens = self.tokenizer.texts_to_sequences(list_str)\n",
        "    list_tokens = tf.convert_to_tensor(list_tokens, dtype=tf.int64)\n",
        "    result_tokens = tf.squeeze(tf.split(list_tokens, num_or_size_splits=list_tokens.shape[0], axis=0), axis=1)\n",
        "\n",
        "    return result_tokens\n",
        "  \n",
        "  def create_mask(self):\n",
        "    \"\"\"\n",
        "    This method creates a mask for the padding, the unknwon words and the start/ending tokens.\n",
        "    \"\"\"\n",
        "    masked_words = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "    token_mask_ids = [self.tokenizer.word_index[mask] for mask in masked_words]\n",
        "\n",
        "    token_mask = np.zeros(shape=(len(self.word_to_idx),), dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    return token_mask\n",
        "\n",
        "  def temperature_sampling(self, logits, temperature=0.5):\n",
        "    \"\"\"\n",
        "\n",
        "    For the temperature choice see here:\n",
        "      Reference :- https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "    \"\"\"\n",
        "    # First of all we use broadcast the generated mask to the expected logits' shape\n",
        "    # token_mask shape: (batch_size, timestep, vocab_size)\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # The logits for all the tokens that have to not be used are set top -1.0\n",
        "    logits = tf.where(token_mask, -1.0, logits)\n",
        "\n",
        "    # Freezing function\n",
        "    # Higher temperature -> greater variety\n",
        "    # Lower temperature -> grammatically correct\n",
        "    if temperature == 0.0:\n",
        "      # the freezing function is the argmax\n",
        "      new_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      # the freezing function now scales the logits.\n",
        "      # for temperature == 1.0 is the identity function\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_token = tf.random.categorical(logits/temperature, num_samples=1)\n",
        "    return new_token\n",
        "\n",
        "  def predict(self, inputs, max_length, return_attention):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_generate_question(inputs, max_length, return_attention)\n",
        "    else:\n",
        "      return self._generate_question(inputs, max_length, return_attention)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_generate_question(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  def _generate_question(self, \n",
        "                        inputs,\n",
        "                        max_length,\n",
        "                        return_attention=True,\n",
        "                        temperature=0.5):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    # Similarly for what it has been done in the train step\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    # Generate the first token of each sentence, that is the <sos> token\n",
        "    new_token = tf.fill([batch_size, 1], self.start_idx)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    timestep = 0\n",
        "    while timestep < max_length:\n",
        "      timestep = timestep + 1\n",
        "      \n",
        "      # Decode the token at the next timestep\n",
        "      decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=(inputs != 0)),\n",
        "        state = decoder_state)\n",
        "      \n",
        "      attention.append(decoder_result.attention_weights)\n",
        "\n",
        "      # Sample the new token accordingly to the distribution produced by the decoder\n",
        "      new_token = self.temperature_sampling(decoder_result.logits, temperature)\n",
        "\n",
        "      # if a sequence has reached <eos> set it as done\n",
        "      \n",
        "      result_tokens.append(new_token)\n",
        "    \n",
        "    #\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.token_to_string(result_tokens)\n",
        "\n",
        "    attention_stack = tf.concat(attention, axis=-1)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}"
      ],
      "metadata": {
        "id": "gn2NTxAq_2sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator = QGeneratorInference(qg_model.encoder, \n",
        "                                   qg_model.decoder, \n",
        "                                   dataset_creator.tokenizer, \n",
        "                                   word_to_idx[1], \n",
        "                                   use_tf_function=True)"
      ],
      "metadata": {
        "id": "1Q2pWJ5YDfqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_logits = tf.random.normal([5, 1, len(word_to_idx[1])])\n",
        "example_output_tokens = qg_generator.temperature_sampling(example_logits, temperature=1.0)"
      ],
      "metadata": {
        "id": "fklYEd1e6XOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator.predict(example_output_tokens, max_length=10, return_attention=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RO-7QCL_MSc",
        "outputId": "5bd1b528-b49a-438a-8c5f-dbc698dfb424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
              " array([b'prelates gauthia steep hirsch unfurl neurology corsica matte gharm bayan',\n",
              "        b'narcokleptocracy mevastatin georgia hannover 1925 566 basic deboned fortuna jhanas',\n",
              "        b'nrel healthy circuit frequency troublesome aufkl malamute unleashed civics employment',\n",
              "        b'stylized stratum ascription witnessed bre subsided juan vasubandhu substituted baltasar',\n",
              "        b'prefer kilby nanjing folklife disappearing 448 teau axis pseudogenes root'],\n",
              "       dtype=object)>}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bro = [[list(), 0.0]]\n",
        "print(bro)"
      ],
      "metadata": {
        "id": "Ha1sP-HiWgeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379b2101-d29f-40be-ff6f-557804d7c1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[], 0.0]]\n"
          ]
        }
      ]
    }
  ]
}