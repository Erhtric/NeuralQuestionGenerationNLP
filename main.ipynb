{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sFBKCIE3Jxf2",
        "ZgeJckqZIufT",
        "dndR7_CNI1jq",
        "MvU7n1LboA6g",
        "hlpy-ayWoEHa",
        "r7qxjGzKJM2w",
        "kx2f7Nn_4en9",
        "wjVfZgIIf1RV",
        "dtM9nOQrf3jq",
        "x6uDOVpginU0",
        "SPSDqU3_3nOg",
        "ZC-NjKElfqNf",
        "ezgR7c68_0nv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erhtric/NeuralQuestionGenerationNLP/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main file: its purpouse is to collect all the code coming from the coding pipeline."
      ],
      "metadata": {
        "id": "8OU-wpGL18xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorflow-addons\n",
        "#!pip install -q \"tensorflow-text==2.8.*\"\n",
        "!pip install keras-nlp"
      ],
      "metadata": {
        "id": "Htbwd0W_Y9IC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c786749-1218-4f7e-8900-a5bdfb621a10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from keras-nlp) (1.0.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-nlp) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-nlp) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-nlp) (21.3)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from keras-nlp) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->keras-nlp) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-nlp) (3.0.9)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (1.46.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (14.0.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-nlp) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-nlp) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-nlp) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-nlp) (3.2.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->keras-nlp) (0.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "import re\n",
        "import os\n",
        "import typing\n",
        "from typing import Any, Tuple, List, NamedTuple\n",
        "import spacy\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "#import seaborn as sns\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from keras.layers import (\n",
        "    Layer, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    Dense, \n",
        "    Bidirectional, \n",
        "    Input, \n",
        "    AdditiveAttention)\n",
        "\n",
        "import keras_nlp\n",
        "import nltk\n",
        "#from nltk import punkt, pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvONYDvKAHz",
        "outputId": "fd991c26-8e98-42f3-8083-42ef38402611"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CIZdy1hp14x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7162b0d2-0b34-403b-8a24-ed1e124c8a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commands to prepare the folder to accomodate data."
      ],
      "metadata": {
        "id": "UqKVWPel_ybt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP/Project/Testing folder/Eric\n",
        "%pwd\n",
        "\n",
        "# disable chained assignments to avoid annoying warning\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKVGy7JPJCoi",
        "outputId": "42992141-7374-4724-a569-6ea3c0a56b10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cVw6eUwM-dRL9BhqtXULyOqeXDrYkwmH/NLP/Project/Testing folder/Eric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./data'):\n",
        "  print('Data folder does not exists. Creating it')\n",
        "  os.makedirs('./data')\n",
        "\n",
        "if not os.path.exists('./training_checkpoints'):\n",
        "  print('Training checkpoint folder does not exists. Creating it')\n",
        "  os.makedirs('./training_checkpoints')"
      ],
      "metadata": {
        "id": "ikmZGjUlitqS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print({tf.__version__})"
      ],
      "metadata": {
        "id": "-ePFW3UcrVtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9006c895-aaa0-4154-83d4-e894165534e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'2.8.2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for the `configuration.json` file, or something similar: "
      ],
      "metadata": {
        "id": "2hyzKyj_-GjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "units = 600\n",
        "\n",
        "dataset_config = {\n",
        "    # 'num_examples': 18896,\n",
        "    'num_examples': 9000,\n",
        "    'num_words_context': 45000,\n",
        "    'num_words_question': 28000,\n",
        "    'buffer_size': 32000,\n",
        "    'batch_size': batch_size,\n",
        "    'random_seed': 13,\n",
        "}\n",
        "\n",
        "encoder_config = {\n",
        "    'context_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_context': None\n",
        "}\n",
        "\n",
        "decoder_config = {\n",
        "    'question_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_question': None,\n",
        "}\n",
        "\n",
        "trainer_config = {\n",
        "    'epochs': 3,\n",
        "    'optimizer': tf.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "}\n",
        "\n",
        "path = {\n",
        "    'training_json_path': \"./data/training_set.json\",\n",
        "    'save_pkl_path': \"./data/squadv2.pkl\",\n",
        "    'checkpoint_dir': \"./training_checkpoints\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "5bS3uLkE-Mvf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data handling and Pre-processing\n",
        "\n",
        "\n",
        "Things to do:\n",
        "1. Add to each sentence $x$ a start of sequence `<SOS>` tag and end of sequence `<EOS>` tag,\n",
        "2. Clean the sentences by removing special chars,\n",
        "3. Perform other preprocessing steps,\n",
        "4. Create a **vocabulary** with a word-to-index and index-to-word mappings by using a **tokenizer**, \n",
        "5. Extract the sentences that contain an answer and use them as input features, whereas the question will be our target\n",
        "6. Pad each context to maximum length.\n",
        "\n",
        "The resulting data that will be used hereinafter will be of type `tf.data.Dataset`. "
      ],
      "metadata": {
        "id": "sFBKCIE3Jxf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(NamedTuple):\n",
        "  \"\"\"\n",
        "  This class represent a a 3-way split processed dataset. \n",
        "  \"\"\"\n",
        "  # Reference :- https://github.com/topper-123/Articles/blob/master/New-interesting-data-types-in-Python3.rst\n",
        "  train: tf.data.Dataset\n",
        "  val: tf.data.Dataset\n",
        "  test: tf.data.Dataset\n",
        "\n",
        "class SQuAD:\n",
        "  def __init__(self):\n",
        "    self.random_seed = None\n",
        "    self.squad_df = None\n",
        "    self.preproc_squad_df = None\n",
        "    self.tokenizer = None\n",
        "    self.buffer_size = 0\n",
        "    self.batch_size = 0\n",
        "\n",
        "  def __call__(self,\n",
        "           num_examples, \n",
        "           buffer_size, \n",
        "           batch_size, \n",
        "           random_seed,\n",
        "           training_json_path,\n",
        "           save_pkl_path,\n",
        "           num_words_context=None,\n",
        "           num_words_question=None,\n",
        "           tokenized=True,\n",
        "           pos_ner_tag=True,\n",
        "           tensor_type=True):\n",
        "    \"\"\"The call() method loads the SQuAD dataset, preprocess it and optionally it returns \n",
        "    it tokenized. Moreover it also perform a 3-way split.\n",
        "\n",
        "    Args:\n",
        "        num_examples (int): number of examples to be taken from the original SQuAD dataset\n",
        "        num_words (int): the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. \n",
        "        buffer_size (int): buffer size for the shuffling operation\n",
        "        batch_size (int): size of the batches\n",
        "        tokenized (boolean): specifies if the context and question data should be both tokenized\n",
        "        pos_ner_tag (boolean):\n",
        "        tensro_type (boolean): \n",
        "\n",
        "    Returns (depending on the input parameters):\n",
        "        pd.DataFrame: training dataset\n",
        "        pd.DataFrame: validation dataset\n",
        "        pd.DataFrame: testing dataset\n",
        "          OR\n",
        "        NamedTuple: dataset, (dict, dict, dict)\n",
        "    \"\"\"\n",
        "    self.random_seed = random_seed\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "    self.training_json_path = training_json_path\n",
        "    self.save_pkl_path = save_pkl_path\n",
        "    self.pos_ner_tag = pos_ner_tag\n",
        "\n",
        "    # Load dataset from file\n",
        "    self.load_dataset(num_examples)\n",
        "    # Extract answer\n",
        "    self.extract_answer()\n",
        "    # Preprocess context and question\n",
        "    self.preprocess()\n",
        "    \n",
        "    # Perform splitting\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.split_train_val(self.preproc_squad_df)\n",
        "    \n",
        "    if self.pos_ner_tag: \n",
        "      pass\n",
        "\n",
        "    # Initialize Tokenizer for the source: in our case the context phrases\n",
        "    # alternatively TextVectorization \n",
        "    self.tokenizer_context = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_context)\n",
        "    # initialize also for the target, namely the question phrases\n",
        "    self.tokenizer_question = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_question)\n",
        "\n",
        "    if tokenized:\n",
        "      X_train_tokenized, word_to_idx_train_context = self.__tokenize_context(X_train)\n",
        "      y_train_tokenized, word_to_idx_train_question = self.__tokenize_question(y_train)\n",
        "\n",
        "      X_val_tokenized, word_to_idx_val_context = self.__tokenize_context(X_val)\n",
        "      y_val_tokenized, word_to_idx_val_question = self.__tokenize_question(y_val)\n",
        "\n",
        "      X_test_tokenized, word_to_idx_test_context = self.__tokenize_context(X_test)\n",
        "      y_test_tokenized, word_to_idx_test_question = self.__tokenize_question(y_test)\n",
        "\n",
        "      word_to_idx_context = (word_to_idx_train_context, word_to_idx_val_context, word_to_idx_test_context)\n",
        "      word_to_idx_question = (word_to_idx_train_question, word_to_idx_val_question, word_to_idx_test_question)\n",
        "      \n",
        "      if tensor_type:\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "        # Returns tf.Data.Dataset objects (tokenized)\n",
        "        train_dataset = self.to_tensor(X_train_tokenized, y_train_tokenized)\n",
        "        val_dataset = self.to_tensor(X_val_tokenized, y_val_tokenized)\n",
        "        test_dataset = self.to_tensor(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "        # Configure the dataset for performance\n",
        "        train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        dataset = Dataset(\n",
        "            train=train_dataset, \n",
        "            val=val_dataset,\n",
        "            test=test_dataset)\n",
        "\n",
        "        return dataset, word_to_idx_context, word_to_idx_question\n",
        "      else:\n",
        "        # Returns pd.DataFrame objects (tokenized)\n",
        "        return X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized\n",
        "    else:\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def load_dataset(self, num_examples):\n",
        "    \"\"\"\n",
        "    Extract the dataset from the json file. Already grouped by title.\n",
        "\n",
        "    :param path: [Optional] specifies the local path where the training_set.json file is located\n",
        "\n",
        "    :return\n",
        "        - the extracted dataset in a dataframe format\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.save_pkl_path):\n",
        "      print('File already exists! Loading from .pkl...\\n')\n",
        "      self.squad_df = pd.read_pickle(self.save_pkl_path)\n",
        "      self.squad_df = self.squad_df[:num_examples]\n",
        "    else:\n",
        "      print('Loading from .json...\\n')\n",
        "      with open(self.training_json_path) as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      df_array = []\n",
        "      for current_subject in data['data']:\n",
        "          title = current_subject['title']\n",
        "\n",
        "          for current_context in current_subject['paragraphs']:\n",
        "              context = current_context['context']\n",
        "\n",
        "              for current_question in current_context['qas']:\n",
        "                  question = current_question['question']\n",
        "                  id = current_question['id']\n",
        "\n",
        "              for answer_text in current_question['answers']:\n",
        "                    answer = answer_text['text']\n",
        "                    answer_start = answer_text['answer_start']\n",
        "                    record = { \"id\": id,\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"answer_start\": answer_start,\n",
        "                                \"answer\": answer\n",
        "                                }\n",
        "\n",
        "              df_array.append(record)\n",
        "      \n",
        "      # Save file\n",
        "      pd.to_pickle(pd.DataFrame(df_array), self.save_pkl_path)\n",
        "      self.squad_df = pd.DataFrame(df_array)[:num_examples]\n",
        "\n",
        "  def preprocess(self):\n",
        "    df = self.squad_df.copy()\n",
        "\n",
        "    # Pre-processing context\n",
        "    context = list(df.context)\n",
        "    preproc_context = []\n",
        "\n",
        "    for c in context:\n",
        "      c = self.__preprocess_sentence(c, question=False)\n",
        "      preproc_context.append(c)\n",
        "    \n",
        "    df.context = preproc_context\n",
        "\n",
        "    # Pre-processing questions\n",
        "    question = list(df.question)\n",
        "    preproc_question = []\n",
        "\n",
        "    for q in question:\n",
        "      q = self.__preprocess_sentence(q, question=True)\n",
        "      preproc_question.append(q)\n",
        "    \n",
        "    df.question = preproc_question\n",
        "\n",
        "    # Remove features that are not useful\n",
        "    df = df.drop(['id'], axis=1)\n",
        "    self.preproc_squad_df = df\n",
        "\n",
        "  def __preprocess_sentence(self, sen, question):\n",
        "    # Creating a space between a word and the punctuation following it\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    sen = re.sub(r\"([?.!,¿])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sen = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", sen)\n",
        "\n",
        "    sen = sen.strip()\n",
        "\n",
        "    # Adding a start and an end token to the sentence so that the model know when to \n",
        "    # start and stop predicting.\n",
        "    # if not question: sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    return sen\n",
        "\n",
        "  def __answer_start_end(self, df):\n",
        "    \"\"\"\n",
        "    Creates a list of starting indexes and ending indexes for the answers.\n",
        "\n",
        "    :param df: the target Dataframe\n",
        "\n",
        "    :return: a dataframe containing the start and the end indexes foreach answer (ending index is excluded).\n",
        "\n",
        "    \"\"\"\n",
        "    start_idx = df.answer_start\n",
        "    end_idx = [start + len(list(answer)) for start, answer in zip(list(start_idx), list(df.answer))]\n",
        "    return pd.DataFrame(list(zip(start_idx, end_idx)), columns=['start', 'end'])\n",
        "\n",
        "  def split_train_val(self, df, train_size=0.8):\n",
        "    \"\"\"\n",
        "    This method splits the dataframe in training and test sets, or eventually, in training, validation and test sets.\n",
        "\n",
        "    Args\n",
        "        :param df: the target Dataframe\n",
        "        :param random_seed: random seed used in the splits\n",
        "        :param train_size: represents the absolute number of train samples\n",
        "        :param val: boolean for choosing between a 3-way split or 2-way one.\n",
        "\n",
        "    Returns:\n",
        "        - Data and labels for training, validation and test sets if val is True \n",
        "        - Data and labels for training and test sets if val is False \n",
        "\n",
        "    \"\"\"\n",
        "    # Maybe we have also to return the index for the starting answer\n",
        "    X = df.drop(['answer_start', 'question', 'answer'], axis=1).copy()\n",
        "    idx = self.__answer_start_end(df)\n",
        "    X['start'] = idx['start']\n",
        "    X['end'] = idx['end']\n",
        "    y = df['question']\n",
        "\n",
        "    # In the first step we will split the data in training and remaining dataset\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X, groups=X['title'])\n",
        "    train_idx, rem_idx = next(split)\n",
        "\n",
        "    X_train = X.iloc[train_idx]\n",
        "    y_train = y.iloc[train_idx]\n",
        "    X_rem = X.iloc[rem_idx]\n",
        "    y_rem = y.iloc[rem_idx]\n",
        "\n",
        "\n",
        "    # Val and test test accounts for 10% of the total data. Both 5%.\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X_rem, groups=X_rem['title'])\n",
        "    val_idx, test_idx = next(split)\n",
        "\n",
        "    X_val = X_rem.iloc[val_idx]\n",
        "    y_val = y_rem.iloc[val_idx]\n",
        "\n",
        "    X_test = X_rem.iloc[test_idx]\n",
        "    y_test = y_rem.iloc[test_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def __tokenize_context(self, X):\n",
        "    context = X.context\n",
        "    self.tokenizer_context.fit_on_texts(context)\n",
        "    context_tf = self.tokenizer_context.texts_to_sequences(context)\n",
        "\n",
        "    # context_lengths = [len(seq) for seq in context_tf]\n",
        "    # sns.boxplot(context_lengths)\n",
        "\n",
        "    context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(context):\n",
        "      X['context'].iloc[i] = context_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_context.word_index['<pad>'] = 0\n",
        "    self.tokenizer_context.index_word[0] = '<pad>'\n",
        "\n",
        "    return X, self.tokenizer_context.word_index\n",
        "\n",
        "  def __tokenize_question(self, y):\n",
        "    question = y\n",
        "    self.tokenizer_question.fit_on_texts(question)\n",
        "    question_tf = self.tokenizer_question.texts_to_sequences(question)\n",
        "\n",
        "    # question_lengths = [len(seq) for seq in question_tf]\n",
        "    # sns.boxplot(question_lengths)\n",
        "    \n",
        "    # See also tf.data.Dataset.padding_batch\n",
        "    question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(question):\n",
        "      y.iloc[i] = question_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_question.word_index['<pad>'] = 0\n",
        "    self.tokenizer_question.index_word[0] = '<pad>'\n",
        "\n",
        "    return y, self.tokenizer_question.word_index\n",
        "\n",
        "  def extract_answer(self):\n",
        "    df = self.squad_df.copy()\n",
        "    start_end = self.__answer_start_end(df)\n",
        "    context = list(df.context)\n",
        "    \n",
        "    selected_sentences = []\n",
        "    for i, par in enumerate(context):\n",
        "      sentences = sent_tokenize(par)\n",
        "      start = start_end.iloc[i].start\n",
        "      end = start_end.iloc[i].end      \n",
        "      right_sentence = \"\"\n",
        "      context_characters = 0\n",
        "\n",
        "      for j, sen in enumerate(sentences):\n",
        "        sen += ' '\n",
        "        context_characters += len(sen)\n",
        "        # If the answer is completely in the current sentence\n",
        "        if(start < context_characters and end <= context_characters):\n",
        "          right_sentence = sen\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break\n",
        "        # the answer is in both the current and the next sentence\n",
        "        if(start < context_characters and end > context_characters):\n",
        "          right_sentence = sen + sentences[j+1]\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break \n",
        "\n",
        "    self.squad_df.context = selected_sentences\n",
        "\n",
        "  def to_tensor(self, X, y):\n",
        "    X = X.context.copy()\n",
        "    y = y.copy()\n",
        "\n",
        "    # Reference:- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(list(X), tf.int64), \n",
        "         tf.cast(list(y), tf.int64)))\n",
        "    dataset = dataset.shuffle(self.buffer_size).batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "XyCKxqwRZelj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the `SQuAD` constructor we create a dataset handling object which will be useful for future operations."
      ],
      "metadata": {
        "id": "iTXVdOGcZSCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_creator = SQuAD()"
      ],
      "metadata": {
        "id": "RfCYdZofJ866"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preprocessed untokenized split"
      ],
      "metadata": {
        "id": "ZgeJckqZIufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                                                                       num_words=None,\n",
        "#                                                                       BUFFER_SIZE=32000,\n",
        "#                                                                       BATCH_SIZE=64,\n",
        "#                                                                       random_seed=RANDOM_SEED,\n",
        "#                                                                       tokenized=False)\n",
        "\n",
        "# print(f'Set target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')\n",
        "\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator(**dataset_config, **path, tokenized=False)"
      ],
      "metadata": {
        "id": "TJFUuu2Y5hc-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Tokenized split"
      ],
      "metadata": {
        "id": "dndR7_CNI1jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Tensor Ready\n",
        "\n",
        "This is the data produced that we are most interested in. As we can see we will have:\n",
        "- a data structure `dataset` containing the training, validation and test set;\n",
        "- a tuple containing the word-to-token mappings for the training, validation and test set respectively."
      ],
      "metadata": {
        "id": "MvU7n1LboA6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "%%time\n",
        "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(**dataset_config, \n",
        "                                                                     training_json_path=path['training_json_path'], \n",
        "                                                                     save_pkl_path=path['save_pkl_path'], \n",
        "                                                                     tokenized=True)\n",
        "\n",
        "max_length_context = dataset.train.element_spec[0].shape[1]\n",
        "max_length_question = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "print(f'Sentences max lenght: {max_length_context}')\n",
        "print(f'Questions max lenght: {max_length_question}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax999y7aI75Y",
        "outputId": "b7f5a8d5-0b13-4754-b1c8-1b94a4fa413e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists! Loading from .pkl...\n",
            "\n",
            "Sentences max lenght: 389\n",
            "Questions max lenght: 40\n",
            "CPU times: user 16 s, sys: 204 ms, total: 16.2 s\n",
            "Wall time: 20.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing such `NamedTuple` data structure (cfr `dataset`) is pretty simple, namely in a:\n",
        "1. tuple-way by accessing it like a list, e.g. `train = dataset[0]`,\n",
        "2. object-way by calling the instance parameters, e.g. `train = dataset.train`.\n",
        "\n",
        "The other two returned values are the word to index mappings for the context and question words respectively. In order to refer to a specific split simply call:\n",
        "1. for the training dataset,\n",
        "2. for the validation dataset,\n",
        "3. for the test dataset,"
      ],
      "metadata": {
        "id": "W7hARM_R2Kod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training vocab size for the context: {len(word_to_idx_context[0])}')\n",
        "print(f'Training vocab size for the question: {len(word_to_idx_question[0])}')\n",
        "print()\n",
        "print(f'Validation vocab size for the context: {len(word_to_idx_context[1])}')\n",
        "print(f'Validation vocab size for the question: {len(word_to_idx_question[1])}')\n",
        "print()\n",
        "print(f'Test vocab size for the context: {len(word_to_idx_context[2])}')\n",
        "print(f'Test vocab size for the question: {len(word_to_idx_question[2])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaRYgawxyXp",
        "outputId": "8b68a99f-ebe0-49cc-ff7a-e63f0d3647c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size for the context: 23465\n",
            "Training vocab size for the question: 11342\n",
            "\n",
            "Validation vocab size for the context: 26136\n",
            "Validation vocab size for the question: 12672\n",
            "\n",
            "Test vocab size for the context: 26567\n",
            "Test vocab size for the question: 12892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Standard"
      ],
      "metadata": {
        "id": "hlpy-ayWoEHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                      BUFFER_SIZE=32000,\n",
        "#                      BATCH_SIZE=64,\n",
        "#                      random_seed=RANDOM_SEED,\n",
        "#                      tokenized=True,\n",
        "#                      tensor_type=False)\n",
        "\n",
        "# print(f'\\nSet target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "vJFBk-r8eIcj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Original SQuAD dataset"
      ],
      "metadata": {
        "id": "r7qxjGzKJM2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original dataset\n",
        "# squad_df = dataset_creator.squad_df\n",
        "# print(f'[Info] SQuAD target: {list(squad_df.columns.values)}')\n",
        "# print(f'[Info] Shape: {squad_df.shape}')"
      ],
      "metadata": {
        "id": "9qhTAc5NErOk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. GloVe and embedding matrix"
      ],
      "metadata": {
        "id": "kx2f7Nn_4en9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVe:\n",
        "  def __init__(self, embedding_dimension):\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "\n",
        "    try:\n",
        "      self.embedding_model = KeyedVectors.load(f'./data/glove_model_{self.embedding_dimension}')\n",
        "    except FileNotFoundError:\n",
        "      print('[Warning] Model not found in local folder, please wait...')\n",
        "      self.embedding_model = self.load_glove()\n",
        "      self.embedding_model.save(f'./data/glove_model_{self.embedding_dimension}')  \n",
        "      print('Download finished. Model loaded!')\n",
        "\n",
        "  def load_glove(self):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained GloVe embedding model via gensim library.\n",
        "\n",
        "    We have a matrix that associate words to a vector of a user-defined dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(self.embedding_dimension)\n",
        "\n",
        "    try:\n",
        "      emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "      print(\"Generic error when loading GloVe\")\n",
        "      print(\"Check embedding dimension\")\n",
        "      raise e\n",
        "\n",
        "    emb_model = gloader.load(download_path)\n",
        "    return emb_model\n",
        "\n",
        "  def build_embedding_matrix(self, word_to_idx, vocab_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the \n",
        "        dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, self.embedding_dimension), dtype=np.float32)\n",
        "    oov_count = 0\n",
        "    oov_words = []\n",
        "\n",
        "    # For each word which is not present in the vocabulary we assign a random vector, otherwise we take the GloVe embedding\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      try:\n",
        "        embedding_vector = self.embedding_model[word]\n",
        "      except (KeyError, TypeError):\n",
        "        oov_count += 1\n",
        "        oov_words.append(word)\n",
        "        embedding_vector = np.random.uniform(low=-0.05, \n",
        "                                             high=0.05, \n",
        "                                             size=self.embedding_dimension)\n",
        "\n",
        "      embedding_matrix[idx] = embedding_vector\n",
        "    \n",
        "    print(f'\\n[Debug] {oov_count} OOV words found!\\n')\n",
        "    return embedding_matrix, oov_words"
      ],
      "metadata": {
        "id": "TRJ1NpSMqaJL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initialize the handler with the desidered `embedding_dimension`. Then to build the embedding matrix with the pre-trained GloVe embeddings simply call the `build_embedding_matrix` method."
      ],
      "metadata": {
        "id": "gk-z8A5y3cpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Initalize the handler for GloVe\n",
        "glove_handler = GloVe(encoder_config['embedding_dimension'])\n",
        "\n",
        "# We will create the matrix by using only the words present in the training and validation set\n",
        "embedding_matrix_context, oov_words_context = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_context[1], \n",
        "    len(word_to_idx_context[1])+1)\n",
        "\n",
        "embedding_matrix_question, oov_words_question = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_question[1], \n",
        "    len(word_to_idx_question[1])+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUmvdCWGavSR",
        "outputId": "ee63fc29-86e3-4b80-fd7f-464a74bfd2d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26136/26136 [00:00<00:00, 45707.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 1706 OOV words found!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12672/12672 [00:00<00:00, 85212.82it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 652 OOV words found!\n",
            "\n",
            "CPU times: user 1.96 s, sys: 753 ms, total: 2.71 s\n",
            "Wall time: 8.77 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert both of them into tensor, but it is fine to also treat them as `numpy` array, still it is better to use the `tensorflow` fundamentals."
      ],
      "metadata": {
        "id": "qdLOk0pQu3Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_context = tf.convert_to_tensor(embedding_matrix_context)\n",
        "embedding_matrix_question = tf.convert_to_tensor(embedding_matrix_question)"
      ],
      "metadata": {
        "id": "EX6PvKTBsdSW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Definition"
      ],
      "metadata": {
        "id": "FF5Rtd4uqa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_context_batch, example_question_batch = next(iter(dataset.train))"
      ],
      "metadata": {
        "id": "GjdRysIvPoLc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Encoder\n",
        "We will use a bidirectional LSTM to encode the sentence,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\overrightarrow{b_t} &= \\overrightarrow{\\text{LSTM}}(x_t, \\overrightarrow{b_{t-1}})\\\\\n",
        "\\overleftarrow{b_t} &= \\overleftarrow{\\text{LSTM}}(x_t, \\overleftarrow{b_{t+1}})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\overrightarrow{b_t}$ is the hidden state at time step $t$ for the forward pass LSTM and $\\overleftarrow{b_t}$ for the backward pass."
      ],
      "metadata": {
        "id": "wjVfZgIIf1RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               context_vocab_size, \n",
        "               embedding_matrix,\n",
        "               embedding_dimension, \n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_context,\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_context = tf.constant(max_length_context)\n",
        "\n",
        "    # Layers\n",
        "    self.inputs = Input(shape=(self.max_length_context,), name='encoder_input')\n",
        "    \n",
        "    self.embedding = Embedding(input_dim=context_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_context,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,\n",
        "                               mask_zero=False,\n",
        "                               name='encoder_embedding_layer')  \n",
        "    \n",
        "    # The LSTM forward pass\n",
        "    self.forward_lstm_layer = LSTM(units//4,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "    \n",
        "    # The LSTM backward pass\n",
        "    self.backward_lstm_layer = LSTM(units//4,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  go_backwards=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "\n",
        "    # The Bidirectional wrapper  \n",
        "    self.bidirectional_lstm = Bidirectional(self.forward_lstm_layer, \n",
        "                                            backward_layer=self.backward_lstm_layer, \n",
        "                                            name='encoder_bi_lstm',\n",
        "                                            merge_mode='concat')\n",
        "    \n",
        "  def call(self, inputs, state=None, training=True):\n",
        "    # x shape = (batch_size, max_length_context, embedding_dimension)\n",
        "    x = self.embedding(inputs)\n",
        "    output, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm(x, initial_state=state, training=training)\n",
        "\n",
        "    # encoder_outputs shape = (batch_size, max_length_context, units)\n",
        "    # forward_h shape = (batch_size, units//2)\n",
        "    # forward_c shape = (batch_size, units//2)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm(output, initial_state=(forward_h, forward_c, backward_h, backward_c), training=training)\n",
        "    \n",
        "    # op:concat shape = (batch_size, units)\n",
        "    h_concat = tf.concat([forward_h, backward_h], axis=1, name='hidden_concat')\n",
        "    c_concat = tf.concat([forward_c, backward_c], axis=1, name='cell_concat')\n",
        "    return encoder_outputs, (h_concat, c_concat)\n",
        "\n",
        "  # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    x = Input(shape=(self.max_length_context,), batch_size=self.batch_size)\n",
        "    return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(), \n",
        "        to_file='encoder.png', dpi=96,              \n",
        "        show_shapes=True, show_layer_names=True,  \n",
        "        expand_nested=True                       \n",
        "    )"
      ],
      "metadata": {
        "id": "GK6Kd1XvqK22"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Test the encoder stack\n",
        "\n"
      ],
      "metadata": {
        "id": "fbjSxPGcFud_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_config['context_vocab_size'] = len(word_to_idx_context[1])\n",
        "encoder_config['max_length_context'] = dataset.train.element_spec[0].shape[1]\n",
        "\n",
        "encoder = Encoder(**encoder_config, embedding_matrix=embedding_matrix_context)\n",
        "encoder_outputs, encoder_state = encoder(inputs=example_context_batch)\n",
        "\n",
        "hidden_state, cell_state = encoder_state\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, max_length_context, units): {encoder_outputs.shape}')\n",
        "print(f'Hidden state shape: (batch_size, units): {hidden_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, units): {cell_state.shape}')"
      ],
      "metadata": {
        "id": "_ffteDMQyzmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00310377-e9b2-4997-cbdf-296174322dc5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, max_length_context, units): (64, 389, 300)\n",
            "Hidden state shape: (batch_size, units): (64, 300)\n",
            "Cell state shape: (batch_size, units): (64, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is still missing an LSTM layer."
      ],
      "metadata": {
        "id": "-0HQt4ukvf5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.build_graph().summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng97TFpLRym2",
        "outputId": "120030c4-5225-4555-a4ae-5e23aff641d1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(64, 389)]          0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding_layer (Embed  (64, 389, 300)      7841100     ['input_6[0][0]']                \n",
            " ding)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_bi_lstm (Bidirectional  [(64, 389, 300),    541200      ['encoder_embedding_layer[0][0]',\n",
            " )                               (64, 150),                       'encoder_bi_lstm[0][0]',        \n",
            "                                 (64, 150),                       'encoder_bi_lstm[0][1]',        \n",
            "                                 (64, 150),                       'encoder_bi_lstm[0][2]',        \n",
            "                                 (64, 150)]                       'encoder_bi_lstm[0][3]',        \n",
            "                                                                  'encoder_bi_lstm[0][4]']        \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)       (64, 300)            0           ['encoder_bi_lstm[1][1]',        \n",
            "                                                                  'encoder_bi_lstm[1][3]']        \n",
            "                                                                                                  \n",
            " tf.concat_5 (TFOpLambda)       (64, 300)            0           ['encoder_bi_lstm[1][2]',        \n",
            "                                                                  'encoder_bi_lstm[1][4]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,382,300\n",
            "Trainable params: 541,200\n",
            "Non-trainable params: 7,841,100\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "aaJFf-ELdIZo",
        "outputId": "41948916-e3ae-4767-f2b4-4eeab7221daf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Decoder"
      ],
      "metadata": {
        "id": "dtM9nOQrf3jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Container classes\n",
        "# Reference :- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "class DecoderInput(NamedTuple):\n",
        "  new_token: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "73wq7CTiTQpX"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(Layer):\n",
        "  # Still it is possible to use Luang's attention as an alternative\n",
        "  # Reference:- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.W1 = Dense(units, use_bias=False, name='Wb1_attention_weights')\n",
        "    self.W2 = Dense(units, use_bias=False, name='Wb2_attention_weights')\n",
        "\n",
        "    self.attention = AdditiveAttention(use_scale=True)\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    \"\"\"\n",
        "    This layer takes 3 inputs:\n",
        "      - the query; this will be generated by the decoder, later,\n",
        "      - the value: the output of the encoder,\n",
        "      - the mask: to exclude the padding, i.e., context_batch != 0.\n",
        "    \"\"\"\n",
        "    #W1@ht\n",
        "    w1_query = self.W1(query)\n",
        "    #W2@hs\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask = [query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    \n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               question_vocab_size, \n",
        "               embedding_matrix, \n",
        "               embedding_dimension,\n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_question,\n",
        "               **kwargs):\n",
        "    \n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_question = tf.constant(max_length_question)\n",
        "    self.embedding_dimension = tf.constant(embedding_dimension)\n",
        "    self.units = tf.constant(units)\n",
        "\n",
        "    # Layers definition\n",
        "    self.inputs = Input(shape=(None,), batch_size=self.batch_size)\n",
        "                        \n",
        "    # Embedding for the questions\n",
        "    self.embedding = Embedding(input_dim=question_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_question,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,  #?\n",
        "                               mask_zero=False,\n",
        "                               name='decoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM layer\n",
        "    self.lstm_layer = LSTM(units//2,\n",
        "                          return_sequences=True,\n",
        "                          return_state=True,\n",
        "                          name='decoder_lstm_layer')\n",
        "    \n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(units//2)\n",
        "\n",
        "    self.Wt = Dense(units, activation=tf.math.tanh, use_bias=False, name='decoder_Wt_weights')\n",
        "\n",
        "    # For the word probabilities\n",
        "    self.Ws = Dense(question_vocab_size, activation=tf.nn.softmax, use_bias=False, name='decoder_Ws_weights')\n",
        "\n",
        "  def call(self, \n",
        "           inputs: DecoderInput, \n",
        "           state=None,\n",
        "           training=True) -> Tuple[DecoderOutput, Tuple[tf.Tensor]]:\n",
        "\n",
        "    # Lookup the embeddings for the questions\n",
        "    x = self.embedding(inputs.new_token)\n",
        "    # embedded_tensor shape: (batch_size, 1, embedding_dimension)\n",
        "    if tf.shape(x).shape == 2: x = tf.expand_dims(x, axis=1)\n",
        "\n",
        "    # Process one step with the RNN\n",
        "    # LSTM expects inputs of shape: (batch_size, timestep, feature)\n",
        "    cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(x, initial_state=state, training=training)\n",
        "    cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(cell_output, initial_state=(hidden_dec_state, cell_dec_state), training=training)\n",
        "    \n",
        "    # Use the LSTM cell output as the query for the attention over the encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=cell_output, \n",
        "        value=inputs.enc_output, \n",
        "        mask=inputs.mask)\n",
        "\n",
        "    # Join the context_vector and cell output [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    cell_output_and_context_vector = tf.concat([cell_output, context_vector], axis=-1)\n",
        "\n",
        "    # at = tanh(Wt@[ht, ct])\n",
        "    attention_vector = self.Wt(cell_output_and_context_vector, training=training)\n",
        "\n",
        "    # logits = softmax(Ws@at)\n",
        "    logits = self.Ws(attention_vector, training=training)\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), (hidden_dec_state, cell_dec_state)"
      ],
      "metadata": {
        "id": "V_-Lef2CqUW2"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Test the decoder stack\n",
        "\n",
        "The decoder will take as input:\n",
        "1. `new_tokens`: the last token generated of shape `(batch_size, 1)`, namely the token obrained in the previous time step of the decoder (we will initialize the decoder with the `\"<sos>\"` token);\n",
        "2. `enc_output`: this is the representation produced by the `Encoder` of shape `(batch_size, max_length_context, enc_units)`;\n",
        "3. `mask`: this is the mask, that is a boolean tensor, indicating which tokens will be considered in the decoding of shape `(batch_size, max_length_context)`; \n",
        "4. `decoder_state`: the previous state of the decoder, namely the internal state of the decoder's LSTM (the paper suggests to input the hidden and cell state produced by the Bi-LSTM). The shape is `[(batch_size, enc_units), (batch_size, enc_units)]`.\n",
        "\n"
      ],
      "metadata": {
        "id": "IF5J42g1l-be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_config['question_vocab_size'] = len(word_to_idx_question[1])\n",
        "decoder_config['max_length_question'] = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "decoder = Decoder(**decoder_config, embedding_matrix=embedding_matrix_question)"
      ],
      "metadata": {
        "id": "kS0UBnMzTbie"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "start_tag_index = word_to_idx_question[2]['<sos>']\n",
        "first_token = tf.squeeze(tf.constant([[start_tag_index]] * decoder_config['batch_size']), axis=1)"
      ],
      "metadata": {
        "id": "KeMvqDnrTkf0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, decoder_state = decoder(\n",
        "    inputs = DecoderInput(first_token, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = encoder_state\n",
        ")\n",
        "\n",
        "hidden_dec_state, cell_dec_state = decoder_state\n",
        "\n",
        "print(f'Logits shape: (batch_size, t, output_vocab_size) {decoder_result.logits.shape}')\n",
        "print(f'Hidden state shape: (batch_size, dec_units) {hidden_dec_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, dec_units) {cell_dec_state.shape}')"
      ],
      "metadata": {
        "id": "BF6PWsNYfmUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78c1d0a-4ecb-47ea-fe18-17c73c1f5ab1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (batch_size, t, output_vocab_size) (64, 1, 12672)\n",
            "Hidden state shape: (batch_size, dec_units) (64, 300)\n",
            "Cell state shape: (batch_size, dec_units) (64, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we cannot provide a detailed summary or a handy plot due to the fact that we pass to the decoder model a structured input which is not preferred by tensorflow."
      ],
      "metadata": {
        "id": "T9FZVz2QyLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AJWwcDacEj",
        "outputId": "a244a049-1823-45fe-a6dd-47fe6e915e66"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_embedding_layer (Em  multiple                 3801900   \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " decoder_lstm_layer (LSTM)   multiple                  721200    \n",
            "                                                                 \n",
            " bahdanau_attention_5 (Bahda  multiple                 180300    \n",
            " nauAttention)                                                   \n",
            "                                                                 \n",
            " decoder_Wt_weights (Dense)  multiple                  360000    \n",
            "                                                                 \n",
            " decoder_Ws_weights (Dense)  multiple                  7603200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,666,600\n",
            "Trainable params: 8,864,700\n",
            "Non-trainable params: 3,801,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on: this means that the decoder will produce a vector of probabilities associated to each vocabulary word. That is, a vector of logits $l_b \\in \\mathbb{R}^{\\mathcal{V}}$ for each element $b$ in the batch, namely indicating the next probable token for a given sentence. Since they are logits they should sum up to `1.0`, evenutally a number really close to it. "
      ],
      "metadata": {
        "id": "BBIQDE0Sl6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result.logits[0, 0, :].numpy().sum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO-C7ELdlv1u",
        "outputId": "d5ca98f8-4ce9-49ce-9718-983deecfbb47"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we sample a token according to the logits computed by the decoder."
      ],
      "metadata": {
        "id": "xrN_dTRtGdQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :],\n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "vocab = np.array(list(word_to_idx_question[1].keys()))\n",
        "\n",
        "first_word = list(vocab[tf.squeeze(sampled_tokens, axis=-1).numpy()])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "kGGwivobvx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a57ac3a-c7c3-47e7-c176-3ffb1c16d4d0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greenhouse', 'closests', 'shield', 'jinrong', 'paintings']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, _ = decoder(\n",
        "    inputs = DecoderInput(sampled_tokens, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = decoder_state\n",
        ")\n",
        "\n",
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :], \n",
        " \n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "sampled_tokens = tf.squeeze(sampled_tokens, axis=-1).numpy()\n",
        "\n",
        "first_word = list(vocab[sampled_tokens])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "Y2ixRaJZn271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ff901a-3c37-4421-cd1d-ab968eb68d05"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['liangzhu', 'connaught', 'reservations', 'critic', 'reporters']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training for QG"
      ],
      "metadata": {
        "id": "qIoySQKuIGlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Training checkpoints\n",
        "\n",
        "See [Manual Checkpointing](https://www.tensorflow.org/guide/checkpoint)."
      ],
      "metadata": {
        "id": "x6uDOVpginU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_prefix = os.path.join(path['checkpoint_dir'], \"tf_ckpt\")\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_iterator = iter(dataset.train) \n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "manager = tf.train.CheckpointManager(checkpoint,\n",
        "                                     checkpoint_prefix,\n",
        "                                     max_to_keep=3)"
      ],
      "metadata": {
        "id": "5tNJR_fRi7qd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Loss\n",
        "\n",
        "The **QG** task is defined as finding $\\hat{y}$ such that:\n",
        "$$\n",
        "\\hat{y} = \\arg{\\max_y P(y|x)}  \n",
        "$$\n",
        "where $P(y|x)$ is the conditional log-likelihood of the predicted question sentence $y$ given the input $x$. Du et al. shown that the conditional probability could be factorized in:\n",
        "$$\n",
        "P(y|x) = \\prod_{t=1}^{|y|} P(y_t|x, y_{<t})\n",
        "$$\n",
        "where the probability of each $y_t$ is predicted based on all the words that have been generated upon time $t$, namely $y_{<t}$.\n",
        "\n",
        "This means that given a training corpus of sentence-question pairs $\\mathcal{S} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the objective is to minimize the negative log-likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L} &= - \\sum_{i=1}^N \\log P(y^{(i)}|x^{(i)}; \\theta)\\\\\n",
        "            &=  - \\sum_{i=1}^N \\sum_{j=1}^{|y^{(i)}|} \\log P (y_j^{(i)}|x^{(i)}, y_{<j}^{(i)}; \\theta)\n",
        "\\end{align*}\n",
        "$$\n",
        "We parameterize the probability of decoding each word $y_j$ by using an RNN:\n",
        "$$\n",
        "P(y_j|y_{<j}, s) = \\text{softmax}(g(h_j))\n",
        "$$\n",
        "where $g(.)$ is a transition function that outputs a vocabulary-sized vector."
      ],
      "metadata": {
        "id": "qyRA2RxZNsx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'conditional_ll_loss'\n",
        "\n",
        "    # The loss needs to work with logits since the decoder is outputting the most probable token\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True,\n",
        "        reduction='none'\n",
        "    )\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch\n",
        "    # Shape of y_true = (batch_size, )\n",
        "    # Shape of y_pred = (batch_size, 1, vocab_size)\n",
        "    loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    \n",
        "    # Mask of the losses on the padding\n",
        "    mask = tf.math.not_equal(y_true, 0)\n",
        "    loss = tf.boolean_mask(loss, mask)\n",
        "    loss = tf.reduce_sum(loss)\n",
        "\n",
        "    # Return the total\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IP_UunM3MUtF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 QG model and training step implementation\n",
        "\n",
        "The training step should:\n",
        "1. Run the encoder on the `input_tokens` to get the `encoder_outputs`, `hidden_state` and `cell_state`. "
      ],
      "metadata": {
        "id": "iwLiPsCyNuor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorTrainer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               context_vocab_size,\n",
        "               question_vocab_size,\n",
        "               embedding_dimension,\n",
        "               embedding_matrix_context,\n",
        "               embedding_matrix_question,\n",
        "               units,\n",
        "               batch_size,\n",
        "               max_length_context,\n",
        "               max_length_question,\n",
        "               use_tf_function=True):\n",
        "    \"\"\"\n",
        "    Prepare the model for the training. It builds the both the encoder and the decoder.\n",
        "    Also it defines a wrapper to use the tf.function compilation for the tensorflow computational\n",
        "    graph.\n",
        "    \"\"\"\n",
        "    self.max_length_question = max_length_question\n",
        "\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(\n",
        "        context_vocab_size,\n",
        "        embedding_matrix_context,\n",
        "        embedding_dimension,\n",
        "        units,\n",
        "        batch_size,\n",
        "        max_length_context)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        question_vocab_size,\n",
        "        embedding_matrix_question,\n",
        "        embedding_dimension,\n",
        "        units, \n",
        "        batch_size,\n",
        "        max_length_question)\n",
        "\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  def _train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Optimization step.\n",
        "    \"\"\"\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      encoder_output, encoder_state = self.encoder(context, training=True)\n",
        "\n",
        "      # The decoder should be initialized with the encoder last state \n",
        "      decoder_state = encoder_state\n",
        "      loss = tf.constant(0.0)\n",
        "      t = 0\n",
        "\n",
        "      # Reference :- https://www.tensorflow.org/guide/function\n",
        "      # We have to run the decoder for all the length of the question \n",
        "      while t < (self.max_length_question - 1):\n",
        "        # We have to pass two tokens:\n",
        "        #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "        #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "        new_token = tf.gather(question, t, axis=1)\n",
        "        target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "        step_loss, decoder_state = self.step_decoder(\n",
        "            (new_token, target_token),\n",
        "            context_mask,\n",
        "            encoder_output,\n",
        "            decoder_state, \n",
        "            training=True)\n",
        "\n",
        "        loss = loss + step_loss\n",
        "        t = t + 1\n",
        "            \n",
        "      # Average the loss for all the legit tokens\n",
        "      avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "      \n",
        "      # Compute metric on the batch: in our case perplexity\n",
        "      # Reference :- https://en.wikipedia.org/wiki/Perplexity\n",
        "      perplexity = tf.exp(avg_loss)\n",
        "\n",
        "    # Apply an optimization step\n",
        "    tr_variables = self.trainable_variables\n",
        "    grads = tape.gradient(avg_loss, tr_variables)\n",
        "\n",
        "    n_vars = len(tr_variables)\n",
        "    \n",
        "    # Apply some clipping (by norm) as done in the paper\n",
        "    grads = [tf.clip_by_norm(grads[i], 5.0) for i in range(n_vars)]\n",
        "    self.optimizer.apply_gradients(zip(grads, tr_variables))\n",
        "\n",
        "    return {'batch_loss': avg_loss, 'batch_perplexity': perplexity}\n",
        "  \n",
        "  @tf.function\n",
        "  def step_decoder(self, \n",
        "                   tokens,\n",
        "                   context_mask,\n",
        "                   encoder_output,\n",
        "                   decoder_state,\n",
        "                   training):\n",
        "    \"\"\"\n",
        "    Run a single iteration of the decoder and computers the incremental loss between the\n",
        "    produced token and the token in the target input.\n",
        "    \"\"\"\n",
        "    new_token, target_token = tokens\n",
        "    \n",
        "    # Run the decoder one time\n",
        "    decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=context_mask),\n",
        "        state = decoder_state, \n",
        "        training = training)\n",
        "  \n",
        "    y_true = target_token\n",
        "    y_pred = decoder_result.logits\n",
        "\n",
        "    step_loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    # metric_value = self.custom_metric(y_true, y_pred)\n",
        "    # return step_loss, metric_value, decoder_state\n",
        "    return step_loss, decoder_state\n",
        "\n",
        "  def __get_mask(self, tokens): return tf.math.not_equal(tokens, 0)"
      ],
      "metadata": {
        "id": "8xSn_StMq9cx",
        "cellView": "code"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_model = QGeneratorTrainer(**encoder_config,\n",
        "                             question_vocab_size=decoder_config['question_vocab_size'],\n",
        "                             max_length_question=decoder_config['max_length_question'],\n",
        "                             embedding_matrix_context=embedding_matrix_context,\n",
        "                             embedding_matrix_question=embedding_matrix_question,\n",
        "                             use_tf_function=True)\n",
        "\n",
        "qg_model.compile(\n",
        "    optimizer=trainer_config['optimizer'],\n",
        "    # loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    loss=MaskedLoss(),\n",
        "    # metrics=[keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0, name='perplexity'),\n",
        "    #          tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")"
      ],
      "metadata": {
        "id": "zwKc0rvrIWkg"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1 Metrics"
      ],
      "metadata": {
        "id": "8WOVZj966EPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Also the accuracy should mask the padding\n",
        "class MaskedAccuracy(tf.keras.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MaskedAccuracy, self).__init__(name='accuracy', **kwargs)\n",
        "    self.results = self.add_weight(name='results', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    raise(\"Not yet Implemented\")\n",
        "\n",
        "class MaskedF1Score(tf.keras.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MaskedF1Score, self).__init__(name='f1_score', **kwargs)\n",
        "    self.results = self.add_weight(name='results', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    raise(\"Not yet Implemented\")\n",
        "\n",
        "# def MaskedAccuracy(y_true, y_pred):\n",
        "#     # y_pred shape is batch_size, seq length, vocab size\n",
        "#     # y_true shape is batch_size, seq length\n",
        "#     pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
        "#     correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
        "\n",
        "#     # 0 is padding, don't include those\n",
        "#     mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "#     n_correct = K.sum(mask * correct)\n",
        "#     n_total = K.sum(mask)\n",
        "  \n",
        "#     return n_correct / n_total"
      ],
      "metadata": {
        "id": "xbK72c5r6LWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 Simple Training\n",
        "The first call with `use_tf_function=True` will be slow since it has to trace the function. So be patient or try `use_tf_function=False` 😀"
      ],
      "metadata": {
        "id": "SPSDqU3_3nOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataset.train))\n",
        "qg_model.train_step(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf5JzL9T47Ha",
        "outputId": "c0feb999-b2b4-4172-c700-8bddb1c3ef63"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.796324>,\n",
              " 'perplexity': <tf.Tensor: shape=(), dtype=float32, numpy=6609.9>}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# losses = []\n",
        "# for n in tqdm(range(10)):\n",
        "#   # print('.', end='')\n",
        "#   logs = qg_model.train_step(next(iter(dataset.train)))\n",
        "#   losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "# print()\n",
        "# plt.plot(losses)\n",
        "# print()\n",
        "# print(losses)"
      ],
      "metadata": {
        "id": "E0ZatZ4g-5iq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Tensorboard"
      ],
      "metadata": {
        "id": "0imUvBAg14Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Train the model"
      ],
      "metadata": {
        "id": "fF8BfykX5faH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key) -> None:\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_ends(self, n, logs):\n",
        "    self.logs.append(logs[self.key])"
      ],
      "metadata": {
        "id": "uhSLln405mfy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_loss = BatchLogs('batch_loss')\n",
        "perplexity = BatchLogs('perplexity')\n",
        "history = qg_model.fit(\n",
        "    dataset.train, \n",
        "    epochs=trainer_config['epochs'], \n",
        "    callbacks=[batch_loss, perplexity],\n",
        "    validation_data=dataset.val, \n",
        "    verbose='auto',\n",
        "    use_multiprocessing = True\n",
        "    )"
      ],
      "metadata": {
        "id": "ys97OwVn61UT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "fcb3b3ba-61a1-46d5-b389-0d2867a61cda"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 21/112 [====>.........................] - ETA: 34:47 - batch_loss: 8.7730 - perplexity: 6460.0142"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8556512b43b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Evaluation for QG\n"
      ],
      "metadata": {
        "id": "ZC-NjKElfqNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorEvaluator(tf.Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def test_step(self, inputs):\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    encoder_output, encoder_state = self.encoder(context, training=False)\n",
        "    decoder_state = encoder_state\n",
        "    loss = tf.constant(0.0)\n",
        "    t = 0\n",
        "\n",
        "    # Reference :- https://www.tensorflow.org/guide/function\n",
        "    # We have to run the decoder for all the length of the question \n",
        "    while t < (self.max_length_question - 1):\n",
        "      # We have to pass two tokens:\n",
        "      #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "      #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "      new_token = tf.gather(question, t, axis=1)\n",
        "      target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "      step_loss, metric_value, decoder_state = self.step_decoder(\n",
        "          (new_token, target_token),\n",
        "          context_mask,\n",
        "          encoder_output,\n",
        "          decoder_state, \n",
        "          training=True)\n",
        "\n",
        "      loss = loss + step_loss\n",
        "      # self.custom_metric_mean.update_state(metric_value)\n",
        "      t = t + 1\n",
        "\n",
        "    # Average the loss for all the legit tokens\n",
        "    avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "    # return {f'batch_loss': avg_loss, f'batch_perplexity': self.custom_metric_mean.result()}\n",
        "    return {f'batch_loss': avg_loss}"
      ],
      "metadata": {
        "id": "TqHNOZo8Kld7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Inference for QG\n",
        "In this section we will provide the class and the methods for the inference part. More specifically, both auxiliary and inferencing methods:\n",
        "1. `token_to_string()`:\n",
        "2. `string_to_token()`:\n",
        "3. `create_mask()`:\n",
        "4. `temperature_sampling()`:\n",
        "5. `generate_question()`:"
      ],
      "metadata": {
        "id": "ezgR7c68_0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorInference(tf.Module):\n",
        "  def __init__(self, encoder, decoder, tokenizer, word_to_idx, use_tf_function):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_idx = word_to_idx\n",
        "   \n",
        "    self.result_tokens = None\n",
        "    self.result_text = None\n",
        "    self.token_mask = self.create_mask()\n",
        "\n",
        "    self.start_idx = word_to_idx['<sos>']\n",
        "    self.end_idx = word_to_idx['<eos>']\n",
        "    self.use_tf_function = False\n",
        "\n",
        "  def token_to_string(self, result_tokens: tf.Tensor):  \n",
        "    \"\"\"\n",
        "    This method converts token IDs to text by using a given mapping.\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    list_text = self.tokenizer.sequences_to_texts(list_tokens)\n",
        "    list_text = tf.convert_to_tensor([list_text])\n",
        "    result_text = tf.strings.reduce_join(list_text, axis=0, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    \n",
        "    self.result_tokens = result_tokens\n",
        "    self.result_text = result_text\n",
        "    return result_text\n",
        "\n",
        "  def string_to_token(self, result_str: tf.Tensor):\n",
        "    \"\"\"\n",
        "    This method converts texts to token IDs by using a given mapping.\n",
        "    \"\"\"  \n",
        "    list_str = [s.decode(\"utf-8\") for s in result_str.numpy().tolist()]\n",
        "    list_tokens = self.tokenizer.texts_to_sequences(list_str)\n",
        "    list_tokens = tf.convert_to_tensor(list_tokens, dtype=tf.int64)\n",
        "    result_tokens = tf.squeeze(tf.split(list_tokens, num_or_size_splits=list_tokens.shape[0], axis=0), axis=1)\n",
        "\n",
        "    return result_tokens\n",
        "  \n",
        "  def create_mask(self):\n",
        "    \"\"\"\n",
        "    This method creates a mask for the padding, the unknwon words and the start/ending tokens.\n",
        "    \"\"\"\n",
        "    masked_words = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "    token_mask_ids = [self.tokenizer.word_index[mask] for mask in masked_words]\n",
        "\n",
        "    token_mask = np.zeros(shape=(len(self.word_to_idx),), dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    return token_mask\n",
        "\n",
        "  # evaluate or predict?\n",
        "  def evaluate(self, inputs, max_length, return_attention, mode='greedy', temperature=0.5):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if mode == 'greedy':\n",
        "      if self.use_tf_function:\n",
        "        return self._tf_generate_greedy(inputs, max_length, temperature, return_attention)\n",
        "      else:\n",
        "        return self._generate_greedy(inputs, max_length, temperature, return_attention)\n",
        "    elif mode == 'beam':\n",
        "      return self._generate_beam(inputs, max_length, return_attention)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_generate_greedy(self, inputs, max_length, temperature, return_attention):\n",
        "    return self._generate_question_greedy(inputs, max_length, return_attention, temperature)\n",
        "  \n",
        "  def _generate_greedy(self, \n",
        "                        inputs,\n",
        "                        max_length,\n",
        "                        return_attention,\n",
        "                        temperature):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    # Similarly for what it has been done in the train step\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    # Generate the first token of each sentence, that is the <sos> token\n",
        "    new_token = tf.fill([batch_size, 1], self.start_idx)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    timestep = 0\n",
        "    \n",
        "    while timestep < max_length:\n",
        "      # Decode the token at the next timestep\n",
        "      decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=(inputs != 0)),\n",
        "        state = decoder_state)\n",
        "      \n",
        "      attention.append(decoder_result.attention_weights)\n",
        "\n",
        "      # Sample the new token accordingly to the distribution produced by the decoder\n",
        "      new_token = self.temperature_sampling(decoder_result.logits, temperature)\n",
        "\n",
        "      # if a sequence has reached <eos> set it as done\n",
        "      # MISSING PART\n",
        "\n",
        "      result_tokens.append(new_token)\n",
        "\n",
        "      timestep = timestep + 1\n",
        "    \n",
        "    # MISSING\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.token_to_string(result_tokens)\n",
        "\n",
        "    attention_stack = tf.concat(attention, axis=-1)\n",
        "\n",
        "    # HANDLING UNK WORDS\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}\n",
        "\n",
        "  def temperature_sampling(self, logits, temperature):\n",
        "    \"\"\"\n",
        "\n",
        "    For the temperature choice see here:\n",
        "      Reference :- https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "    \"\"\"\n",
        "    # First of all we use broadcast the generated mask to the expected logits' shape\n",
        "    # token_mask shape: (batch_size, timestep, vocab_size)\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # The logits for all the tokens that have to not be used are set top -1.0\n",
        "    logits = tf.where(token_mask, -1.0, logits)\n",
        "\n",
        "    # Freezing function\n",
        "    # Higher temperature -> greater variety\n",
        "    # Lower temperature -> grammatically correct\n",
        "    if temperature == 0.0:\n",
        "      # the freezing function is the argmax, behaving like a greedy search\n",
        "      new_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      # the freezing function now scales the logits.\n",
        "      # for temperature == 1.0 is the identity function\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_token = tf.random.categorical(logits / temperature, num_samples=1)\n",
        "    return new_token\n",
        "\n",
        "  # def _beam_search(self, k, max_length, encoder_output, encoder_state, mask):\n",
        "  #   decoder_states = list()\n",
        "    \n",
        "  #   for i in range(k):\n",
        "  #     decoder_states.append(encoder_state)\n",
        "\n",
        "  #   print(decoder_states[0])\n",
        "  #   # decoders = []\n",
        "    \n",
        "  #   sequences = [[list(), 0.0, 0]]\n",
        "  #   timestep = 0\n",
        "  #   new_tokens = tf.fill([1, k, 1], self.start_idx)\n",
        "  #   all_candidates = list()\n",
        "    \n",
        "  #   while timestep < max_length:\n",
        "  #     timestep = timestep+1\n",
        "      \n",
        "  #     for i in range(len(sequences)):\n",
        "  #       seq, score, _ = sequences[i]\n",
        "\n",
        "  #       decoder_result, decoder_state = self.decoder(\n",
        "  #         inputs = DecoderInput(\n",
        "  #             new_token=new_tokens[i],\n",
        "  #             enc_output=encoder_output,\n",
        "  #             mask=mask),\n",
        "  #         state = decoder_states[i])\n",
        "\n",
        "  #       decoder_states[i] = decoder_state\n",
        "        \n",
        "  #       for j in range(decoder.logits.shape[-1]):\n",
        "  #         candidate = [seq + [j], score - np.log(decoder.logits[0][j]), i]\n",
        "  #         all_candidates.append(candidate)\n",
        "  #       # order all candidates by score\n",
        "  #     ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "  #     sequences = ordered[:k]\n",
        "  #     new_tokens = list()\n",
        "  #     # new_decoders = []\n",
        "  #     new_decoder_states = []\n",
        "  #     for k in range(k):\n",
        "  #       new_tokens.append(all_candidates[k][0][-1])\n",
        "  #       # new_decoders.append(decoders[all_candidates[k][2]])\n",
        "  #       new_decoder_states.append(decoder_states[all_candidates[k][2]])\n",
        "  #     # decoders = new_decoders\n",
        "  #     decoder_states = new_decoder_states\n",
        "  #   return sequences\n",
        "\n",
        "  # def _generate_question_beam_search(self, inputs, max_length, return_attention=True):\n",
        "  #   batch_size = tf.shape(inputs)[0]\n",
        "  #   encoder_output, encoder_state = self.encoder(inputs)\n",
        "\n",
        "  #   hidden_state, cell_state = encoder_state\n",
        "  #   batch_sequences = []\n",
        "  #   for i in range(batch_size):\n",
        "  #     encoder_state_batch = (hidden_state[i, :], cell_state[i, :])\n",
        "  #     sequences = self._beam_search(3, \n",
        "  #                                   max_length, \n",
        "  #                                   encoder_output[i, :, :], \n",
        "  #                                   encoder_state_batch, \n",
        "  #                                   (inputs[i,] != 0))\n",
        "  #     batch_sequences.append(sequences)\n",
        "  #   return batch_sequences\n"
      ],
      "metadata": {
        "id": "gn2NTxAq_2sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator = QGeneratorInference(qg_model.encoder, \n",
        "                                   qg_model.decoder, \n",
        "                                   dataset_creator.tokenizer_question, \n",
        "                                   word_to_idx=word_to_idx_question[1], \n",
        "                                   use_tf_function=True)"
      ],
      "metadata": {
        "id": "1Q2pWJ5YDfqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_logits = tf.random.normal([5, 1, len(word_to_idx_question[1])])\n",
        "example_output_tokens = qg_generator.temperature_sampling(example_logits, temperature=1.0)"
      ],
      "metadata": {
        "id": "fklYEd1e6XOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator.evaluate(example_output_tokens, max_length=10, return_attention=False, mode='greedy')"
      ],
      "metadata": {
        "id": "7RO-7QCL_MSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}