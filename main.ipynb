{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sFBKCIE3Jxf2",
        "ZgeJckqZIufT",
        "dndR7_CNI1jq",
        "MvU7n1LboA6g",
        "hlpy-ayWoEHa",
        "r7qxjGzKJM2w",
        "kx2f7Nn_4en9",
        "FF5Rtd4uqa_k",
        "wjVfZgIIf1RV",
        "dtM9nOQrf3jq",
        "x6uDOVpginU0",
        "fF8BfykX5faH",
        "ZC-NjKElfqNf",
        "ezgR7c68_0nv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erhtric/NeuralQuestionGenerationNLP/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main file: its purpouse is to collect all the code coming from the coding pipeline."
      ],
      "metadata": {
        "id": "8OU-wpGL18xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U tensorflow-addons\n",
        "#!pip install -q \"tensorflow-text==2.8.*\"\n",
        "# !pip install keras-nlp"
      ],
      "metadata": {
        "id": "Htbwd0W_Y9IC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "import re\n",
        "import os\n",
        "import typing\n",
        "from typing import Any, Tuple, List, NamedTuple\n",
        "import spacy\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "#import seaborn as sns\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from keras.layers import (\n",
        "    Layer, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    Dense, \n",
        "    Bidirectional, \n",
        "    Input, \n",
        "    AdditiveAttention)\n",
        "\n",
        "# import keras_nlp\n",
        "import nltk\n",
        "#from nltk import punkt, pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvONYDvKAHz",
        "outputId": "7a05e12b-cf6e-49f5-c86c-df98520f6090"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CIZdy1hp14x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4efb9f-b7dc-4862-c04b-469f04e98022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commands to prepare the folder to accomodate data."
      ],
      "metadata": {
        "id": "UqKVWPel_ybt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP/Project/Testing folder/Eric\n",
        "%pwd\n",
        "\n",
        "# disable chained assignments to avoid annoying warning\n",
        "pd.options.mode.chained_assignment = None "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKVGy7JPJCoi",
        "outputId": "55f72643-b77c-46dd-ad34-577d8b6cb794"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cVw6eUwM-dRL9BhqtXULyOqeXDrYkwmH/NLP/Project/Testing folder/Eric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./data'):\n",
        "  print('Data folder does not exists. Creating it')\n",
        "  os.makedirs('./data')\n",
        "\n",
        "if not os.path.exists('./training_checkpoints'):\n",
        "  print('Training checkpoint folder does not exists. Creating it')\n",
        "  os.makedirs('./training_checkpoints')"
      ],
      "metadata": {
        "id": "ikmZGjUlitqS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print({tf.__version__})"
      ],
      "metadata": {
        "id": "-ePFW3UcrVtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f893d6f-1172-451a-a5dd-859cf5b453cc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'2.8.2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for the `configuration.json` file, or something similar: "
      ],
      "metadata": {
        "id": "2hyzKyj_-GjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "units = 600\n",
        "\n",
        "dataset_config = {\n",
        "    # 'num_examples': 18896,\n",
        "    'num_examples': 9000,\n",
        "    'num_words_context': 45000,\n",
        "    'num_words_question': 28000,\n",
        "    'buffer_size': 32000,\n",
        "    'batch_size': batch_size,\n",
        "    'random_seed': 13,\n",
        "}\n",
        "\n",
        "encoder_config = {\n",
        "    'context_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_context': None\n",
        "}\n",
        "\n",
        "decoder_config = {\n",
        "    'question_vocab_size': None,\n",
        "    'embedding_dimension': 300,\n",
        "    'units': units,\n",
        "    'batch_size': batch_size,\n",
        "    'max_length_question': None,\n",
        "}\n",
        "\n",
        "trainer_config = {\n",
        "    'epochs': 3,\n",
        "    'optimizer': tf.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "}\n",
        "\n",
        "path = {\n",
        "    'training_json_path': \"./data/training_set.json\",\n",
        "    'save_pkl_path': \"./data/squadv2.pkl\",\n",
        "    'checkpoint_dir': \"./training_checkpoints\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "5bS3uLkE-Mvf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data handling and Pre-processing\n",
        "\n",
        "\n",
        "Things to do:\n",
        "1. Add to each sentence $x$ a start of sequence `<SOS>` tag and end of sequence `<EOS>` tag,\n",
        "2. Clean the sentences by removing special chars,\n",
        "3. Perform other preprocessing steps,\n",
        "4. Create a **vocabulary** with a word-to-index and index-to-word mappings by using a **tokenizer**, \n",
        "5. Extract the sentences that contain an answer and use them as input features, whereas the question will be our target\n",
        "6. Pad each context to maximum length.\n",
        "\n",
        "The resulting data that will be used hereinafter will be of type `tf.data.Dataset`. "
      ],
      "metadata": {
        "id": "sFBKCIE3Jxf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(NamedTuple):\n",
        "  \"\"\"\n",
        "  This class represent a a 3-way split processed dataset. \n",
        "  \"\"\"\n",
        "  # Reference :- https://github.com/topper-123/Articles/blob/master/New-interesting-data-types-in-Python3.rst\n",
        "  train: tf.data.Dataset\n",
        "  val: tf.data.Dataset\n",
        "  test: tf.data.Dataset\n",
        "\n",
        "class SQuAD:\n",
        "  def __init__(self):\n",
        "    self.random_seed = None\n",
        "    self.squad_df = None\n",
        "    self.preproc_squad_df = None\n",
        "    self.tokenizer = None\n",
        "    self.buffer_size = 0\n",
        "    self.batch_size = 0\n",
        "\n",
        "  def __call__(self,\n",
        "           num_examples, \n",
        "           buffer_size, \n",
        "           batch_size, \n",
        "           random_seed,\n",
        "           training_json_path,\n",
        "           save_pkl_path,\n",
        "           num_words_context=None,\n",
        "           num_words_question=None,\n",
        "           tokenized=True,\n",
        "           pos_ner_tag=True,\n",
        "           tensor_type=True):\n",
        "    \"\"\"The call() method loads the SQuAD dataset, preprocess it and optionally it returns \n",
        "    it tokenized. Moreover it also perform a 3-way split.\n",
        "\n",
        "    Args:\n",
        "        num_examples (int): number of examples to be taken from the original SQuAD dataset\n",
        "        num_words (int): the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. \n",
        "        buffer_size (int): buffer size for the shuffling operation\n",
        "        batch_size (int): size of the batches\n",
        "        tokenized (boolean): specifies if the context and question data should be both tokenized\n",
        "        pos_ner_tag (boolean):\n",
        "        tensro_type (boolean): \n",
        "\n",
        "    Returns (depending on the input parameters):\n",
        "        pd.DataFrame: training dataset\n",
        "        pd.DataFrame: validation dataset\n",
        "        pd.DataFrame: testing dataset\n",
        "          OR\n",
        "        NamedTuple: dataset, (dict, dict, dict)\n",
        "    \"\"\"\n",
        "    self.random_seed = random_seed\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "    self.training_json_path = training_json_path\n",
        "    self.save_pkl_path = save_pkl_path\n",
        "    self.pos_ner_tag = pos_ner_tag\n",
        "\n",
        "    # Load dataset from file\n",
        "    self.load_dataset(num_examples)\n",
        "    # Extract answer\n",
        "    self.extract_answer()\n",
        "    # Preprocess context and question\n",
        "    self.preprocess()\n",
        "    \n",
        "    # Perform splitting\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.split_train_val(self.preproc_squad_df)\n",
        "    \n",
        "    if self.pos_ner_tag: \n",
        "      pass\n",
        "\n",
        "    # Initialize Tokenizer for the source: in our case the context phrases\n",
        "    # alternatively TextVectorization \n",
        "    self.tokenizer_context = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_context)\n",
        "    # initialize also for the target, namely the question phrases\n",
        "    self.tokenizer_question = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=num_words_question)\n",
        "\n",
        "    if tokenized:\n",
        "      X_train_tokenized, word_to_idx_train_context = self.__tokenize_context(X_train)\n",
        "      y_train_tokenized, word_to_idx_train_question = self.__tokenize_question(y_train)\n",
        "\n",
        "      X_val_tokenized, word_to_idx_val_context = self.__tokenize_context(X_val)\n",
        "      y_val_tokenized, word_to_idx_val_question = self.__tokenize_question(y_val)\n",
        "\n",
        "      X_test_tokenized, word_to_idx_test_context = self.__tokenize_context(X_test)\n",
        "      y_test_tokenized, word_to_idx_test_question = self.__tokenize_question(y_test)\n",
        "\n",
        "      word_to_idx_context = (word_to_idx_train_context, word_to_idx_val_context, word_to_idx_test_context)\n",
        "      word_to_idx_question = (word_to_idx_train_question, word_to_idx_val_question, word_to_idx_test_question)\n",
        "      \n",
        "      if tensor_type:\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "        # Returns tf.Data.Dataset objects (tokenized)\n",
        "        train_dataset = self.to_tensor(X_train_tokenized, y_train_tokenized)\n",
        "        val_dataset = self.to_tensor(X_val_tokenized, y_val_tokenized)\n",
        "        test_dataset = self.to_tensor(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "        # Configure the dataset for performance\n",
        "        train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        dataset = Dataset(\n",
        "            train=train_dataset, \n",
        "            val=val_dataset,\n",
        "            test=test_dataset)\n",
        "\n",
        "        return dataset, word_to_idx_context, word_to_idx_question\n",
        "      else:\n",
        "        # Returns pd.DataFrame objects (tokenized)\n",
        "        return X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized\n",
        "    else:\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def load_dataset(self, num_examples):\n",
        "    \"\"\"\n",
        "    Extract the dataset from the json file. Already grouped by title.\n",
        "\n",
        "    :param path: [Optional] specifies the local path where the training_set.json file is located\n",
        "\n",
        "    :return\n",
        "        - the extracted dataset in a dataframe format\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.save_pkl_path):\n",
        "      print('File already exists! Loading from .pkl...\\n')\n",
        "      self.squad_df = pd.read_pickle(self.save_pkl_path)\n",
        "      self.squad_df = self.squad_df[:num_examples]\n",
        "    else:\n",
        "      print('Loading from .json...\\n')\n",
        "      with open(self.training_json_path) as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      df_array = []\n",
        "      for current_subject in data['data']:\n",
        "          title = current_subject['title']\n",
        "\n",
        "          for current_context in current_subject['paragraphs']:\n",
        "              context = current_context['context']\n",
        "\n",
        "              for current_question in current_context['qas']:\n",
        "                  question = current_question['question']\n",
        "                  id = current_question['id']\n",
        "\n",
        "              for answer_text in current_question['answers']:\n",
        "                    answer = answer_text['text']\n",
        "                    answer_start = answer_text['answer_start']\n",
        "                    record = { \"id\": id,\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"answer_start\": answer_start,\n",
        "                                \"answer\": answer\n",
        "                                }\n",
        "\n",
        "              df_array.append(record)\n",
        "      \n",
        "      # Save file\n",
        "      pd.to_pickle(pd.DataFrame(df_array), self.save_pkl_path)\n",
        "      self.squad_df = pd.DataFrame(df_array)[:num_examples]\n",
        "\n",
        "  def preprocess(self):\n",
        "    df = self.squad_df.copy()\n",
        "\n",
        "    # Pre-processing context\n",
        "    context = list(df.context)\n",
        "    preproc_context = []\n",
        "\n",
        "    for c in context:\n",
        "      c = self.__preprocess_sentence(c, question=False)\n",
        "      preproc_context.append(c)\n",
        "    \n",
        "    df.context = preproc_context\n",
        "\n",
        "    # Pre-processing questions\n",
        "    question = list(df.question)\n",
        "    preproc_question = []\n",
        "\n",
        "    for q in question:\n",
        "      q = self.__preprocess_sentence(q, question=True)\n",
        "      preproc_question.append(q)\n",
        "    \n",
        "    df.question = preproc_question\n",
        "\n",
        "    # Remove features that are not useful\n",
        "    df = df.drop(['id'], axis=1)\n",
        "    self.preproc_squad_df = df\n",
        "\n",
        "  def __preprocess_sentence(self, sen, question):\n",
        "    # Creating a space between a word and the punctuation following it\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    sen = re.sub(r\"([?.!,¿])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sen = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", sen)\n",
        "\n",
        "    sen = sen.strip()\n",
        "\n",
        "    # Adding a start and an end token to the sentence so that the model know when to \n",
        "    # start and stop predicting.\n",
        "    # if not question: sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    return sen\n",
        "\n",
        "  def __answer_start_end(self, df):\n",
        "    \"\"\"\n",
        "    Creates a list of starting indexes and ending indexes for the answers.\n",
        "\n",
        "    :param df: the target Dataframe\n",
        "\n",
        "    :return: a dataframe containing the start and the end indexes foreach answer (ending index is excluded).\n",
        "\n",
        "    \"\"\"\n",
        "    start_idx = df.answer_start\n",
        "    end_idx = [start + len(list(answer)) for start, answer in zip(list(start_idx), list(df.answer))]\n",
        "    return pd.DataFrame(list(zip(start_idx, end_idx)), columns=['start', 'end'])\n",
        "\n",
        "  def split_train_val(self, df, train_size=0.8):\n",
        "    \"\"\"\n",
        "    This method splits the dataframe in training and test sets, or eventually, in training, validation and test sets.\n",
        "\n",
        "    Args\n",
        "        :param df: the target Dataframe\n",
        "        :param random_seed: random seed used in the splits\n",
        "        :param train_size: represents the absolute number of train samples\n",
        "        :param val: boolean for choosing between a 3-way split or 2-way one.\n",
        "\n",
        "    Returns:\n",
        "        - Data and labels for training, validation and test sets if val is True \n",
        "        - Data and labels for training and test sets if val is False \n",
        "\n",
        "    \"\"\"\n",
        "    # Maybe we have also to return the index for the starting answer\n",
        "    X = df.drop(['answer_start', 'question', 'answer'], axis=1).copy()\n",
        "    idx = self.__answer_start_end(df)\n",
        "    X['start'] = idx['start']\n",
        "    X['end'] = idx['end']\n",
        "    y = df['question']\n",
        "\n",
        "    # In the first step we will split the data in training and remaining dataset\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X, groups=X['title'])\n",
        "    train_idx, rem_idx = next(split)\n",
        "\n",
        "    X_train = X.iloc[train_idx]\n",
        "    y_train = y.iloc[train_idx]\n",
        "    X_rem = X.iloc[rem_idx]\n",
        "    y_rem = y.iloc[rem_idx]\n",
        "\n",
        "\n",
        "    # Val and test test accounts for 10% of the total data. Both 5%.\n",
        "    splitter = GroupShuffleSplit(train_size=train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X_rem, groups=X_rem['title'])\n",
        "    val_idx, test_idx = next(split)\n",
        "\n",
        "    X_val = X_rem.iloc[val_idx]\n",
        "    y_val = y_rem.iloc[val_idx]\n",
        "\n",
        "    X_test = X_rem.iloc[test_idx]\n",
        "    y_test = y_rem.iloc[test_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def __tokenize_context(self, X):\n",
        "    context = X.context\n",
        "    self.tokenizer_context.fit_on_texts(context)\n",
        "    context_tf = self.tokenizer_context.texts_to_sequences(context)\n",
        "\n",
        "    # context_lengths = [len(seq) for seq in context_tf]\n",
        "    # sns.boxplot(context_lengths)\n",
        "\n",
        "    context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(context):\n",
        "      X['context'].iloc[i] = context_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_context.word_index['<pad>'] = 0\n",
        "    self.tokenizer_context.index_word[0] = '<pad>'\n",
        "\n",
        "    return X, self.tokenizer_context.word_index\n",
        "\n",
        "  def __tokenize_question(self, y):\n",
        "    question = y\n",
        "    self.tokenizer_question.fit_on_texts(question)\n",
        "    question_tf = self.tokenizer_question.texts_to_sequences(question)\n",
        "\n",
        "    # question_lengths = [len(seq) for seq in question_tf]\n",
        "    # sns.boxplot(question_lengths)\n",
        "    \n",
        "    # See also tf.data.Dataset.padding_batch\n",
        "    question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(question):\n",
        "      y.iloc[i] = question_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_question.word_index['<pad>'] = 0\n",
        "    self.tokenizer_question.index_word[0] = '<pad>'\n",
        "\n",
        "    return y, self.tokenizer_question.word_index\n",
        "\n",
        "  def extract_answer(self):\n",
        "    df = self.squad_df.copy()\n",
        "    start_end = self.__answer_start_end(df)\n",
        "    context = list(df.context)\n",
        "    \n",
        "    selected_sentences = []\n",
        "    for i, par in enumerate(context):\n",
        "      sentences = sent_tokenize(par)\n",
        "      start = start_end.iloc[i].start\n",
        "      end = start_end.iloc[i].end      \n",
        "      right_sentence = \"\"\n",
        "      context_characters = 0\n",
        "\n",
        "      for j, sen in enumerate(sentences):\n",
        "        sen += ' '\n",
        "        context_characters += len(sen)\n",
        "        # If the answer is completely in the current sentence\n",
        "        if(start < context_characters and end <= context_characters):\n",
        "          right_sentence = sen\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break\n",
        "        # the answer is in both the current and the next sentence\n",
        "        if(start < context_characters and end > context_characters):\n",
        "          right_sentence = sen + sentences[j+1]\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break \n",
        "\n",
        "    self.squad_df.context = selected_sentences\n",
        "\n",
        "  def to_tensor(self, X, y):\n",
        "    X = X.context.copy()\n",
        "    y = y.copy()\n",
        "\n",
        "    # Reference:- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(list(X), tf.int64), \n",
        "         tf.cast(list(y), tf.int64)))\n",
        "    dataset = dataset.shuffle(self.buffer_size).batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "XyCKxqwRZelj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the `SQuAD` constructor we create a dataset handling object which will be useful for future operations."
      ],
      "metadata": {
        "id": "iTXVdOGcZSCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_creator = SQuAD()"
      ],
      "metadata": {
        "id": "RfCYdZofJ866"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Preprocessed untokenized split"
      ],
      "metadata": {
        "id": "ZgeJckqZIufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                                                                       num_words=None,\n",
        "#                                                                       BUFFER_SIZE=32000,\n",
        "#                                                                       BATCH_SIZE=64,\n",
        "#                                                                       random_seed=RANDOM_SEED,\n",
        "#                                                                       tokenized=False)\n",
        "\n",
        "# print(f'Set target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')\n",
        "\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator(**dataset_config, **path, tokenized=False)"
      ],
      "metadata": {
        "id": "TJFUuu2Y5hc-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Tokenized split"
      ],
      "metadata": {
        "id": "dndR7_CNI1jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Tensor Ready\n",
        "\n",
        "This is the data produced that we are most interested in. As we can see we will have:\n",
        "- a data structure `dataset` containing the training, validation and test set;\n",
        "- a tuple containing the word-to-token mappings for the training, validation and test set respectively."
      ],
      "metadata": {
        "id": "MvU7n1LboA6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "%%time\n",
        "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(**dataset_config, \n",
        "                                                                     training_json_path=path['training_json_path'], \n",
        "                                                                     save_pkl_path=path['save_pkl_path'], \n",
        "                                                                     tokenized=True)\n",
        "\n",
        "max_length_context = dataset.train.element_spec[0].shape[1]\n",
        "max_length_question = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "print(f'Sentences max lenght: {max_length_context}')\n",
        "print(f'Questions max lenght: {max_length_question}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax999y7aI75Y",
        "outputId": "aae99d46-2432-4af7-efa1-05cea66237fc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists! Loading from .pkl...\n",
            "\n",
            "Sentences max lenght: 389\n",
            "Questions max lenght: 40\n",
            "CPU times: user 10.4 s, sys: 408 ms, total: 10.8 s\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing such `NamedTuple` data structure (cfr `dataset`) is pretty simple, namely in a:\n",
        "1. tuple-way by accessing it like a list, e.g. `train = dataset[0]`,\n",
        "2. object-way by calling the instance parameters, e.g. `train = dataset.train`.\n",
        "\n",
        "The other two returned values are the word to index mappings for the context and question words respectively. In order to refer to a specific split simply call:\n",
        "1. for the training dataset,\n",
        "2. for the validation dataset,\n",
        "3. for the test dataset,"
      ],
      "metadata": {
        "id": "W7hARM_R2Kod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training vocab size for the context: {len(word_to_idx_context[0])}')\n",
        "print(f'Training vocab size for the question: {len(word_to_idx_question[0])}')\n",
        "print()\n",
        "print(f'Validation vocab size for the context: {len(word_to_idx_context[1])}')\n",
        "print(f'Validation vocab size for the question: {len(word_to_idx_question[1])}')\n",
        "print()\n",
        "print(f'Test vocab size for the context: {len(word_to_idx_context[2])}')\n",
        "print(f'Test vocab size for the question: {len(word_to_idx_question[2])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaRYgawxyXp",
        "outputId": "15c99692-ef18-400d-c965-c7a1f1267b41"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size for the context: 23465\n",
            "Training vocab size for the question: 11342\n",
            "\n",
            "Validation vocab size for the context: 26136\n",
            "Validation vocab size for the question: 12672\n",
            "\n",
            "Test vocab size for the context: 26567\n",
            "Test vocab size for the question: 12892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Standard"
      ],
      "metadata": {
        "id": "hlpy-ayWoEHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "# %%time\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = dataset_creator.call(num_examples=dataset_creator.og_n_samples+1,\n",
        "#                      BUFFER_SIZE=32000,\n",
        "#                      BATCH_SIZE=64,\n",
        "#                      random_seed=RANDOM_SEED,\n",
        "#                      tokenized=True,\n",
        "#                      tensor_type=False)\n",
        "\n",
        "# print(f'\\nSet target: {X_train.columns.values}')\n",
        "\n",
        "# print(f'Train set samples: {X_train.shape[0]}')\n",
        "# print(f'Validation set samples: {X_val.shape[0]}')\n",
        "# print(f'Test set samples: {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "vJFBk-r8eIcj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Original SQuAD dataset"
      ],
      "metadata": {
        "id": "r7qxjGzKJM2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original dataset\n",
        "# squad_df = dataset_creator.squad_df\n",
        "# print(f'[Info] SQuAD target: {list(squad_df.columns.values)}')\n",
        "# print(f'[Info] Shape: {squad_df.shape}')"
      ],
      "metadata": {
        "id": "9qhTAc5NErOk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. GloVe and embedding matrix"
      ],
      "metadata": {
        "id": "kx2f7Nn_4en9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GloVe:\n",
        "  def __init__(self, embedding_dimension):\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "\n",
        "    try:\n",
        "      self.embedding_model = KeyedVectors.load(f'./data/glove_model_{self.embedding_dimension}')\n",
        "    except FileNotFoundError:\n",
        "      print('[Warning] Model not found in local folder, please wait...')\n",
        "      self.embedding_model = self.load_glove()\n",
        "      self.embedding_model.save(f'./data/glove_model_{self.embedding_dimension}')  \n",
        "      print('Download finished. Model loaded!')\n",
        "\n",
        "  def load_glove(self):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained GloVe embedding model via gensim library.\n",
        "\n",
        "    We have a matrix that associate words to a vector of a user-defined dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(self.embedding_dimension)\n",
        "\n",
        "    try:\n",
        "      emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "      print(\"Generic error when loading GloVe\")\n",
        "      print(\"Check embedding dimension\")\n",
        "      raise e\n",
        "\n",
        "    emb_model = gloader.load(download_path)\n",
        "    return emb_model\n",
        "\n",
        "  def build_embedding_matrix(self, word_to_idx, vocab_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the \n",
        "        dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, self.embedding_dimension), dtype=np.float32)\n",
        "    oov_count = 0\n",
        "    oov_words = []\n",
        "\n",
        "    # For each word which is not present in the vocabulary we assign a random vector, otherwise we take the GloVe embedding\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      try:\n",
        "        embedding_vector = self.embedding_model[word]\n",
        "      except (KeyError, TypeError):\n",
        "        oov_count += 1\n",
        "        oov_words.append(word)\n",
        "        embedding_vector = np.random.uniform(low=-0.05, \n",
        "                                             high=0.05, \n",
        "                                             size=self.embedding_dimension)\n",
        "\n",
        "      embedding_matrix[idx] = embedding_vector\n",
        "    \n",
        "    print(f'\\n[Debug] {oov_count} OOV words found!\\n')\n",
        "    return embedding_matrix, oov_words"
      ],
      "metadata": {
        "id": "TRJ1NpSMqaJL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to initialize the handler with the desidered `embedding_dimension`. Then to build the embedding matrix with the pre-trained GloVe embeddings simply call the `build_embedding_matrix` method."
      ],
      "metadata": {
        "id": "gk-z8A5y3cpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Initalize the handler for GloVe\n",
        "glove_handler = GloVe(encoder_config['embedding_dimension'])\n",
        "\n",
        "# We will create the matrix by using only the words present in the training and validation set\n",
        "embedding_matrix_context, oov_words_context = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_context[1], \n",
        "    len(word_to_idx_context[1])+1)\n",
        "\n",
        "embedding_matrix_question, oov_words_question = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_question[1], \n",
        "    len(word_to_idx_question[1])+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUmvdCWGavSR",
        "outputId": "b562bfc7-8c9b-4314-d2a5-859c9d9906f6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26136/26136 [00:00<00:00, 177453.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 1706 OOV words found!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12672/12672 [00:00<00:00, 177390.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 652 OOV words found!\n",
            "\n",
            "CPU times: user 1.63 s, sys: 602 ms, total: 2.23 s\n",
            "Wall time: 2.68 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert both of them into tensor, but it is fine to also treat them as `numpy` array, still it is better to use the `tensorflow` fundamentals."
      ],
      "metadata": {
        "id": "qdLOk0pQu3Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_context = tf.convert_to_tensor(embedding_matrix_context)\n",
        "embedding_matrix_question = tf.convert_to_tensor(embedding_matrix_question)"
      ],
      "metadata": {
        "id": "EX6PvKTBsdSW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Definition"
      ],
      "metadata": {
        "id": "FF5Rtd4uqa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_context_batch, example_question_batch = next(iter(dataset.train))"
      ],
      "metadata": {
        "id": "GjdRysIvPoLc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Encoder\n",
        "We will use a bidirectional LSTM to encode the sentence,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\overrightarrow{b_t} &= \\overrightarrow{\\text{LSTM}}(x_t, \\overrightarrow{b_{t-1}})\\\\\n",
        "\\overleftarrow{b_t} &= \\overleftarrow{\\text{LSTM}}(x_t, \\overleftarrow{b_{t+1}})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\overrightarrow{b_t}$ is the hidden state at time step $t$ for the forward pass LSTM and $\\overleftarrow{b_t}$ for the backward pass."
      ],
      "metadata": {
        "id": "wjVfZgIIf1RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               context_vocab_size, \n",
        "               embedding_matrix,\n",
        "               embedding_dimension, \n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_context,\n",
        "               **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_context = tf.constant(max_length_context)\n",
        "\n",
        "    # Layers\n",
        "    self.inputs = Input(shape=(self.max_length_context,), name='encoder_input')\n",
        "    \n",
        "    self.embedding = Embedding(input_dim=context_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_context,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,\n",
        "                               mask_zero=False,\n",
        "                               name='encoder_embedding_layer')  \n",
        "    \n",
        "    # The LSTM forward pass\n",
        "    self.forward_lstm_layer = LSTM(units//4,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "    \n",
        "    # The LSTM backward pass\n",
        "    self.backward_lstm_layer = LSTM(units//4,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  go_backwards=True,\n",
        "                                  name='encoder_lstm_layer')\n",
        "\n",
        "    # The Bidirectional wrapper  \n",
        "    self.bidirectional_lstm = Bidirectional(self.forward_lstm_layer, \n",
        "                                            backward_layer=self.backward_lstm_layer, \n",
        "                                            name='encoder_bi_lstm',\n",
        "                                            merge_mode='concat')\n",
        "    \n",
        "  def call(self, inputs, state=None, training=True):\n",
        "    # x shape = (batch_size, max_length_context, embedding_dimension)\n",
        "    x = self.embedding(inputs)\n",
        "    output, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm(x, initial_state=state, training=training)\n",
        "\n",
        "    # encoder_outputs shape = (batch_size, max_length_context, units)\n",
        "    # forward_h shape = (batch_size, units//2)\n",
        "    # forward_c shape = (batch_size, units//2)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm(output, initial_state=(forward_h, forward_c, backward_h, backward_c), training=training)\n",
        "    \n",
        "    # op:concat shape = (batch_size, units)\n",
        "    h_concat = tf.concat([forward_h, backward_h], axis=1, name='hidden_concat')\n",
        "    c_concat = tf.concat([forward_c, backward_c], axis=1, name='cell_concat')\n",
        "    return encoder_outputs, (h_concat, c_concat)\n",
        "\n",
        "  # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    x = Input(shape=(self.max_length_context,), batch_size=self.batch_size)\n",
        "    return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(), \n",
        "        # to_file='encoder.png', dpi=96,              \n",
        "        show_shapes=True, show_layer_names=True,  \n",
        "        expand_nested=True                       \n",
        "    )"
      ],
      "metadata": {
        "id": "GK6Kd1XvqK22"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Test the encoder stack\n",
        "\n"
      ],
      "metadata": {
        "id": "fbjSxPGcFud_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_config['context_vocab_size'] = len(word_to_idx_context[1])\n",
        "encoder_config['max_length_context'] = dataset.train.element_spec[0].shape[1]\n",
        "\n",
        "encoder = Encoder(**encoder_config, embedding_matrix=embedding_matrix_context)\n",
        "encoder_outputs, encoder_state = encoder(inputs=example_context_batch)\n",
        "\n",
        "hidden_state, cell_state = encoder_state\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, max_length_context, units): {encoder_outputs.shape}')\n",
        "print(f'Hidden state shape: (batch_size, units): {hidden_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, units): {cell_state.shape}')"
      ],
      "metadata": {
        "id": "_ffteDMQyzmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33bd095-ca98-488e-a1e8-dff4b4162aa7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, max_length_context, units): (64, 389, 300)\n",
            "Hidden state shape: (batch_size, units): (64, 300)\n",
            "Cell state shape: (batch_size, units): (64, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is still missing an LSTM layer."
      ],
      "metadata": {
        "id": "-0HQt4ukvf5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.build_graph().summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng97TFpLRym2",
        "outputId": "1c491e0e-3a22-4539-ef28-c5b4b7b44456"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(64, 389)]          0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding_layer (Embed  (64, 389, 300)      7841100     ['input_4[0][0]']                \n",
            " ding)                                                                                            \n",
            "                                                                                                  \n",
            " encoder_bi_lstm (Bidirectional  [(64, 389, 300),    541200      ['encoder_embedding_layer[0][0]',\n",
            " )                               (64, 150),                       'encoder_bi_lstm[0][0]',        \n",
            "                                 (64, 150),                       'encoder_bi_lstm[0][1]',        \n",
            "                                 (64, 150),                       'encoder_bi_lstm[0][2]',        \n",
            "                                 (64, 150)]                       'encoder_bi_lstm[0][3]',        \n",
            "                                                                  'encoder_bi_lstm[0][4]']        \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)       (64, 300)            0           ['encoder_bi_lstm[1][1]',        \n",
            "                                                                  'encoder_bi_lstm[1][3]']        \n",
            "                                                                                                  \n",
            " tf.concat_5 (TFOpLambda)       (64, 300)            0           ['encoder_bi_lstm[1][2]',        \n",
            "                                                                  'encoder_bi_lstm[1][4]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,382,300\n",
            "Trainable params: 541,200\n",
            "Non-trainable params: 7,841,100\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "aaJFf-ELdIZo",
        "outputId": "95acb754-7c4c-4d81-9c48-af604eca8d4f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Decoder"
      ],
      "metadata": {
        "id": "dtM9nOQrf3jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Container classes\n",
        "# Reference :- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "class DecoderInput(NamedTuple):\n",
        "  new_token: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "73wq7CTiTQpX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(Layer):\n",
        "  # Still it is possible to use Luang's attention as an alternative\n",
        "  # Reference:- https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    self.W1 = Dense(units, use_bias=False, name='Wb1_attention_weights')\n",
        "    self.W2 = Dense(units, use_bias=False, name='Wb2_attention_weights')\n",
        "\n",
        "    self.attention = AdditiveAttention(use_scale=True)\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    \"\"\"\n",
        "    This layer takes 3 inputs:\n",
        "      - the query; this will be generated by the decoder, later,\n",
        "      - the value: the output of the encoder,\n",
        "      - the mask: to exclude the padding, i.e., context_batch != 0.\n",
        "    \"\"\"\n",
        "    #W1@ht\n",
        "    w1_query = self.W1(query)\n",
        "    #W2@hs\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask = [query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    \n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               question_vocab_size, \n",
        "               embedding_matrix, \n",
        "               embedding_dimension,\n",
        "               units, \n",
        "               batch_size, \n",
        "               max_length_question,\n",
        "               **kwargs):\n",
        "    \n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    self.batch_size = tf.constant(batch_size)\n",
        "    self.max_length_question = tf.constant(max_length_question)\n",
        "    self.embedding_dimension = tf.constant(embedding_dimension)\n",
        "    self.units = tf.constant(units)\n",
        "\n",
        "    # Layers definition\n",
        "    self.inputs = Input(shape=(None,), batch_size=self.batch_size)\n",
        "                        \n",
        "    # Embedding for the questions\n",
        "    self.embedding = Embedding(input_dim=question_vocab_size+1,\n",
        "                               output_dim=embedding_dimension,\n",
        "                               input_length=self.max_length_question,\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,  #?\n",
        "                               mask_zero=False,\n",
        "                               name='decoder_embedding_layer')\n",
        "    \n",
        "    # The LSTM layer\n",
        "    self.lstm_layer = LSTM(units//2,\n",
        "                          return_sequences=True,\n",
        "                          return_state=True,\n",
        "                          name='decoder_lstm_layer')\n",
        "    \n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(units//2)\n",
        "\n",
        "    self.Wt = Dense(units, activation=tf.math.tanh, use_bias=False, name='decoder_Wt_weights')\n",
        "\n",
        "    # For the word probabilities\n",
        "    self.Ws = Dense(question_vocab_size, activation=tf.nn.softmax, use_bias=False, name='decoder_Ws_weights')\n",
        "\n",
        "  def call(self, \n",
        "           inputs: DecoderInput, \n",
        "           state=None,\n",
        "           training=True) -> Tuple[DecoderOutput, Tuple[tf.Tensor]]:\n",
        "\n",
        "    # Lookup the embeddings for the questions\n",
        "    x = self.embedding(inputs.new_token)\n",
        "    # embedded_tensor shape: (batch_size, 1, embedding_dimension)\n",
        "    if tf.shape(x).shape == 2: x = tf.expand_dims(x, axis=1)\n",
        "\n",
        "    # Process one step with the RNN\n",
        "    # LSTM expects inputs of shape: (batch_size, timestep, feature)\n",
        "    cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(x, initial_state=state, training=training)\n",
        "    cell_output, hidden_dec_state, cell_dec_state = self.lstm_layer(cell_output, initial_state=(hidden_dec_state, cell_dec_state), training=training)\n",
        "    \n",
        "    # Use the LSTM cell output as the query for the attention over the encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=cell_output, \n",
        "        value=inputs.enc_output, \n",
        "        mask=inputs.mask)\n",
        "\n",
        "    # Join the context_vector and cell output [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    cell_output_and_context_vector = tf.concat([cell_output, context_vector], axis=-1)\n",
        "\n",
        "    # at = tanh(Wt@[ht, ct])\n",
        "    attention_vector = self.Wt(cell_output_and_context_vector, training=training)\n",
        "\n",
        "    # logits = softmax(Ws@at)\n",
        "    logits = self.Ws(attention_vector, training=training)\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), (hidden_dec_state, cell_dec_state)"
      ],
      "metadata": {
        "id": "V_-Lef2CqUW2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Test the decoder stack\n",
        "\n",
        "The decoder will take as input:\n",
        "1. `new_tokens`: the last token generated of shape `(batch_size, 1)`, namely the token obrained in the previous time step of the decoder (we will initialize the decoder with the `\"<sos>\"` token);\n",
        "2. `enc_output`: this is the representation produced by the `Encoder` of shape `(batch_size, max_length_context, enc_units)`;\n",
        "3. `mask`: this is the mask, that is a boolean tensor, indicating which tokens will be considered in the decoding of shape `(batch_size, max_length_context)`; \n",
        "4. `decoder_state`: the previous state of the decoder, namely the internal state of the decoder's LSTM (the paper suggests to input the hidden and cell state produced by the Bi-LSTM). The shape is `[(batch_size, enc_units), (batch_size, enc_units)]`.\n",
        "\n"
      ],
      "metadata": {
        "id": "IF5J42g1l-be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_config['question_vocab_size'] = len(word_to_idx_question[1])\n",
        "decoder_config['max_length_question'] = dataset.train.element_spec[1].shape[1]\n",
        "\n",
        "decoder = Decoder(**decoder_config, embedding_matrix=embedding_matrix_question)"
      ],
      "metadata": {
        "id": "kS0UBnMzTbie"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "start_tag_index = word_to_idx_question[2]['<sos>']\n",
        "first_token = tf.squeeze(tf.constant([[start_tag_index]] * decoder_config['batch_size']), axis=1)"
      ],
      "metadata": {
        "id": "KeMvqDnrTkf0"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, decoder_state = decoder(\n",
        "    inputs = DecoderInput(first_token, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = encoder_state\n",
        ")\n",
        "\n",
        "hidden_dec_state, cell_dec_state = decoder_state\n",
        "\n",
        "print(f'Logits shape: (batch_size, t, output_vocab_size) {decoder_result.logits.shape}')\n",
        "print(f'Hidden state shape: (batch_size, dec_units) {hidden_dec_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, dec_units) {cell_dec_state.shape}')"
      ],
      "metadata": {
        "id": "BF6PWsNYfmUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed83761-b7e7-4ab9-b5cb-24a29416d79c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (batch_size, t, output_vocab_size) (64, 1, 12672)\n",
            "Hidden state shape: (batch_size, dec_units) (64, 300)\n",
            "Cell state shape: (batch_size, dec_units) (64, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we cannot provide a detailed summary or a handy plot due to the fact that we pass to the decoder model a structured input which is not preferred by tensorflow."
      ],
      "metadata": {
        "id": "T9FZVz2QyLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AJWwcDacEj",
        "outputId": "4428c705-51ad-43e2-ee61-a304d1ab3223"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_embedding_layer (Em  multiple                 3801900   \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " decoder_lstm_layer (LSTM)   multiple                  721200    \n",
            "                                                                 \n",
            " bahdanau_attention_1 (Bahda  multiple                 180300    \n",
            " nauAttention)                                                   \n",
            "                                                                 \n",
            " decoder_Wt_weights (Dense)  multiple                  360000    \n",
            "                                                                 \n",
            " decoder_Ws_weights (Dense)  multiple                  7603200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,666,600\n",
            "Trainable params: 8,864,700\n",
            "Non-trainable params: 3,801,900\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving on: this means that the decoder will produce a vector of probabilities associated to each vocabulary word. That is, a vector of logits $l_b \\in \\mathbb{R}^{\\mathcal{V}}$ for each element $b$ in the batch, namely indicating the next probable token for a given sentence. Since they are logits they should sum up to `1.0`, evenutally a number really close to it. "
      ],
      "metadata": {
        "id": "BBIQDE0Sl6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result.logits[0, 0, :].numpy().sum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO-C7ELdlv1u",
        "outputId": "40542ab6-21a5-44d1-b03c-e64be80d7b5a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we sample a token according to the logits computed by the decoder."
      ],
      "metadata": {
        "id": "xrN_dTRtGdQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :],\n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "vocab = np.array(list(word_to_idx_question[1].keys()))\n",
        "\n",
        "first_word = list(vocab[tf.squeeze(sampled_tokens, axis=-1).numpy()])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "kGGwivobvx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284d9dc0-e3af-41d5-e3a5-5bd19c2c35b4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wombles', 'renovated', 'toxocariasis', 'korea', 'styles']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_result, _ = decoder(\n",
        "    inputs = DecoderInput(sampled_tokens, \n",
        "                          encoder_outputs,\n",
        "                          mask=(example_context_batch != 0)),\n",
        "    state = decoder_state\n",
        ")\n",
        "\n",
        "sampled_tokens = tf.random.categorical(\n",
        "    logits=decoder_result.logits[:, 0, :], \n",
        " \n",
        "    num_samples=1, \n",
        "    seed=dataset_config['random_seed'])\n",
        "sampled_tokens = tf.squeeze(sampled_tokens, axis=-1).numpy()\n",
        "\n",
        "first_word = list(vocab[sampled_tokens])\n",
        "first_word[:5]"
      ],
      "metadata": {
        "id": "Y2ixRaJZn271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e04084-6189-4614-fa2e-0524861281f3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sell', 'april', 'inexpensive', 'inspiration', 'summerwood']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training for QG"
      ],
      "metadata": {
        "id": "qIoySQKuIGlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Training checkpoints\n",
        "\n",
        "See [Manual Checkpointing](https://www.tensorflow.org/guide/checkpoint)."
      ],
      "metadata": {
        "id": "x6uDOVpginU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_prefix = os.path.join(path['checkpoint_dir'], \"tf_ckpt\")\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_iterator = iter(dataset.train) \n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "manager = tf.train.CheckpointManager(checkpoint,\n",
        "                                     checkpoint_prefix,\n",
        "                                     max_to_keep=3)"
      ],
      "metadata": {
        "id": "5tNJR_fRi7qd"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Loss\n",
        "\n",
        "The **QG** task is defined as finding $\\hat{y}$ such that:\n",
        "$$\n",
        "\\hat{y} = \\arg{\\max_y P(y|x)}  \n",
        "$$\n",
        "where $P(y|x)$ is the conditional log-likelihood of the predicted question sentence $y$ given the input $x$. Du et al. shown that the conditional probability could be factorized in:\n",
        "$$\n",
        "P(y|x) = \\prod_{t=1}^{|y|} P(y_t|x, y_{<t})\n",
        "$$\n",
        "where the probability of each $y_t$ is predicted based on all the words that have been generated upon time $t$, namely $y_{<t}$.\n",
        "\n",
        "This means that given a training corpus of sentence-question pairs $\\mathcal{S} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the objective is to minimize the negative log-likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L} &= - \\sum_{i=1}^N \\log P(y^{(i)}|x^{(i)}; \\theta)\\\\\n",
        "            &=  - \\sum_{i=1}^N \\sum_{j=1}^{|y^{(i)}|} \\log P (y_j^{(i)}|x^{(i)}, y_{<j}^{(i)}; \\theta)\n",
        "\\end{align*}\n",
        "$$\n",
        "We parameterize the probability of decoding each word $y_j$ by using an RNN:\n",
        "$$\n",
        "P(y_j|y_{<j}, s) = \\text{softmax}(g(h_j))\n",
        "$$\n",
        "where $g(.)$ is a transition function that outputs a vocabulary-sized vector."
      ],
      "metadata": {
        "id": "qyRA2RxZNsx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'conditional_ll_loss'\n",
        "\n",
        "    # The loss needs to work with logits since the decoder is outputting the most probable token\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True,\n",
        "        reduction='none'\n",
        "    )\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch\n",
        "    # Shape of y_true = (batch_size, )\n",
        "    # Shape of y_pred = (batch_size, 1, vocab_size)\n",
        "    loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    \n",
        "    # Mask of the losses on the padding\n",
        "    mask = tf.math.not_equal(y_true, 0)\n",
        "    loss = tf.boolean_mask(loss, mask)\n",
        "    loss = tf.reduce_sum(loss)\n",
        "\n",
        "    # Return the total\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IP_UunM3MUtF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Metrics\n",
        "\n",
        "The metrics used during training will be:\n",
        "1. perplexity,\n",
        "2. masked accuracy,\n",
        "3. masked f1 score.\n",
        "\n",
        "They will be implemented by exploiting the `Metric` object in `tensorflow`. See [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) for more."
      ],
      "metadata": {
        "id": "8WOVZj966EPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='perplexity', **kwargs):\n",
        "    super(Perplexity, self).__init__(name=name, **kwargs)\n",
        "    self.scores = self.add_weight(name='perplexity_scores', initializer='zeros')\n",
        "\n",
        "  def update_state(self, loss):\n",
        "    \"\"\"\n",
        "    Reference :- https://www.surgehq.ai/blog/how-good-is-your-chatbot-an-introduction-to-perplexity-in-nlp\n",
        "    \"\"\"\n",
        "    self.scores.assign(tf.exp(loss))\n",
        "\n",
        "  def result(self): return self.scores\n",
        "  def reset_states(self): self.scores.assign(0)\n",
        "\n",
        "# Also the accuracy should mask the padding\n",
        "class MaskedAccuracy(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='masked_accuracy',**kwargs):\n",
        "    super(MaskedAccuracy, self).__init__(name=name, **kwargs)\n",
        "    self.scores = self.add_weight(name='accuracy_scores', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # y_pred = tf.cast(tf.math.argmax(y_pred, axis=-1), dtype=tf.int32)\n",
        "\n",
        "    # We mask since we are not interested in the final accuracy score\n",
        "    mask = tf.cast(tf.math.greater(y_true, 0), dtype=tf.float32)\n",
        "\n",
        "    correct = tf.cast(tf.math.equal(y_true, y_pred), dtype=tf.float32)\n",
        "    correct = tf.math.reduce_sum(mask * correct)\n",
        "    total_legit = tf.math.reduce_sum(mask)\n",
        "\n",
        "    self.scores.assign(correct / total_legit)\n",
        "\n",
        "  def result(self): return self.scores\n",
        "  def reset_states(self): self.scores.assign(0)\n",
        "\n",
        "# DOES NOT WORK\n",
        "class MaskedF1Score(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='masked_f1', **kwargs):\n",
        "    super(MaskedF1Score, self).__init__(name='f1_score', **kwargs)\n",
        "    self.scores = self.add_weight(name='masked_f1_scores', initializer='zeros')\n",
        "    self.precision_fn = tf.keras.metrics.Precision(thresholds=0.5)\n",
        "    self.recall_fn = tf.keras.metrics.Recall(thresholds=0.5)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # We mask since we are not interested in the final accuracy score\n",
        "    mask = tf.cast(tf.math.greater(y_true, 0), dtype=tf.float32)\n",
        "\n",
        "    p = self.precision_fn(y_true, y_pred)\n",
        "    r = self.recall_fn(y_true, y_pred)\n",
        "    # since f1 is a variable, we use assign\n",
        "    self.scores.assign_add(2 * ((p * r) / (p + r + 1e-6)))\n",
        "\n",
        "  def result(self): return self.scores\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.precision_fn.reset_states()\n",
        "    self.recall_fn.reset_states()\n",
        "    self.scores.assign(0)"
      ],
      "metadata": {
        "id": "xbK72c5r6LWP"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Perplexity()"
      ],
      "metadata": {
        "id": "H1lzpAeoFDs3"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.update_state(9.)"
      ],
      "metadata": {
        "id": "056Bz-ySFGBA"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViYa8JFeFIVk",
        "outputId": "f991cd82-6050-4371-e009-2fc45f1b93d4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=8103.084>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)"
      ],
      "metadata": {
        "id": "LzkwhKOTH5WB"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ta = ta.write(0, [3, 2, 1])\n",
        "ta = ta.write(1, [1, 1, 1])\n",
        "ta.stack()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eAbIvKcH72W",
        "outputId": "7ff90854-7a4e-40f4-ac3a-4f56ae999a10"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[3., 2., 1.],\n",
              "       [1., 1., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 QG model and training step implementation\n",
        "\n",
        "The training step should:\n",
        "1. Run the encoder on the `input_tokens` to get the `encoder_outputs`, `hidden_state` and `cell_state`. "
      ],
      "metadata": {
        "id": "iwLiPsCyNuor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from platform import python_version_tuple\n",
        "class QGeneratorTrainer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               context_vocab_size,\n",
        "               question_vocab_size,\n",
        "               embedding_dimension,\n",
        "               embedding_matrix_context,\n",
        "               embedding_matrix_question,\n",
        "               units,\n",
        "               batch_size,\n",
        "               max_length_context,\n",
        "               max_length_question,\n",
        "               use_tf_function=True):\n",
        "    \"\"\"\n",
        "    Prepare the model for the training. It builds the both the encoder and the decoder.\n",
        "    Also it defines a wrapper to use the tf.function compilation for the tensorflow computational\n",
        "    graph.\n",
        "    \"\"\"\n",
        "    self.max_length_question = max_length_question\n",
        "\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(\n",
        "        context_vocab_size,\n",
        "        embedding_matrix_context,\n",
        "        embedding_dimension,\n",
        "        units,\n",
        "        batch_size,\n",
        "        max_length_context)\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        question_vocab_size,\n",
        "        embedding_matrix_question,\n",
        "        embedding_dimension,\n",
        "        units, \n",
        "        batch_size,\n",
        "        max_length_question)\n",
        "\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "    self.perplexity_metric = Perplexity()\n",
        "    self.accuracy_metric = MaskedAccuracy()\n",
        "    self.f1_metric = MaskedF1Score() \n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    # We list our `Metric` objects here so that `reset_states()` can be\n",
        "    # called automatically at the start of each epoch\n",
        "    # or at the start of `evaluate()`.\n",
        "    # If you don't implement this property, you have to call\n",
        "    # `reset_states()` yourself at the time of your choosing.\n",
        "    return [self.perplexity_metric, self.accuracy_metric]\n",
        "\n",
        "  def _train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Optimization step for a batch.\n",
        "    \"\"\"\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      encoder_output, encoder_state = self.encoder(context, training=True)\n",
        "\n",
        "      # The decoder should be initialized with the encoder last state \n",
        "      decoder_state = encoder_state \n",
        "      # pred_question = tf.fill([question.shape[0], 1,], question[0][0].numpy())\n",
        "      # We collect the question predicted by the decoder\n",
        "      pred_question = ta = tf.TensorArray(tf.int64, size=2, dynamic_size=True, clear_after_read=False)\n",
        "      loss = tf.constant(0.0)\n",
        "      t = 0\n",
        "\n",
        "      # Reference :- https://www.tensorflow.org/guide/function\n",
        "      # We have to run the decoder for all the length of the question \n",
        "      while t < (self.max_length_question - 1):\n",
        "        # We have to pass two tokens:\n",
        "        #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "        #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "        new_token = tf.gather(question, t, axis=1)\n",
        "        target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "        step_loss, decoder_state, pred_token = self.step_decoder(\n",
        "            (new_token, target_token),\n",
        "            context_mask,\n",
        "            encoder_output,\n",
        "            decoder_state, \n",
        "            training=True)\n",
        "\n",
        "        pred_question = pred_question.write(t, tf.transpose(pred_token))\n",
        "\n",
        "        loss = loss + step_loss\n",
        "        t = t + 1\n",
        "\n",
        "      pred_question = pred_question.write(t, tf.transpose(pred_token))\n",
        "      # Average the loss for all the legit tokens\n",
        "      avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "\n",
        "    pred_question = tf.transpose(pred_question.concat())\n",
        "\n",
        "    # Compute gradients\n",
        "    tr_variables = self.trainable_variables\n",
        "    grads = tape.gradient(avg_loss, tr_variables)\n",
        "    \n",
        "    # Apply some clipping (by norm) as done in the paper and update the weights\n",
        "    grads = [tf.clip_by_norm(grads[i], 5.0) for i in range(len(tr_variables))]\n",
        "    self.optimizer.apply_gradients(zip(grads, tr_variables))\n",
        "\n",
        "    # Compute metrics\n",
        "    self.perplexity_metric.update_state(avg_loss)\n",
        "\n",
        "    print(question.shape, pred_question.shape)\n",
        "\n",
        "    return {'batch_loss': avg_loss, 'perplexity': self.perplexity_metric.result()}\n",
        "  \n",
        "  @tf.function\n",
        "  def step_decoder(self, \n",
        "                   tokens,\n",
        "                   context_mask,\n",
        "                   encoder_output,\n",
        "                   decoder_state,\n",
        "                   training):\n",
        "    \"\"\"\n",
        "    Run a single iteration of the decoder and computers the incremental loss between the\n",
        "    produced token and the token in the target input.\n",
        "    \"\"\"\n",
        "    new_token, target_token = tokens\n",
        "    \n",
        "    # Run the decoder one time\n",
        "    decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=context_mask),\n",
        "        state = decoder_state, \n",
        "        training = training)\n",
        "  \n",
        "    y_true = target_token\n",
        "    y_pred = decoder_result.logits\n",
        "\n",
        "    step_loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "    return step_loss, decoder_state, y_pred\n",
        "\n",
        "  def __get_mask(self, tokens): return tf.math.not_equal(tokens, 0)"
      ],
      "metadata": {
        "id": "8xSn_StMq9cx",
        "cellView": "code"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model = QGeneratorTrainer(**encoder_config,\n",
        "                             question_vocab_size=decoder_config['question_vocab_size'],\n",
        "                             max_length_question=decoder_config['max_length_question'],\n",
        "                             embedding_matrix_context=embedding_matrix_context,\n",
        "                             embedding_matrix_question=embedding_matrix_question,\n",
        "                             use_tf_function=False)\n",
        "\n",
        "# We do not pass any metric here since they are already in the model \n",
        "# Reference :- https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
        "train_model.compile(\n",
        "    optimizer=trainer_config['optimizer'],\n",
        "    loss=MaskedLoss()\n",
        ")"
      ],
      "metadata": {
        "id": "zwKc0rvrIWkg"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 Simple Training\n",
        "The first call with `use_tf_function=True` will be slow since it has to trace the function. So be patient or try `use_tf_function=False` 😀"
      ],
      "metadata": {
        "id": "SPSDqU3_3nOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataset.train))\n",
        "train_model.train_step(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf5JzL9T47Ha",
        "outputId": "9643ad30-dbe4-487a-c2ed-df6c2b8a8f15"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 40) (64, 40)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=8.805988>,\n",
              " 'perplexity': <tf.Tensor: shape=(), dtype=float32, numpy=6674.0913>}"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# losses = []\n",
        "# for n in tqdm(range(10)):\n",
        "#   # print('.', end='')\n",
        "#   logs = qg_model.train_step(next(iter(dataset.train)))\n",
        "#   losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "# print()\n",
        "# plt.plot(losses)\n",
        "# print()\n",
        "# print(losses)"
      ],
      "metadata": {
        "id": "E0ZatZ4g-5iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Tensorboard"
      ],
      "metadata": {
        "id": "0imUvBAg14Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Train the model"
      ],
      "metadata": {
        "id": "fF8BfykX5faH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key) -> None:\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_ends(self, n, logs):\n",
        "    self.logs.append(logs[self.key])"
      ],
      "metadata": {
        "id": "uhSLln405mfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_loss = BatchLogs('batch_loss')\n",
        "perplexity = BatchLogs('perplexity')\n",
        "history = qg_model.fit(\n",
        "    dataset.train, \n",
        "    epochs=trainer_config['epochs'], \n",
        "    callbacks=[batch_loss, perplexity],\n",
        "    validation_data=dataset.val, \n",
        "    verbose='auto',\n",
        "    use_multiprocessing = True\n",
        "    )"
      ],
      "metadata": {
        "id": "ys97OwVn61UT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "fcb3b3ba-61a1-46d5-b389-0d2867a61cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 21/112 [====>.........................] - ETA: 34:47 - batch_loss: 8.7730 - perplexity: 6460.0142"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8556512b43b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Evaluation for QG\n"
      ],
      "metadata": {
        "id": "ZC-NjKElfqNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorEvaluator(tf.Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def test_step(self, inputs):\n",
        "    context, question = inputs\n",
        "\n",
        "    # Generate teh mask for both the context and the question\n",
        "    context_mask = self.__get_mask(context)\n",
        "    question_mask = self.__get_mask(question)\n",
        "\n",
        "    encoder_output, encoder_state = self.encoder(context, training=False)\n",
        "    decoder_state = encoder_state\n",
        "    loss = tf.constant(0.0)\n",
        "    t = 0\n",
        "\n",
        "    # Reference :- https://www.tensorflow.org/guide/function\n",
        "    # We have to run the decoder for all the length of the question \n",
        "    while t < (self.max_length_question - 1):\n",
        "      # We have to pass two tokens:\n",
        "      #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "      #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "      new_token = tf.gather(question, t, axis=1)\n",
        "      target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "      step_loss, metric_value, decoder_state = self.step_decoder(\n",
        "          (new_token, target_token),\n",
        "          context_mask,\n",
        "          encoder_output,\n",
        "          decoder_state, \n",
        "          training=True)\n",
        "\n",
        "      loss = loss + step_loss\n",
        "      # self.custom_metric_mean.update_state(metric_value)\n",
        "      t = t + 1\n",
        "\n",
        "    # Average the loss for all the legit tokens\n",
        "    avg_loss = loss / tf.math.reduce_sum(tf.cast(question_mask, dtype=loss.dtype))\n",
        "    # return {f'batch_loss': avg_loss, f'batch_perplexity': self.custom_metric_mean.result()}\n",
        "    return {f'batch_loss': avg_loss}"
      ],
      "metadata": {
        "id": "TqHNOZo8Kld7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Inference for QG\n",
        "In this section we will provide the class and the methods for the inference part. More specifically, both auxiliary and inferencing methods:\n",
        "1. `token_to_string()`:\n",
        "2. `string_to_token()`:\n",
        "3. `create_mask()`:\n",
        "4. `temperature_sampling()`:\n",
        "5. `generate_question()`:"
      ],
      "metadata": {
        "id": "ezgR7c68_0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGeneratorInference(tf.Module):\n",
        "  def __init__(self, encoder, decoder, tokenizer, word_to_idx, use_tf_function):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_idx = word_to_idx\n",
        "   \n",
        "    self.result_tokens = None\n",
        "    self.result_text = None\n",
        "    self.token_mask = self.create_mask()\n",
        "\n",
        "    self.start_idx = word_to_idx['<sos>']\n",
        "    self.end_idx = word_to_idx['<eos>']\n",
        "    self.use_tf_function = False\n",
        "\n",
        "  def token_to_string(self, result_tokens: tf.Tensor):  \n",
        "    \"\"\"\n",
        "    This method converts token IDs to text by using a given mapping.\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    list_text = self.tokenizer.sequences_to_texts(list_tokens)\n",
        "    list_text = tf.convert_to_tensor([list_text])\n",
        "    result_text = tf.strings.reduce_join(list_text, axis=0, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    \n",
        "    self.result_tokens = result_tokens\n",
        "    self.result_text = result_text\n",
        "    return result_text\n",
        "\n",
        "  def string_to_token(self, result_str: tf.Tensor):\n",
        "    \"\"\"\n",
        "    This method converts texts to token IDs by using a given mapping.\n",
        "    \"\"\"  \n",
        "    list_str = [s.decode(\"utf-8\") for s in result_str.numpy().tolist()]\n",
        "    list_tokens = self.tokenizer.texts_to_sequences(list_str)\n",
        "    list_tokens = tf.convert_to_tensor(list_tokens, dtype=tf.int64)\n",
        "    result_tokens = tf.squeeze(tf.split(list_tokens, num_or_size_splits=list_tokens.shape[0], axis=0), axis=1)\n",
        "\n",
        "    return result_tokens\n",
        "  \n",
        "  def create_mask(self):\n",
        "    \"\"\"\n",
        "    This method creates a mask for the padding, the unknwon words and the start/ending tokens.\n",
        "    \"\"\"\n",
        "    masked_words = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "    token_mask_ids = [self.tokenizer.word_index[mask] for mask in masked_words]\n",
        "\n",
        "    token_mask = np.zeros(shape=(len(self.word_to_idx),), dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    return token_mask\n",
        "\n",
        "  # evaluate or predict?\n",
        "  def evaluate(self, inputs, max_length, return_attention, mode='greedy', temperature=0.5):\n",
        "    \"\"\"\n",
        "    Wrapper that switches on and off the tf.function compilation for performance, see the \n",
        "    tensorflow documentation for the computation graph.\n",
        "    \"\"\"\n",
        "    if mode == 'greedy':\n",
        "      if self.use_tf_function:\n",
        "        return self._tf_generate_greedy(inputs, max_length, temperature, return_attention)\n",
        "      else:\n",
        "        return self._generate_greedy(inputs, max_length, temperature, return_attention)\n",
        "    elif mode == 'beam':\n",
        "      return self._generate_beam(inputs, max_length, return_attention)\n",
        "\n",
        "  @tf.function\n",
        "  def _tf_generate_greedy(self, inputs, max_length, temperature, return_attention):\n",
        "    return self._generate_question_greedy(inputs, max_length, return_attention, temperature)\n",
        "  \n",
        "  def _generate_greedy(self, \n",
        "                        inputs,\n",
        "                        max_length,\n",
        "                        return_attention,\n",
        "                        temperature):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    # Similarly for what it has been done in the train step\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    # Generate the first token of each sentence, that is the <sos> token\n",
        "    new_token = tf.fill([batch_size, 1], self.start_idx)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    timestep = 0\n",
        "    \n",
        "    while timestep < max_length:\n",
        "      # Decode the token at the next timestep\n",
        "      decoder_result, decoder_state = self.decoder(\n",
        "        inputs = DecoderInput(\n",
        "            new_token=new_token,\n",
        "            enc_output=encoder_output,\n",
        "            mask=(inputs != 0)),\n",
        "        state = decoder_state)\n",
        "      \n",
        "      attention.append(decoder_result.attention_weights)\n",
        "\n",
        "      # Sample the new token accordingly to the distribution produced by the decoder\n",
        "      new_token = self.temperature_sampling(decoder_result.logits, temperature)\n",
        "\n",
        "      # if a sequence has reached <eos> set it as done\n",
        "      # MISSING PART\n",
        "\n",
        "      result_tokens.append(new_token)\n",
        "\n",
        "      timestep = timestep + 1\n",
        "    \n",
        "    # MISSING\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.token_to_string(result_tokens)\n",
        "\n",
        "    attention_stack = tf.concat(attention, axis=-1)\n",
        "\n",
        "    # HANDLING UNK WORDS\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}\n",
        "\n",
        "  def temperature_sampling(self, logits, temperature):\n",
        "    \"\"\"\n",
        "\n",
        "    For the temperature choice see here:\n",
        "      Reference :- https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "    \"\"\"\n",
        "    # First of all we use broadcast the generated mask to the expected logits' shape\n",
        "    # token_mask shape: (batch_size, timestep, vocab_size)\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # The logits for all the tokens that have to not be used are set top -1.0\n",
        "    logits = tf.where(token_mask, -1.0, logits)\n",
        "\n",
        "    # Freezing function\n",
        "    # Higher temperature -> greater variety\n",
        "    # Lower temperature -> grammatically correct\n",
        "    if temperature == 0.0:\n",
        "      # the freezing function is the argmax, behaving like a greedy search\n",
        "      new_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      # the freezing function now scales the logits.\n",
        "      # for temperature == 1.0 is the identity function\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_token = tf.random.categorical(logits / temperature, num_samples=1)\n",
        "    return new_token\n",
        "\n",
        "  # def _beam_search(self, k, max_length, encoder_output, encoder_state, mask):\n",
        "  #   decoder_states = list()\n",
        "    \n",
        "  #   for i in range(k):\n",
        "  #     decoder_states.append(encoder_state)\n",
        "\n",
        "  #   print(decoder_states[0])\n",
        "  #   # decoders = []\n",
        "    \n",
        "  #   sequences = [[list(), 0.0, 0]]\n",
        "  #   timestep = 0\n",
        "  #   new_tokens = tf.fill([1, k, 1], self.start_idx)\n",
        "  #   all_candidates = list()\n",
        "    \n",
        "  #   while timestep < max_length:\n",
        "  #     timestep = timestep+1\n",
        "      \n",
        "  #     for i in range(len(sequences)):\n",
        "  #       seq, score, _ = sequences[i]\n",
        "\n",
        "  #       decoder_result, decoder_state = self.decoder(\n",
        "  #         inputs = DecoderInput(\n",
        "  #             new_token=new_tokens[i],\n",
        "  #             enc_output=encoder_output,\n",
        "  #             mask=mask),\n",
        "  #         state = decoder_states[i])\n",
        "\n",
        "  #       decoder_states[i] = decoder_state\n",
        "        \n",
        "  #       for j in range(decoder.logits.shape[-1]):\n",
        "  #         candidate = [seq + [j], score - np.log(decoder.logits[0][j]), i]\n",
        "  #         all_candidates.append(candidate)\n",
        "  #       # order all candidates by score\n",
        "  #     ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "  #     sequences = ordered[:k]\n",
        "  #     new_tokens = list()\n",
        "  #     # new_decoders = []\n",
        "  #     new_decoder_states = []\n",
        "  #     for k in range(k):\n",
        "  #       new_tokens.append(all_candidates[k][0][-1])\n",
        "  #       # new_decoders.append(decoders[all_candidates[k][2]])\n",
        "  #       new_decoder_states.append(decoder_states[all_candidates[k][2]])\n",
        "  #     # decoders = new_decoders\n",
        "  #     decoder_states = new_decoder_states\n",
        "  #   return sequences\n",
        "\n",
        "  # def _generate_question_beam_search(self, inputs, max_length, return_attention=True):\n",
        "  #   batch_size = tf.shape(inputs)[0]\n",
        "  #   encoder_output, encoder_state = self.encoder(inputs)\n",
        "\n",
        "  #   hidden_state, cell_state = encoder_state\n",
        "  #   batch_sequences = []\n",
        "  #   for i in range(batch_size):\n",
        "  #     encoder_state_batch = (hidden_state[i, :], cell_state[i, :])\n",
        "  #     sequences = self._beam_search(3, \n",
        "  #                                   max_length, \n",
        "  #                                   encoder_output[i, :, :], \n",
        "  #                                   encoder_state_batch, \n",
        "  #                                   (inputs[i,] != 0))\n",
        "  #     batch_sequences.append(sequences)\n",
        "  #   return batch_sequences\n"
      ],
      "metadata": {
        "id": "gn2NTxAq_2sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator = QGeneratorInference(qg_model.encoder, \n",
        "                                   qg_model.decoder, \n",
        "                                   dataset_creator.tokenizer_question, \n",
        "                                   word_to_idx=word_to_idx_question[1], \n",
        "                                   use_tf_function=True)"
      ],
      "metadata": {
        "id": "1Q2pWJ5YDfqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_logits = tf.random.normal([5, 1, len(word_to_idx_question[1])])\n",
        "example_output_tokens = qg_generator.temperature_sampling(example_logits, temperature=1.0)"
      ],
      "metadata": {
        "id": "fklYEd1e6XOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qg_generator.evaluate(example_output_tokens, max_length=10, return_attention=False, mode='greedy')"
      ],
      "metadata": {
        "id": "7RO-7QCL_MSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}