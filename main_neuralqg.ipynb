{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erhtric/NeuralQuestionGenerationNLP/blob/master/main_neuralqg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OU-wpGL18xj"
      },
      "source": [
        "This is the main file: its purpouse is to collect all the code coming from the coding pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Miscellanous"
      ],
      "metadata": {
        "id": "ZS1Y56XayL9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard"
      ],
      "metadata": {
        "id": "dm3zC-RigXJH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Libraries"
      ],
      "metadata": {
        "id": "xl7H67iPmZHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install datasets\n",
        "%pip install keras-tuner --upgrade\n",
        "%pip install rouge_score\n",
        "#!pip install tflearn - batchnorm"
      ],
      "metadata": {
        "id": "jJ6IfruxRu7S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLvONYDvKAHz",
        "outputId": "f607db63-e5a0-4b6d-8887-0f3663aa9c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import typing\n",
        "from typing import Any, Tuple, List, NamedTuple\n",
        "import spacy\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "from gensim.models import KeyedVectors\n",
        "#import seaborn as sns\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "#import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Layer, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    Dense, \n",
        "    Bidirectional, \n",
        "    Input, \n",
        "    AdditiveAttention,\n",
        "    Dropout)\n",
        "\n",
        "# import keras_nlp\n",
        "import nltk\n",
        "#from nltk import punkt, pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMLxr4LDpZGc",
        "outputId": "84efebda-6c39-436f-a314-e49136604588"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 Directories"
      ],
      "metadata": {
        "id": "Y8NrwPONoVQ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqKVWPel_ybt"
      },
      "source": [
        "Commands to prepare the folder to accomodate data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbwRWifNfo2k",
        "outputId": "1b89bb0d-e60e-42f8-e24f-b23cc381933a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "4 drwx------ 5 root root 4096 Jul  5 15:13 \u001b[0m\u001b[01;34mdrive\u001b[0m/\n",
            "4 drwxr-xr-x 1 root root 4096 Jun 29 13:44 \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKVGy7JPJCoi",
        "outputId": "0af577db-51f4-4684-f52b-031d9f4fda2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cVw6eUwM-dRL9BhqtXULyOqeXDrYkwmH/NLP/Project/qg_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/NLP/Project/qg_project/\n",
        "%pwd\n",
        "\n",
        "# disable chained assignments to avoid annoying warning\n",
        "pd.options.mode.chained_assignment = None "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ikmZGjUlitqS"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./data'):\n",
        "  print('Data folder does not exists. Creating it')\n",
        "  os.makedirs('./data')\n",
        "\n",
        "if not os.path.exists('./training_checkpoints'):\n",
        "  print('Training checkpoint folder does not exists. Creating it')\n",
        "  os.makedirs('./training_checkpoints')\n",
        "\n",
        "if not os.path.exists('./logs'):\n",
        "  print('Tensorboard Logs folder does not exists. Creating it')\n",
        "  os.makedirs('./logs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hyzKyj_-GjI"
      },
      "source": [
        "## 0.3 Configuration\n",
        "This is for the `configuration.json` file, or something similar: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5bS3uLkE-Mvf"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "units = 600\n",
        "keras_tuner = False\n",
        "\n",
        "dataset_config = {\n",
        "    'num_examples': 90000,\n",
        "    # 'num_examples': 100,\n",
        "    'train_size': 0.65,\n",
        "    'test_size': 0.40,\n",
        "    'num_words_context': 45000,\n",
        "    'num_words_question': 28000,\n",
        "    'buffer_size': 32000,\n",
        "    'batch_size': batch_size,\n",
        "    'random_seed': 13,\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    'enc_units': units,\n",
        "    'dec_units': units,\n",
        "    'max_length_context': None,\n",
        "    'max_length_question': None,\n",
        "    'dropout_rate': None,\n",
        "    'regularizer': None,\n",
        "}\n",
        "\n",
        "path = {\n",
        "    'training_json_path': \"./data/training_set.json\",\n",
        "    'save_pkl_path': \"./data/squadv2.pkl\",\n",
        "    # 'save_pkl_path': \"./data/squad.pkl\",\n",
        "    'checkpoint_dir': \"./training_checkpoints\",\n",
        "}\n",
        "\n",
        "evaluation_config = {\n",
        "    'temperature' : 0.7\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFBKCIE3Jxf2"
      },
      "source": [
        "# 1. Dataset Definition\n",
        "\n",
        "Things to do:\n",
        "1. Add to each sentence $x$ a start of sequence `<SOS>` tag and end of sequence `<EOS>` tag,\n",
        "2. Clean the sentences by removing special chars,\n",
        "3. Perform other preprocessing steps,\n",
        "4. Create a **vocabulary** with a word-to-index and index-to-word mappings by using a **tokenizer**, \n",
        "5. Extract the sentences that contain an answer and use them as input features, whereas the question will be our target\n",
        "6. Pad each context to maximum length.\n",
        "\n",
        "The resulting data that will be used hereinafter will be of type `tf.data.Dataset`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XyCKxqwRZelj"
      },
      "outputs": [],
      "source": [
        "class Dataset(NamedTuple):\n",
        "  \"\"\"\n",
        "  This class represent a a 3-way split processed dataset. \n",
        "  \"\"\"\n",
        "  # Reference :- https://github.com/topper-123/Articles/blob/master/New-interesting-data-types-in-Python3.rst\n",
        "  train: tf.data.Dataset\n",
        "  val: tf.data.Dataset\n",
        "  test: tf.data.Dataset\n",
        "\n",
        "class SQuAD:\n",
        "  def __init__(self):\n",
        "    self.random_seed = None\n",
        "    self.squad_df = None\n",
        "    self.preproc_squad_df = None\n",
        "    self.tokenizer = None\n",
        "    self.buffer_size = 0\n",
        "\n",
        "  def __call__(self, dataset_config, path, tokenized=True, tensor_type=True):\n",
        "    \"\"\"The call() method loads the SQuAD dataset, preprocess it and optionally it returns \n",
        "    it tokenized. Moreover it also perform a 3-way split.\n",
        "\n",
        "    Args:\n",
        "        num_examples (int): number of examples to be taken from the original SQuAD dataset\n",
        "        num_words (int): the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept. \n",
        "        buffer_size (int): buffer size for the shuffling operation\n",
        "        batch_size (int): size of the batches\n",
        "        tokenized (boolean): specifies if the context and question data should be both tokenized\n",
        "        pos_ner_tag (boolean):\n",
        "        tensor_type (boolean): \n",
        "\n",
        "    Returns (depending on the input parameters):\n",
        "        pd.DataFrame: training dataset\n",
        "        pd.DataFrame: validation dataset\n",
        "        pd.DataFrame: testing dataset\n",
        "          OR\n",
        "        NamedTuple: dataset, (dict, dict, dict)\n",
        "    \"\"\"\n",
        "    self.random_seed = dataset_config['random_seed']\n",
        "    self.buffer_size = dataset_config['buffer_size']\n",
        "    self.batch_size = dataset_config['batch_size']\n",
        "    self.train_size = dataset_config['train_size']\n",
        "    self.test_size = dataset_config['test_size']\n",
        "    self.training_json_path = path['training_json_path']\n",
        "    self.save_pkl_path = path['save_pkl_path']\n",
        "    self.max_length_context = 0\n",
        "    self.max_length_question = 0\n",
        "\n",
        "    # Load dataset from file\n",
        "    self.load_dataset(dataset_config['num_examples'])\n",
        "    # Extract answer\n",
        "    self.extract_answer()\n",
        "    # Preprocess context and question\n",
        "    self.preprocess()\n",
        "    self.compute_max_length()\n",
        "\n",
        "    # Perform splitting\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = self.split_train_val(self.preproc_squad_df)\n",
        "\n",
        "    # Initialize Tokenizer for the source: in our case the context sentences\n",
        "    self.tokenizer_context = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=dataset_config['num_words_context'])\n",
        "    # initialize also for the target, namely the question sentences\n",
        "    self.tokenizer_question = tf.keras.preprocessing.text.Tokenizer(filters='', \n",
        "                                                                   oov_token='<unk>',\n",
        "                                                                   num_words=dataset_config['num_words_question'])\n",
        "\n",
        "    if tokenized:\n",
        "      X_train_tokenized, word_to_idx_train_context = self.__tokenize_context(X_train, test=False)\n",
        "      y_train_tokenized, word_to_idx_train_question = self.__tokenize_question(y_train, test=False)\n",
        "\n",
        "      # update the max length for the other splits\n",
        "      self.max_length_context = X_train_tokenized.context.iloc[0].shape[0]\n",
        "      self.max_length_question = y_train_tokenized.iloc[0].shape[0]\n",
        "\n",
        "      X_val_tokenized, word_to_idx_val_context = self.__tokenize_context(X_val, test=False)\n",
        "      y_val_tokenized, word_to_idx_val_question = self.__tokenize_question(y_val, test=False)\n",
        "\n",
        "      # The test set should handle the oov words as unkwown words\n",
        "      X_test_tokenized, word_to_idx_test_context = self.__tokenize_context(X_test, test=True)\n",
        "      y_test_tokenized, word_to_idx_test_question = self.__tokenize_question(y_test, test=True)\n",
        "\n",
        "      word_to_idx_context = (word_to_idx_train_context, word_to_idx_val_context, word_to_idx_test_context)\n",
        "      word_to_idx_question = (word_to_idx_train_question, word_to_idx_val_question, word_to_idx_test_question)\n",
        "      \n",
        "      if tensor_type:\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "        # Returns tf.Data.Dataset objects (tokenized)\n",
        "        train_dataset = self.to_tensor(X_train_tokenized, y_train_tokenized)\n",
        "        val_dataset = self.to_tensor(X_val_tokenized, y_val_tokenized)\n",
        "        test_dataset = self.to_tensor(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "        # Configure the dataset for performance\n",
        "        train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        dataset = Dataset(\n",
        "            train=train_dataset, \n",
        "            val=val_dataset,\n",
        "            test=test_dataset)\n",
        "\n",
        "        return dataset, word_to_idx_context, word_to_idx_question\n",
        "      else:\n",
        "        # Returns pd.DataFrame objects (tokenized)\n",
        "        return X_train_tokenized, y_train_tokenized, X_val_tokenized, y_val_tokenized, X_test_tokenized, y_test_tokenized\n",
        "    else:\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def compute_max_length(self):\n",
        "    context_list = list(self.preproc_squad_df.context)\n",
        "    question_list = list(self.preproc_squad_df.question)\n",
        "\n",
        "    context_length = [len(sen.split()) for sen in context_list]\n",
        "    question_length = [len(sen.split()) for sen in question_list]\n",
        "\n",
        "    self.max_length_context = int(np.quantile(context_length, 0.995))\n",
        "    self.max_length_question = int(np.quantile(question_length, 0.995))\n",
        "\n",
        "  def load_dataset(self, num_examples):\n",
        "    \"\"\"\n",
        "    Extract the dataset from the json file. Already grouped by title.\n",
        "\n",
        "    :param path: [Optional] specifies the local path where the training_set.json file is located\n",
        "\n",
        "    :return\n",
        "        - the extracted dataset in a dataframe format\n",
        "    \"\"\"\n",
        "    if os.path.exists(self.save_pkl_path):\n",
        "      print('File already exists! Loading from .pkl...\\n')\n",
        "      print(f'Dir path {self.save_pkl_path}')\n",
        "      self.squad_df = pd.read_pickle(self.save_pkl_path)\n",
        "      self.squad_df = self.squad_df[:num_examples]\n",
        "    else:\n",
        "      print('Loading from .json...\\n')\n",
        "      print(f'Dir path {self.training_json_path}')\n",
        "      with open(self.training_json_path) as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      df_array = []\n",
        "      for current_subject in data['data']:\n",
        "      # for current_subject in data:\n",
        "          title = current_subject['title']\n",
        "\n",
        "          for current_context in current_subject['paragraphs']:\n",
        "              context = current_context['context']\n",
        "\n",
        "              for current_qas in current_context['qas']:\n",
        "                # Each qas is a list made of id, question, answers\n",
        "                id = current_qas['id']\n",
        "                question = current_qas['question']\n",
        "                answers = current_qas['answers']\n",
        "\n",
        "                for current_answer in current_qas['answers']:\n",
        "                  answer_start = current_answer['answer_start']\n",
        "                  text = current_answer['text']\n",
        "\n",
        "                  record = { \"id\": id,\n",
        "                            \"title\": title,\n",
        "                            \"context\": context,\n",
        "                            \"question\": question,\n",
        "                            \"answer_start\": answer_start,\n",
        "                            \"answer\": text\n",
        "                            }\n",
        "\n",
        "                  df_array.append(record)\n",
        "      # Save file\n",
        "      pd.to_pickle(pd.DataFrame(df_array), self.save_pkl_path)\n",
        "      self.squad_df = pd.DataFrame(df_array)[:num_examples]\n",
        "\n",
        "  def preprocess(self):\n",
        "    df = self.squad_df.copy()\n",
        "\n",
        "    # Pre-processing context\n",
        "    context = list(df.context)\n",
        "    preproc_context = []\n",
        "\n",
        "    for c in context:\n",
        "      c = self.__preprocess_sentence(c, question=False)\n",
        "      preproc_context.append(c)\n",
        "    \n",
        "    df.context = preproc_context\n",
        "\n",
        "    # Pre-processing questions\n",
        "    question = list(df.question)\n",
        "    preproc_question = []\n",
        "\n",
        "    for q in question:\n",
        "      q = self.__preprocess_sentence(q, question=True)\n",
        "      preproc_question.append(q)\n",
        "    \n",
        "    df.question = preproc_question\n",
        "\n",
        "    # Remove features that are not useful\n",
        "    df = df.drop(['id'], axis=1)\n",
        "    self.preproc_squad_df = df\n",
        "\n",
        "  def __preprocess_sentence(self, sen, question):\n",
        "    # Creating a space between a word and the punctuation following it\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    sen = re.sub(r\"([?.!,¿])\", r\" \\1 \", sen)\n",
        "    sen = re.sub(r'[\" \"]+', \" \", sen)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sen = re.sub(r\"[^a-zA-Z0-9?.!,¿]+\", \" \", sen)\n",
        "\n",
        "    sen = sen.strip()\n",
        "\n",
        "    # Adding a start and an end token to the sentence so that the model know when to \n",
        "    # start and stop predicting.\n",
        "    # if not question: sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    sen = '<SOS> ' + sen + ' <EOS>'\n",
        "    return sen\n",
        "\n",
        "  def __answer_start_end(self, df):\n",
        "    \"\"\"\n",
        "    Creates a list of starting indexes and ending indexes for the answers.\n",
        "\n",
        "    :param df: the target Dataframe\n",
        "\n",
        "    :return: a dataframe containing the start and the end indexes foreach answer (ending index is excluded).\n",
        "\n",
        "    \"\"\"\n",
        "    start_idx = df.answer_start\n",
        "    end_idx = [start + len(list(answer)) for start, answer in zip(list(start_idx), list(df.answer))]\n",
        "    return pd.DataFrame(list(zip(start_idx, end_idx)), columns=['start', 'end'])\n",
        "\n",
        "  def split_train_val(self, df):\n",
        "    \"\"\"\n",
        "    This method splits the dataframe in training and test sets, or eventually, in training, validation and test sets.\n",
        "\n",
        "    Args\n",
        "        :param df: the target Dataframe\n",
        "        :param random_seed: random seed used in the splits\n",
        "        :param train_size: represents the absolute number of train samples\n",
        "\n",
        "    Returns:\n",
        "        - Data and labels for training, validation and test sets if val is True \n",
        "        - Data and labels for training and test sets if val is False \n",
        "\n",
        "    \"\"\"\n",
        "    # Maybe we have also to return the index for the starting answer\n",
        "    X = df.drop(['answer_start', 'question', 'answer'], axis=1).copy()\n",
        "    idx = self.__answer_start_end(df)\n",
        "    X['start'] = idx['start']\n",
        "    X['end'] = idx['end']\n",
        "    y = df['question']\n",
        "\n",
        "    # In the first step we will split the data in training and remaining dataset\n",
        "    splitter = GroupShuffleSplit(train_size=self.train_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X, groups=X['title'])\n",
        "    train_idx, rem_idx = next(split)\n",
        "\n",
        "    X_train = X.iloc[train_idx]\n",
        "    y_train = y.iloc[train_idx]\n",
        "    X_rem = X.iloc[rem_idx]\n",
        "    y_rem = y.iloc[rem_idx]\n",
        "\n",
        "\n",
        "    # Val and test test accounts for the remaining percentage of the total data\n",
        "    splitter = GroupShuffleSplit(test_size=self.test_size, n_splits=2, random_state=self.random_seed)\n",
        "    split = splitter.split(X_rem, groups=X_rem['title'])\n",
        "    val_idx, test_idx = next(split)\n",
        "\n",
        "    X_val = X_rem.iloc[val_idx]\n",
        "    y_val = y_rem.iloc[val_idx]\n",
        "\n",
        "    X_test = X_rem.iloc[test_idx]\n",
        "    y_test = y_rem.iloc[test_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "  def __tokenize_context(self, X, test):\n",
        "    context = X.context\n",
        "    if not test: self.tokenizer_context.fit_on_texts(context)\n",
        "    context_tf = self.tokenizer_context.texts_to_sequences(context)\n",
        "\n",
        "    if self.max_length_context != 0:\n",
        "      context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, maxlen=self.max_length_context, padding='post')\n",
        "    else:\n",
        "      context_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(context_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(context):\n",
        "      X['context'].iloc[i] = context_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_context.word_index['<pad>'] = 0\n",
        "    self.tokenizer_context.index_word[0] = '<pad>'\n",
        "\n",
        "    return X, self.tokenizer_context.word_index\n",
        "\n",
        "  def __tokenize_question(self, y, test):\n",
        "    question = y\n",
        "    if not test: self.tokenizer_question.fit_on_texts(question)\n",
        "    question_tf = self.tokenizer_question.texts_to_sequences(question)\n",
        "    \n",
        "    if self.max_length_question != 0:\n",
        "      question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, maxlen=self.max_length_question, padding='post')\n",
        "    else:\n",
        "      question_tf_pad = tf.keras.preprocessing.sequence.pad_sequences(question_tf, padding='post')\n",
        "\n",
        "    for i, _ in enumerate(question):\n",
        "      y.iloc[i] = question_tf_pad[i]\n",
        "\n",
        "    # Add the padding\n",
        "    self.tokenizer_question.word_index['<pad>'] = 0\n",
        "    self.tokenizer_question.index_word[0] = '<pad>'\n",
        "\n",
        "    return y, self.tokenizer_question.word_index\n",
        "\n",
        "  def extract_answer(self):\n",
        "    df = self.squad_df.copy()\n",
        "    start_end = self.__answer_start_end(df)\n",
        "    context = list(df.context)\n",
        "    \n",
        "    selected_sentences = []\n",
        "    for i, par in enumerate(context):\n",
        "      sentences = sent_tokenize(par)\n",
        "      start = start_end.iloc[i].start\n",
        "      end = start_end.iloc[i].end      \n",
        "      right_sentence = \"\"\n",
        "      context_characters = 0\n",
        "\n",
        "      for j, sen in enumerate(sentences):\n",
        "        sen += ' '\n",
        "        context_characters += len(sen)\n",
        "        # If the answer is completely in the current sentence\n",
        "        if(start < context_characters and end <= context_characters):\n",
        "          right_sentence = sen\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break\n",
        "        # the answer is in both the current and the next sentence\n",
        "        if(start < context_characters and end > context_characters):\n",
        "          right_sentence = sen + sentences[j+1]\n",
        "          selected_sentences.append(right_sentence)\n",
        "          break \n",
        "\n",
        "    self.squad_df.context = selected_sentences\n",
        "\n",
        "  def to_tensor(self, X, y, train=True):\n",
        "    X = X.context.copy()\n",
        "    y = y.copy()\n",
        "\n",
        "    # Reference:- https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.cast(list(X), tf.int64), \n",
        "         tf.cast(list(y), tf.int64)))\n",
        "    if train: \n",
        "      dataset = dataset.shuffle(self.buffer_size).batch(self.batch_size, drop_remainder=True)\n",
        "    else:\n",
        "      dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTXVdOGcZSCT"
      },
      "source": [
        "By calling the `SQuAD` constructor we create a dataset handling object which will be useful for future operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RfCYdZofJ866"
      },
      "outputs": [],
      "source": [
        "dataset_creator = SQuAD()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvU7n1LboA6g"
      },
      "source": [
        "## 1.1 Building the dataset\n",
        "\n",
        "This is the data produced that we are most interested in. As we can see we will have:\n",
        "- a data structure `dataset` containing the training, validation and test set;\n",
        "- a tuple containing the word-to-token mappings for the training, validation and test set respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Br1HATdhVD8",
        "outputId": "4237ebce-3809-46a8-bdf9-e5cb4a5a42b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 256,\n",
              " 'buffer_size': 32000,\n",
              " 'num_examples': 90000,\n",
              " 'num_words_context': 45000,\n",
              " 'num_words_question': 28000,\n",
              " 'random_seed': 13,\n",
              " 'test_size': 0.4,\n",
              " 'train_size': 0.65}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax999y7aI75Y",
        "outputId": "3c7973c8-7dc7-499c-9e91-442a0715d6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists! Loading from .pkl...\n",
            "\n",
            "Dir path ./data/squadv2.pkl\n",
            "Sentences max lenght: 102\n",
            "Questions max lenght: 27\n",
            "CPU times: user 1min 49s, sys: 5.16 s, total: 1min 54s\n",
            "Wall time: 2min 4s\n"
          ]
        }
      ],
      "source": [
        "# Preprocessed dataset without tokenizing\n",
        "%%time\n",
        "dataset, word_to_idx_context, word_to_idx_question = dataset_creator(dataset_config, path, tokenized=True)\n",
        "\n",
        "max_length_context = dataset_creator.max_length_context\n",
        "max_length_question = dataset_creator.max_length_question\n",
        "\n",
        "print(f'Sentences max lenght: {max_length_context}')\n",
        "print(f'Questions max lenght: {max_length_question}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7hARM_R2Kod"
      },
      "source": [
        "Accessing such `NamedTuple` data structure (cfr `dataset`) is pretty simple, namely in a:\n",
        "1. tuple-way by accessing it like a list, e.g. `train = dataset[0]`,\n",
        "2. object-way by calling the instance parameters, e.g. `train = dataset.train`.\n",
        "\n",
        "The other two returned values are the word to index mappings for the context and question words respectively. In order to refer to a specific split simply call:\n",
        "1. for the training dataset,\n",
        "2. for the validation dataset,\n",
        "3. for the test dataset,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples_train = dataset.train.cardinality().numpy() * dataset_config['batch_size']\n",
        "num_samples_val = dataset.val.cardinality().numpy() * dataset_config['batch_size']\n",
        "num_samples_test = dataset.test.cardinality().numpy() * dataset_config['batch_size']\n",
        "\n",
        "print(f'Num samples --- [Train]: {num_samples_train} [Validation]: {num_samples_val} [Test]: {num_samples_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ85_hoZFz1R",
        "outputId": "41c5d418-d469-457e-b1d5-90ec4f6c8468"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num samples --- [Train]: 58112 [Validation]: 17152 [Test]: 11776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "obaRYgawxyXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be5ecf8-1a5d-4907-eaf7-b1f2debccc14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vocab size for the context: 52628\n",
            "Training vocab size for the question: 29191\n",
            "\n",
            "Validation vocab size for the context: 60390\n",
            "Validation vocab size for the question: 33660\n",
            "\n",
            "Test vocab size for the context: 60390\n",
            "Test vocab size for the question: 33660\n"
          ]
        }
      ],
      "source": [
        "print(f'Training vocab size for the context: {len(word_to_idx_context[0])}')\n",
        "print(f'Training vocab size for the question: {len(word_to_idx_question[0])}')\n",
        "print()\n",
        "print(f'Validation vocab size for the context: {len(word_to_idx_context[1])}')\n",
        "print(f'Validation vocab size for the question: {len(word_to_idx_question[1])}')\n",
        "print()\n",
        "print(f'Test vocab size for the context: {len(word_to_idx_context[2])}')\n",
        "print(f'Test vocab size for the question: {len(word_to_idx_question[2])}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Vocab size --- [Context]: {len(word_to_idx_context[1])} [Question]: {len(word_to_idx_question[1])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EC5HkothI9t",
        "outputId": "c1b37021-27ad-4065-9eca-65b7453a96c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size --- [Context]: 60390 [Question]: 33660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_list = list(dataset_creator.preproc_squad_df.context)\n",
        "question_list = list(dataset_creator.preproc_squad_df.question)\n",
        "\n",
        "context_length = [len(sen.split()) for sen in context_list]\n",
        "question_length = [len(sen.split()) for sen in question_list]\n",
        "\n",
        "print(\"Lenght of longest sentence in the context: {}\".format(np.max(context_length)))\n",
        "print(\"Mean length in for sentences in the context: {}\\n\".format(np.mean(context_length)))\n",
        "\n",
        "print(\"Lenght of longest sentence in the question: {}\".format(np.max(question_length)))\n",
        "print(\"Mean length in for sentences in the question: {}\\n\".format(np.mean(question_length)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7siSawSqsUvO",
        "outputId": "32cf910c-7a4f-46f6-f65b-5ffa5e008a98"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenght of longest sentence in the context: 389\n",
            "Mean length in for sentences in the context: 33.75989451934383\n",
            "\n",
            "Lenght of longest sentence in the question: 62\n",
            "Mean length in for sentences in the question: 13.322275368440279\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2f7Nn_4en9"
      },
      "source": [
        "# 2. GloVe and Embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TRJ1NpSMqaJL"
      },
      "outputs": [],
      "source": [
        "class GloVe:\n",
        "  def __init__(self, embedding_dimension):\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "\n",
        "    try:\n",
        "      self.embedding_model = KeyedVectors.load(f'./data/glove_model_{self.embedding_dimension}')\n",
        "    except FileNotFoundError:\n",
        "      print('[Warning] Model not found in local folder, please wait...')\n",
        "      self.embedding_model = self.load_glove()\n",
        "      self.embedding_model.save(f'./data/glove_model_{self.embedding_dimension}')  \n",
        "      print('Download finished. Model loaded!')\n",
        "\n",
        "  def load_glove(self):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained GloVe embedding model via gensim library.\n",
        "\n",
        "    We have a matrix that associate words to a vector of a user-defined dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(self.embedding_dimension)\n",
        "\n",
        "    try:\n",
        "      emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "      print(\"Generic error when loading GloVe\")\n",
        "      print(\"Check embedding dimension\")\n",
        "      raise e\n",
        "\n",
        "    emb_model = gloader.load(download_path)\n",
        "    return emb_model\n",
        "\n",
        "  def build_embedding_matrix(self, word_to_idx, vocab_size: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the \n",
        "        dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, self.embedding_dimension), dtype=np.float32)\n",
        "    oov_count = 0\n",
        "    oov_words = []\n",
        "\n",
        "    # For each word which is not present in the vocabulary we assign a random vector, otherwise we take the GloVe embedding\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      try:\n",
        "        embedding_vector = self.embedding_model[word]\n",
        "      except (KeyError, TypeError):\n",
        "        oov_count += 1\n",
        "        oov_words.append(word)\n",
        "        embedding_vector = np.random.uniform(low=-0.5, \n",
        "                                             high=0.5, \n",
        "                                             size=self.embedding_dimension)\n",
        "\n",
        "      embedding_matrix[idx] = embedding_vector\n",
        "    \n",
        "    print(f'\\n[Debug] {oov_count} OOV words found!\\n')\n",
        "    return embedding_matrix, oov_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk-z8A5y3cpI"
      },
      "source": [
        "The next step is to initialize the handler with the desidered `embedding_dimension`. Then to build the embedding matrix with the pre-trained GloVe embeddings simply call the `build_embedding_matrix` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YUmvdCWGavSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f12c32-d6f8-4d96-db4f-a904af974240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60390/60390 [00:00<00:00, 225841.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 7690 OOV words found!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33660/33660 [00:00<00:00, 237545.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] 3776 OOV words found!\n",
            "\n",
            "CPU times: user 1.31 s, sys: 457 ms, total: 1.77 s\n",
            "Wall time: 10.5 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Initalize the handler for GloVe\n",
        "glove_handler = GloVe(embedding_dimension=300)\n",
        "\n",
        "# We will create the matrix by using only the words present in the training and validation set\n",
        "embedding_matrix_context, oov_words_context = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_context[2], \n",
        "    len(word_to_idx_context[2]))\n",
        "\n",
        "embedding_matrix_question, oov_words_question = glove_handler.build_embedding_matrix(\n",
        "    word_to_idx_question[2], \n",
        "    len(word_to_idx_question[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdLOk0pQu3Iu"
      },
      "source": [
        "Convert both of them into tensor, but it is fine to also treat them as `numpy` array, still it is better to use the `tensorflow` fundamentals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EX6PvKTBsdSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b14a5e-b9ec-4885-a3d1-c849d135f59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix for the context: (60390, 300)\n",
            "Embedding matrix for the question: (33660, 300)\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix_context = tf.convert_to_tensor(embedding_matrix_context)\n",
        "embedding_matrix_question = tf.convert_to_tensor(embedding_matrix_question)\n",
        "\n",
        "print(f'Embedding matrix for the context: {embedding_matrix_context.shape}')\n",
        "print(f'Embedding matrix for the question: {embedding_matrix_question.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF5Rtd4uqa_k"
      },
      "source": [
        "# 3. Encoder-Decoder Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GjdRysIvPoLc"
      },
      "outputs": [],
      "source": [
        "example_context_batch, example_question_batch = next(iter(dataset.train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjVfZgIIf1RV"
      },
      "source": [
        "## 3.1 Encoder\n",
        "We will use a bidirectional LSTM to encode the sentence,\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\overrightarrow{b_t} &= \\overrightarrow{\\text{LSTM}}(x_t, \\overrightarrow{b_{t-1}})\\\\\n",
        "\\overleftarrow{b_t} &= \\overleftarrow{\\text{LSTM}}(x_t, \\overleftarrow{b_{t+1}})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\overrightarrow{b_t}$ is the hidden state at time step $t$ for the forward pass LSTM and $\\overleftarrow{b_t}$ for the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GK6Kd1XvqK22"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, model_config, embedding_matrix, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "    \n",
        "    self.encoder_input = Input(shape=(model_config['max_length_context'],), \n",
        "                             batch_size=model_config['batch_size'], \n",
        "                             dtype=tf.int32, \n",
        "                             name='Context')\n",
        "    self.embedding = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                               output_dim=embedding_matrix.shape[1],\n",
        "                               input_length=model_config['max_length_context'],\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,\n",
        "                               mask_zero=True,\n",
        "                               name='Context_embedding') \n",
        "    \n",
        "    self.spatial_dropout = tf.keras.layers.SpatialDropout1D(model_config['dropout_rate'])\n",
        "\n",
        "    self.bi_lstm = Bidirectional(LSTM(model_config['enc_units']//2, \n",
        "                                      return_sequences=True, \n",
        "                                      return_state=True,\n",
        "                                      dropout=model_config['dropout_rate'],\n",
        "                                      kernel_regularizer=tf.keras.regularizers.L2(model_config['regularizer'])), \n",
        "                                  name='Context_encoding',\n",
        "                                  merge_mode='concat')\n",
        "\n",
        "    self.concatenate = tf.keras.layers.Concatenate(axis=1, name='Merge') \n",
        "\n",
        "  def call(self, inputs, state=None, training=False):\n",
        "    # 1. The input is a tokenized and padded sentence containing the answer from the context\n",
        "    # 2. The embedding layer looks up for the embedding for each token, the mask is automatically produced\n",
        "    vectors = self.embedding(inputs)\n",
        "\n",
        "    vectors = self.spatial_dropout(vectors)\n",
        "\n",
        "    # 3. The Bi-LSTM processes the embedding sequence forward and backward:\n",
        "    #     output shape: ('batch', 'max_length_context', 'units')\n",
        "    #     hidden state shape: fw ('batch', 'units//2'), bw ('batch', 'units//2')\n",
        "    #     cell state shape: fw ('batch', 'units//2'), bw ('batch', 'units//2')\n",
        "    output, forward_h, forward_c, backward_h, backward_c = self.bi_lstm(vectors, initial_state=state, training=training)\n",
        "\n",
        "    # 4. Concatenate the forward and the backward states\n",
        "    h = self.concatenate([forward_h, backward_h])\n",
        "    c = self.concatenate([forward_c, backward_c])\n",
        "    encoder_state = [h, c]\n",
        "\n",
        "    # 5. Return the new sequence processed by the encoder and its state\n",
        "    return [output, encoder_state]\n",
        "\n",
        "  # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    return tf.keras.Model(inputs=self.encoder_input, outputs=self.call(self.encoder_input))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(),               \n",
        "        show_shapes=True, \n",
        "        show_layer_names=True,  \n",
        "        expand_nested=True                       \n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbjSxPGcFud_"
      },
      "source": [
        "### 3.1.1 Test the encoder stack\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    'batch_size': 64,\n",
        "    'enc_units': 256,\n",
        "    'dec_units': 256,\n",
        "    'max_length_context': dataset.train.element_spec[0].shape[1],\n",
        "    'max_length_question': dataset.train.element_spec[1].shape[1],\n",
        "    'dropout_rate': .3,\n",
        "    'regularizer': 1e-2,\n",
        "}"
      ],
      "metadata": {
        "id": "4T927b_88tjA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple example could be done by taking a single batch from the dataset."
      ],
      "metadata": {
        "id": "J2yhcqnrbxTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_ffteDMQyzmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f71ccbe-b501-4ef6-d879-1e807e1c03b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch_size, max_length_context, units): (256, 102, 256)\n",
            "Hidden state shape: (batch_size, units): (256, 256)\n",
            "Cell state shape: (batch_size, units): (256, 256)\n"
          ]
        }
      ],
      "source": [
        "# We need to encode the context, so we will pass the embedding matrix for the context\n",
        "encoder = Encoder(model_config=model_config, embedding_matrix=embedding_matrix_context)\n",
        "encoder_outputs, encoder_state = encoder(example_context_batch)\n",
        "\n",
        "hidden_state, cell_state = encoder_state\n",
        "\n",
        "print(f'Encoder output shape: (batch_size, max_length_context, units): {encoder_outputs.shape}')\n",
        "print(f'Hidden state shape: (batch_size, units): {hidden_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, units): {cell_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder.build_graph()\n",
        "# encoder.summary()"
      ],
      "metadata": {
        "id": "eZ2NgqMLW3RO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder.plot_model()"
      ],
      "metadata": {
        "id": "vpn2GGuJX4cd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder returns its internal state so that its state can be used to initialize the decoder.\n",
        "\n",
        "It's also common for a LSTM to return its state so that it can process a sequence over multiple calls."
      ],
      "metadata": {
        "id": "JjVmUmzJb1xf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtM9nOQrf3jq"
      },
      "source": [
        "## 3.2 Decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, model_config, embedding_matrix, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "    \n",
        "    # Attributes\n",
        "    self.new_token_input = Input(shape=(1), \n",
        "                           batch_size=model_config['batch_size'], \n",
        "                           name='Token_t')\n",
        "    self.enc_output_input = Input(shape=(model_config['max_length_context'], model_config['enc_units']), \n",
        "                            batch_size=model_config['batch_size'], \n",
        "                            name='Enc_output')\n",
        "                        \n",
        "    self.embedding = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                               output_dim=embedding_matrix.shape[1],\n",
        "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                               trainable=False,  \n",
        "                               mask_zero=True,\n",
        "                               name='Question_embedding')\n",
        "        \n",
        "    self.lstm_layer_1 = LSTM(model_config['dec_units'], \n",
        "                           return_sequences=True,\n",
        "                           return_state=np.True_,\n",
        "                           dropout=model_config['dropout_rate'],\n",
        "                           kernel_regularizer=tf.keras.regularizers.L2(model_config['regularizer']),\n",
        "                           name='Decoding_question_1')\n",
        "    \n",
        "    self.lstm_layer_2 = LSTM(model_config['dec_units'], \n",
        "                           return_sequences=True,\n",
        "                           return_state=True,\n",
        "                           dropout=model_config['dropout_rate'],\n",
        "                           kernel_regularizer=tf.keras.regularizers.L2(model_config['regularizer']),\n",
        "                           name='Decoding_question_2')\n",
        "    \n",
        "    # self.attention = BahdanauAttention(decoder_config['dec_units'])\n",
        "    # Additive attention layer, a.k.a. Bahdanau-style attention.\n",
        "    self.attention = tf.keras.layers.AdditiveAttention(name='Attention_head')\n",
        "\n",
        "    self.concatenate = tf.keras.layers.Concatenate(axis=-1, name='Merge')\n",
        "\n",
        "    self.fc1 = Dense(model_config['dec_units']*2, \n",
        "                     activation=tf.keras.activations.tanh, \n",
        "                     use_bias=False,\n",
        "                     kernel_regularizer=tf.keras.regularizers.L2(model_config['regularizer']),\n",
        "                     name='Dense_Wt')\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(model_config['dropout_rate'])\n",
        "\n",
        "    # Compute the logits\n",
        "    self.decoder_logits = Dense(embedding_matrix.shape[0], \n",
        "                                use_bias=False, \n",
        "                                name='Logits_Ws',\n",
        "                                kernel_regularizer=tf.keras.regularizers.L2(model_config['regularizer']))\n",
        "\n",
        "  def call(self, inputs, state=None, training=False):\n",
        "    new_token = inputs[0]\n",
        "    enc_output = inputs[1]\n",
        "\n",
        "    # 1. The embedding layer looks up for the embedding for each token, masks is automatically computed\n",
        "    # vectors shape: (batch_size, 1, embedding_dimension)\n",
        "    vectors = self.embedding(new_token)\n",
        "    if tf.shape(vectors).shape == 2: vectors = tf.expand_dims(vectors, axis=1)\n",
        "\n",
        "    # 2. Process one step with the LSTM\n",
        "    # LSTM expects inputs of shape: (batch_size, timestep, feature)\n",
        "    output, h, c = self.lstm_layer_1(vectors, initial_state=state, training=training)\n",
        "    output, h, c = self.lstm_layer_2(output, initial_state=(h, c), training=training)\n",
        "    # output, h, c = self.lstm_layer_2(output, initial_state=(h, c), training=training)\n",
        "\n",
        "    # 4. Use the LSTM cell output as the query for the attention over the encoder output, that is the value.\n",
        "    # The mask is automatically passed as argument by the keras backend.\n",
        "    context_vector, attention_weights = self.attention([output, enc_output],\n",
        "                                                       training=training,\n",
        "                                                       return_attention_scores=True)\n",
        "\n",
        "    # 5. Join the context_vector and cell output [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    output_and_context_vector = self.concatenate([context_vector, output])\n",
        "\n",
        "    # at = tanh(Wt@[ht, ct])\n",
        "    attention_vector = self.fc1(output_and_context_vector)\n",
        "\n",
        "    attention_vector = self.dropout(attention_vector)\n",
        "\n",
        "    # logits = softmax(Ws@at), it produces unscaled logits\n",
        "    logits = self.decoder_logits(attention_vector)\n",
        "\n",
        "    return [logits, attention_weights, (h, c)]\n",
        "\n",
        "      # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    return tf.keras.Model(inputs=[self.new_token_input, self.enc_output_input], outputs=self.call([self.new_token_input, self.enc_output_input]))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(),               \n",
        "        show_shapes=True, \n",
        "        show_layer_names=True,  \n",
        "        expand_nested=True                       \n",
        "    )"
      ],
      "metadata": {
        "id": "zLt-d5-x_WIC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF5J42g1l-be"
      },
      "source": [
        "### 3.2.1 Test the decoder stack\n",
        "\n",
        "The decoder will take as input:\n",
        "1. `new_tokens`: the last token generated of shape `(batch_size, 1)`, namely the token obrained in the previous time step of the decoder (we will initialize the decoder with the `\"<sos>\"` token);\n",
        "2. `enc_output`: this is the representation produced by the `Encoder` of shape `(batch_size, max_length_context, enc_units)`;\n",
        "3. `mask`: this is the mask, that is a boolean tensor, indicating which tokens will be considered in the decoding of shape `(batch_size, max_length_context)`; \n",
        "4. `decoder_state`: the previous state of the decoder, namely the internal state of the decoder's LSTM (the paper suggests to input the hidden and cell state produced by the Bi-LSTM). The shape is `[(batch_size, enc_units), (batch_size, enc_units)]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kS0UBnMzTbie"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(model_config=model_config, embedding_matrix=embedding_matrix_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KeMvqDnrTkf0"
      },
      "outputs": [],
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "start_tag_index = word_to_idx_question[2]['<sos>']\n",
        "first_token = tf.constant([[start_tag_index]] * dataset_config['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BF6PWsNYfmUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9c2522-49f5-4783-baf0-2acb2984acfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (batch_size, t, output_vocab_size) (256, 1, 33660)\n",
            "Attention weights shape: (batch_size, t, max_length_context) (256, 1, 102)\n",
            "Hidden state shape: (batch_size, dec_units) (256, 256)\n",
            "Cell state shape: (batch_size, dec_units) (256, 256)\n"
          ]
        }
      ],
      "source": [
        "decoder_logits, attention_weights, decoder_state = decoder(inputs=[first_token, encoder_outputs], state=encoder_state)\n",
        "\n",
        "hidden_dec_state, cell_dec_state = decoder_state\n",
        "\n",
        "print(f'Logits shape: (batch_size, t, output_vocab_size) {decoder_logits.shape}')\n",
        "print(f'Attention weights shape: (batch_size, t, max_length_context) {attention_weights.shape}')\n",
        "print(f'Hidden state shape: (batch_size, dec_units) {hidden_dec_state.shape}')\n",
        "print(f'Cell state shape: (batch_size, dec_units) {cell_dec_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder.build_graph()\n",
        "# decoder.summary()"
      ],
      "metadata": {
        "id": "JvSanVszYhL4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder.plot_model()"
      ],
      "metadata": {
        "id": "RLimoPxQYlVb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBIQDE0Sl6k8"
      },
      "source": [
        "Moving on: this means that the decoder will produce a vector of unnormalized log probabilities (**logits**) associated to each vocabulary word. That is, a vector of logits $l_b \\in \\mathbb{R}^{\\mathcal{V}}$ for each element $b$ in the batch, namely indicating the next probable token for a given sentence. \n",
        "\n",
        "Now we sample a token according to the logits computed by the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kGGwivobvx_W"
      },
      "outputs": [],
      "source": [
        "# sampled_tokens = tf.random.categorical(\n",
        "#     logits=decoder_logits[:, 0, :],\n",
        "#     num_samples=1, \n",
        "#     seed=dataset_config['random_seed'])\n",
        "# vocab = np.array(list(word_to_idx_question[2].keys()))\n",
        "\n",
        "# first_word = list(vocab[tf.squeeze(sampled_tokens, axis=-1).numpy()])\n",
        "# first_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Y2ixRaJZn271"
      },
      "outputs": [],
      "source": [
        "# decoder_logits, _, _ = decoder([sampled_tokens, encoder_outputs], state=decoder_state)\n",
        "\n",
        "# sampled_tokens = tf.random.categorical(\n",
        "#     logits=decoder_logits[:, 0, :], \n",
        "#     num_samples=1, \n",
        "#     seed=dataset_config['random_seed'])\n",
        "# sampled_tokens = tf.squeeze(sampled_tokens, axis=-1).numpy()\n",
        "\n",
        "# first_word = list(vocab[sampled_tokens])\n",
        "# first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the weights."
      ],
      "metadata": {
        "id": "U8xwVgulXUab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.subplot(1, 2, 1)\n",
        "# plt.pcolormesh(attention_weights[:, 0, :])\n",
        "# plt.title('Attention weights')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.pcolormesh(example_context_batch != 0)\n",
        "# plt.title('Mask')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "JtJrZ7UeWac-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the weights for only one sequence."
      ],
      "metadata": {
        "id": "wjc7zQkDY19Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attention_slice = attention_weights[0, 0].numpy()\n",
        "# attention_slice = attention_slice[attention_slice != 0]\n",
        "\n",
        "# plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# a1 = plt.subplot(1, 2, 1)\n",
        "# plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# # freeze the xlim\n",
        "# plt.xlim(plt.xlim())\n",
        "# plt.xlabel('Attention weights')\n",
        "\n",
        "# a2 = plt.subplot(1, 2, 2)\n",
        "# plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# # zoom in\n",
        "# top = max(a1.get_ylim())\n",
        "# zoom = 0.85*top\n",
        "# a2.set_ylim([0.90*top, top])\n",
        "# a1.plot(a1.get_xlim(), [zoom, zoom], color='k')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "XwTgLXw0YjQ1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIoySQKuIGlc"
      },
      "source": [
        "# 4. Question Generation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0imUvBAg14Ep"
      },
      "source": [
        "## 4.1 Tensorboard initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to clear any logs from previous runs."
      ],
      "metadata": {
        "id": "PCyECPRVWmQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./logs/"
      ],
      "metadata": {
        "id": "WOeZd4tdVvDR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up summary writers to write the summaries to disk in a different logs directory."
      ],
      "metadata": {
        "id": "qr26qMjV1gcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = './logs'\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = log_dir + '/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = log_dir + '/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "metadata": {
        "id": "dcxRteAC1fcv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyRA2RxZNsx4"
      },
      "source": [
        "## 4.2 Loss\n",
        "\n",
        "The **QG** task is defined as finding $\\hat{y}$ such that:\n",
        "$$\n",
        "\\hat{y} = \\arg{\\max_y P(y|x)}  \n",
        "$$\n",
        "where $P(y|x)$ is the conditional log-likelihood of the predicted question sentence $y$ given the input $x$. Du et al. shown that the conditional probability could be factorized in:\n",
        "$$\n",
        "P(y|x) = \\prod_{t=1}^{|y|} P(y_t|x, y_{<t})\n",
        "$$\n",
        "where the probability of each $y_t$ is predicted based on all the words that have been generated upon time $t$, namely $y_{<t}$.\n",
        "\n",
        "This means that given a training corpus of sentence-question pairs $\\mathcal{S} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, the objective is to minimize the negative log-likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L} &= - \\sum_{i=1}^N \\log P(y^{(i)}|x^{(i)}; \\theta)\\\\\n",
        "            &=  - \\sum_{i=1}^N \\sum_{j=1}^{|y^{(i)}|} \\log P (y_j^{(i)}|x^{(i)}, y_{<j}^{(i)}; \\theta)\n",
        "\\end{align*}\n",
        "$$\n",
        "We parameterize the probability of decoding each word $y_j$ by using an RNN:\n",
        "$$\n",
        "P(y_j|y_{<j}, s) = \\text{softmax}(g(h_j))\n",
        "$$\n",
        "where $g(.)$ is a transition function that outputs a vocabulary-sized vector.\n",
        "\n",
        "For more information see [here](https://stackoverflow.com/questions/47057361/how-do-i-mask-a-loss-function-in-keras-with-the-tensorflow-backend)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IP_UunM3MUtF"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred) \n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    # Sum the loss over the batch since reduction is set to NONE\n",
        "    loss = tf.reduce_sum(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOVZj966EPP"
      },
      "source": [
        "## 4.3 Training metrics\n",
        "\n",
        "The metrics used during training will be:\n",
        "1. **perplexity**, that is defined as $e^{H(p,q)}$ where $H(p,q)$ is the cross-entropy between $p$, that is the ground truth (labels), and $q$, that are unscaled probabilities distributions (logits), \n",
        "2. **masked accuracy**, namely an accuracy computed only on tokens which are not `<pad>`.\n",
        "\n",
        "They will be implemented by exploiting the `Metric` object in `tensorflow`. See [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "xbK72c5r6LWP"
      },
      "outputs": [],
      "source": [
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='perplexity', **kwargs):\n",
        "    super(Perplexity, self).__init__(name=name, **kwargs)\n",
        "    self.scores = self.add_weight(name='perplexity_scores', initializer='zeros')\n",
        "\n",
        "  def update_state(self, loss):\n",
        "    \"\"\"\n",
        "    Reference :- https://www.surgehq.ai/blog/how-good-is-your-chatbot-an-introduction-to-perplexity-in-nlp\n",
        "    \"\"\"\n",
        "    self.scores.assign(tf.exp(loss))\n",
        "\n",
        "  def result(self): return self.scores\n",
        "  def reset_state(self): self.scores.assign(0)\n",
        "\n",
        "# Also the accuracy should mask the padding\n",
        "class MaskedAccuracy(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='masked_accuracy',**kwargs):\n",
        "    super(MaskedAccuracy, self).__init__(name=name, **kwargs)\n",
        "    self.scores = self.add_weight(name='accuracy_scores', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    # We mask since we are not interested in the final accuracy score with the padding\n",
        "    mask = tf.cast(tf.math.greater(y_true, 0), dtype=tf.float32)\n",
        "\n",
        "    correct = tf.cast(tf.math.equal(y_true, y_pred), dtype=tf.float32)\n",
        "    correct = tf.math.reduce_sum(mask * correct)\n",
        "    total_legit = tf.math.reduce_sum(mask)\n",
        "\n",
        "    self.scores.assign(correct / total_legit)\n",
        "\n",
        "  def result(self): return self.scores\n",
        "  def reset_state(self): self.scores.assign(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Callbacks"
      ],
      "metadata": {
        "id": "LsdTOIHxEphh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetEpochNumber(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, **kwargs) -> None:\n",
        "    super(GetEpochNumber, self).__init__(**kwargs)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.model.epoch_number.assign_add(delta=1)\n",
        "\n",
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key) -> None:\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_ends(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
        "  \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
        "\n",
        "  Arguments:\n",
        "      schedule: a function that takes an epoch index (integer, indexed from 0) \n",
        "        and current learning rate as inputs and returns a new learning rate \n",
        "        as output (float).\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(CustomLearningRateScheduler, self).__init__()\n",
        "    self.schedule = self.lr_schedule\n",
        "\n",
        "    self.LR_SCHEDULE = [\n",
        "      # (epoch to start, learning rate) tuples\n",
        "      (15, 1e-1),\n",
        "      (23, 5e-2),\n",
        "      # (12, 5e-5),\n",
        "      # (14, 1e-5),\n",
        "        ]\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    if not hasattr(self.model.optimizer, \"lr\"):\n",
        "        raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "    # Get the current learning rate from model's optimizer.\n",
        "    lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "    \n",
        "    # Call schedule function to get the scheduled learning rate.\n",
        "    scheduled_lr = self.schedule(epoch, lr)\n",
        "    \n",
        "    # Set the value back to the optimizer before this epoch starts\n",
        "    tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "    print(f\"\\nEpoch {epoch+1}: Learning rate is {scheduled_lr}.\" )\n",
        "\n",
        "  def lr_schedule(self, epoch, lr):\n",
        "    \"\"\"\n",
        "    Helper function to retrieve the scheduled learning rate based on epoch.\n",
        "    \"\"\"\n",
        "    if epoch < self.LR_SCHEDULE[0][0] or epoch > self.LR_SCHEDULE[-1][0]:\n",
        "        return lr\n",
        "    for i in range(len(self.LR_SCHEDULE)):\n",
        "        if epoch == self.LR_SCHEDULE[i][0]:\n",
        "            return self.LR_SCHEDULE[i][1]\n",
        "    return lr\n",
        "\n",
        "# Initialize the callbacks\n",
        "batch_loss = BatchLogs('batch_loss')\n",
        "perplexity = BatchLogs('perplexity')\n",
        "accuracy = BatchLogs('accuracy')\n",
        "lr_scheduler = CustomLearningRateScheduler()\n",
        "epoch_counter = GetEpochNumber()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_perplexity', \n",
        "                                                  patience=8, \n",
        "                                                  mode='max', \n",
        "                                                  restore_best_weights=True) "
      ],
      "metadata": {
        "id": "cEuSPhHf0aZs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwLiPsCyNuor"
      },
      "source": [
        "## 4.5 Trainer Model definition\n",
        "\n",
        "The training step should:\n",
        "1. Run the encoder on the `input_tokens` to get the `encoder_outputs`, `hidden_state` and `cell_state`. \n",
        "\n",
        "... complete"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMasking(Layer):\n",
        "    def __init__(self, mask_value=0, **kwargs):\n",
        "        super(CustomMasking, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.mask_value = mask_value\n",
        "        self._compute_output_and_mask_jointly = True\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.reduce_any(tf.math.not_equal(inputs, self.mask_value), \n",
        "                                  axis=list(range(2, len(inputs.shape))))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        axes = list(range(2, len(inputs.shape)))\n",
        "        boolean_mask = tf.math.reduce_any(tf.math.not_equal(inputs, self.mask_value),\n",
        "                             axis=axes, keepdims=True)\n",
        "        outputs = inputs * tf.cast(boolean_mask, inputs.dtype)\n",
        "        # Compute the mask and outputs simultaneously.\n",
        "        outputs._keras_mask = tf.squeeze(boolean_mask, axis=axes)  # pylint: disable=protected-access\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'mask_value': self.mask_value}\n",
        "        base_config = super(tf.keras.layers.Masking, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "metadata": {
        "id": "hx8wGohrshe9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "code",
        "id": "8xSn_StMq9cx"
      },
      "outputs": [],
      "source": [
        "class QGModel(tf.keras.Model):\n",
        "  def __init__(self, model_config, embedding_matrix_context, embedding_matrix_question, **kwargs):\n",
        "    \"\"\"\n",
        "    Prepare the model for the training. It builds the both the encoder and the decoder by\n",
        "    exploiting the tf.keras.Model Sub-Classing API.\n",
        "    Also it defines a wrapper to use the tf.function compilation for the tensorflow computational\n",
        "    graph.\n",
        "    \"\"\"\n",
        "    super(QGModel, self).__init__(**kwargs)\n",
        "    self.context_input = Input(shape=(model_config['max_length_context']), batch_size=model_config['batch_size'])\n",
        "    self.question_input = Input(shape=(model_config['max_length_question']), batch_size=model_config['batch_size'])\n",
        "    self.masking = CustomMasking(mask_value=0)\n",
        "    self.encoder = Encoder(model_config=model_config, embedding_matrix=embedding_matrix_context)\n",
        "    self.decoder = Decoder(model_config=model_config, embedding_matrix=embedding_matrix_question)\n",
        "\n",
        "    # Attributes\n",
        "    self.max_length_question = model_config['max_length_question']\n",
        "    self.batch_size = model_config['batch_size']\n",
        "\n",
        "    # Performance metrics\n",
        "    self.train_accuracy = tf.keras.metrics.Accuracy()\n",
        "    self.train_accuracy_sentence = MaskedAccuracy()\n",
        "    self.test_accuracy = tf.keras.metrics.Accuracy()\n",
        "    self.test_accuracy_sentence = MaskedAccuracy()\n",
        "    self.train_perplexity = Perplexity()\n",
        "    self.test_perplexity = Perplexity()\n",
        "\n",
        "    # Loss tracker, resets at the beginning of each epoch\n",
        "    self.train_loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
        "    self.test_loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    # We list our `Metric` objects here so that `reset_states()` can be called \n",
        "    # automatically at the start of each epoch or at the start of `evaluate()`.\n",
        "    # If you don't implement this property, you have to call # `reset_states()` \n",
        "    # yourself at the time of your choosing.\n",
        "    return [self.train_loss_tracker, self.train_accuracy, self.train_perplexity, self.train_accuracy_sentence,\n",
        "            self.test_loss_tracker, self.test_accuracy, self.test_perplexity, self.test_accuracy_sentence]\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    \"\"\"\n",
        "    It performs a forward pass. Calls the model on new inputs and returns the outputs as tensors.\n",
        "    \"\"\"\n",
        "    context, question = inputs\n",
        "\n",
        "    context = self.masking(context)\n",
        "    question = self.masking(question)\n",
        "\n",
        "    # We collect the question predicted by the decoder, the first character is the starting token\n",
        "    y_pred = tf.fill([self.batch_size, 1], question[0][0])\n",
        "    \n",
        "    # Keep a loss tracking value\n",
        "    if training:\n",
        "      self.train_loss_tracker.reset_state()\n",
        "    else:\n",
        "      self.test_loss_tracker.reset_state()\n",
        "    \n",
        "    # Set the loss \n",
        "    loss = tf.constant(0.0)\n",
        "    \n",
        "    # Encode the input, that is the context\n",
        "    encoder_outputs, encoder_state = self.encoder(context, training=training)\n",
        "\n",
        "    # The decoder should be initialized with the encoder last state \n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    t = 0\n",
        "    # We have to run the decoder for all the length of the question \n",
        "    while t < (self.max_length_question - 1):\n",
        "      # We have to pass two tokens:\n",
        "      #   1. the token at time step t, namely the token in which we need to start run the decoder \n",
        "      #   2. the token at time step t+1, that is the next token in the sequence that needs to be compared with\n",
        "      # Note that: at the start of the question the new_token will be start-of-seq tag\n",
        "      new_token = tf.gather(question, t, axis=1)\n",
        "      target_token = tf.gather(question, t+1, axis=1)\n",
        "\n",
        "      # Here we call the decoder in order to produce the token at time step t+1, it returns,\n",
        "      #   1. the partial loss for the predicted token,\n",
        "      #   2. the new decoder state,\n",
        "      #   3. the predicted token at time step t+1\n",
        "      step_loss, decoder_state, token_pred = self.step_decoder(\n",
        "          (new_token, target_token),\n",
        "          encoder_outputs,\n",
        "          decoder_state, \n",
        "          training=training)\n",
        "      \n",
        "      # Concatenate the predicted token to the already obtained question\n",
        "      y_pred = tf.concat([y_pred, token_pred], axis=1)\n",
        "\n",
        "      # Each partial loss contributes to the total loss\n",
        "      loss = loss + step_loss\n",
        "      t = t + 1\n",
        "\n",
        "    # Since we have computed a cumulated partial scores of the losses produced \n",
        "    # at each decoder step we then average for the number of elements that are \n",
        "    # not padding\n",
        "    loss = loss / tf.math.reduce_sum(tf.cast(question != 0, dtype=loss.dtype))\n",
        "\n",
        "    return y_pred, loss\n",
        "  \n",
        "  def step_decoder(self, tokens, encoder_outputs, decoder_state, training):\n",
        "    \"\"\"\n",
        "    Run a single iteration of the decoder and computers the incremental loss between the\n",
        "    produced token and the token in the target input.\n",
        "    \"\"\"\n",
        "    new_token = tokens[0]\n",
        "    \n",
        "    # Run the decoder one time, this will return the logits for the token at timestep t+1 for the entire batch\n",
        "    decoder_logits, _, decoder_state = self.decoder([new_token, encoder_outputs], state=decoder_state, training=training)\n",
        "  \n",
        "    y_true = tf.expand_dims(tokens[1], axis=1)\n",
        "    y_pred = decoder_logits\n",
        "\n",
        "    # Compare the produced logits with the true token\n",
        "    step_loss = self.loss(y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # Greedily extract the word that has the maximum value in the produced logits\n",
        "    y_pred = tf.cast(tf.math.argmax(y_pred, axis=-1), dtype=tf.int64)\n",
        "\n",
        "    if training:\n",
        "      self.train_accuracy.update_state(y_true=y_true, y_pred=y_pred, sample_weight=tf.cast((y_true != 0), tf.int32))\n",
        "    else:\n",
        "      self.test_accuracy.update_state(y_true=y_true, y_pred=y_pred, sample_weight=tf.cast((y_true != 0), tf.int32))\n",
        "\n",
        "    return step_loss, decoder_state, y_pred\n",
        "  \n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    \"\"\"\n",
        "    Optimization step for a batch.\n",
        "    \"\"\"\n",
        "    context, question = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Computes the forward pass: it returns the predicted question and the loss value averaged over the batch\n",
        "      y_pred, loss = self(inputs, training=True)\n",
        "\n",
        "    self.train_loss_tracker.update_state(loss)\n",
        "\n",
        "    # Compute gradients\n",
        "    tr_variables = self.trainable_variables\n",
        "    grads = tape.gradient(loss, tr_variables)\n",
        "\n",
        "    # Apply some clipping (by norm) as done in the paper and update the weights\n",
        "    ads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
        "    self.optimizer.apply_gradients(zip(grads, tr_variables))\n",
        "\n",
        "    # Compute metrics\n",
        "    self.train_perplexity.update_state(loss)\n",
        "    self.train_accuracy_sentence.update_state(y_true=question, y_pred=y_pred)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics[:4]}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, inputs):\n",
        "    \"\"\"\n",
        "    The logic for one evaluation step. This function should contain the mathematical logic \n",
        "    for one step of evaluation. This typically includes the forward pass, \n",
        "    loss calculation, and metrics updates.\n",
        "\n",
        "    Called when at each epoch's end to validate on the validation data given.\n",
        "    \"\"\"\n",
        "    context, question = inputs\n",
        "\n",
        "    context = self.masking(context)\n",
        "    question = self.masking(question)\n",
        "\n",
        "    # Computes a forward pass with training set to False\n",
        "    y_pred, loss = self(inputs, training=False)\n",
        "    self.test_loss_tracker.update_state(loss)\n",
        "\n",
        "    # Compute metrics\n",
        "    self.test_perplexity.update_state(loss)\n",
        "    self.test_accuracy_sentence.update_state(y_true=question, y_pred=y_pred)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics[4:]}\n",
        "\n",
        "  # # # Reference :- https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model\n",
        "  def build_graph(self):\n",
        "    return tf.keras.Model(inputs=[self.context_input, self.question_input], outputs=self.call([self.context_input, self.question_input]))\n",
        "  \n",
        "  def plot_model(self):\n",
        "    return tf.keras.utils.plot_model(\n",
        "        self.build_graph(),               \n",
        "        show_shapes=True, \n",
        "        show_layer_names=True,  \n",
        "        expand_nested=True                       \n",
        "    )\n",
        "\n",
        "\n",
        "# Utility function in order to build the compiled model\n",
        "def build_model(model_config,\n",
        "                embedding_matrix_context, \n",
        "                embedding_matrix_question, \n",
        "                compile_info):\n",
        "  print(\"Model Configuration \\nParameters: {}\".format(model_config))\n",
        "  print(\"Compile \\nParameters: {}\".format(compile_info))\n",
        "  model = QGModel(model_config,\n",
        "                  embedding_matrix_context=embedding_matrix_context,\n",
        "                  embedding_matrix_question=embedding_matrix_question)\n",
        "  \n",
        "  model.compile(**compile_info)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8BfykX5faH"
      },
      "source": [
        "# 5. Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Keras Tuner "
      ],
      "metadata": {
        "id": "BEW5hmQMhgyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "if keras_tuner:  \n",
        "  # Max epochs for the KerasTuner\n",
        "  epochs_tuning = 20\n",
        "\n",
        "  # Batch size used\n",
        "\n",
        "  # tune the hyperparameters of the possible choices\n",
        "  tuner = kt.Hyperband(lambda hp: build_model(embedding_matrix_context=embedding_matrix_context,\n",
        "                                              embedding_matrix_question=embedding_matrix_question, \n",
        "                                              model_config={\n",
        "                                                  'batch_size': hp.Choice('batch_size', [128]),\n",
        "                                                  \"dropout_rate\": hp.Choice('dropout_rate', [0.3]),\n",
        "                                                  \"regularizer\": hp.Choice(\"regularizer\", [1e-2, 1e-3]),\n",
        "                                                  \"enc_units\":  hp.Choice('units', [150, 300]),\n",
        "                                                  \"dec_units\": hp.Choice('units', [150, 300]),\n",
        "                                                  'max_length_context': dataset.train.element_spec[0].shape[1],\n",
        "                                                  'max_length_question': dataset.train.element_spec[1].shape[1],  \n",
        "                                              }, \n",
        "                                              compile_info={\n",
        "                                                  'loss': MaskedLoss(),\n",
        "                                                  'optimizer': keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])),  \n",
        "                                              }),\n",
        "                      objective=kt.Objective(\"val_perplexity\", direction=\"min\"),\n",
        "                      max_epochs=epochs_tuning,\n",
        "                      overwrite=True,\n",
        "                      directory=\"tuner\",\n",
        "                      project_name=\"tuner_qg\"\n",
        "                      )\n",
        "\n",
        "  tuner.search_space_summary()\n",
        "\n",
        "  #stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode=\"max\", restore_best_weights=True)\n",
        "\n",
        "  tuner.search( dataset.train, \n",
        "                validation_data = dataset.val,\n",
        "                epochs=epochs_tuning,\n",
        "                callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_perplexity', \n",
        "                                                  patience=5, \n",
        "                                                  mode='max', \n",
        "                                                  restore_best_weights=True)] )\n",
        "  best_hps = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "  print(f\"The hyperparameter search is complete.\\n\" \n",
        "        f\"The optimal regualizer rate is: {best_hps.get('batch_size')}.\\n\" \n",
        "        f\"The optimal regualizer rate is: {best_hps.get('regularizer')}.\\n\" \n",
        "        f\"The optimal rate for Dropout layer is: {best_hps.get('dropout_rate')}.\\n\"\n",
        "        f\"The optimal number of units is: {best_hps.get('enc_units')}.\\n\"\n",
        "        f\"The optimal learning rate for the optimizer is: {best_hps.get('learning_rate')}.\")"
      ],
      "metadata": {
        "id": "QhcBal1Lhnen"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Fine Tuning"
      ],
      "metadata": {
        "id": "Za1zIzi4hkki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,\n",
        "                dataset: NamedTuple,\n",
        "                training_info):\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param dataset: the split dataset\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(dataset.train,\n",
        "                        validation_data=dataset.val,\n",
        "                        **training_info)\n",
        "    print(\"Training completed\")\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "pVVLn9bMs2P3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': 20,\n",
        "    'batch_size': dataset_config['batch_size'],\n",
        "    'callbacks': [\n",
        "                  batch_loss,\n",
        "                  perplexity,\n",
        "                  accuracy,\n",
        "                  # lr_scheduler,\n",
        "                  # tensorboard_callback,\n",
        "                  # epoch_counter,\n",
        "                  # early_stopping\n",
        "                  ],\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    'batch_size': dataset_config['batch_size'],\n",
        "    'enc_units': 512,\n",
        "    'dec_units': 512,\n",
        "    'max_length_context': dataset_creator.max_length_context,\n",
        "    'max_length_question': dataset_creator.max_length_question,\n",
        "    'dropout_rate': 0.3,\n",
        "    'regularizer': 1e-3,\n",
        "}\n",
        "\n",
        "compile_info = {\n",
        "    'loss': MaskedLoss(),\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=8e-6)}\n",
        "\n",
        "qg_model = build_model(model_config, embedding_matrix_context, embedding_matrix_question, compile_info)"
      ],
      "metadata": {
        "id": "kVEf6VZIYbgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6942f1d7-6159-4844-d189-b1a30fe5469b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Configuration \n",
            "Parameters: {'batch_size': 256, 'enc_units': 512, 'dec_units': 512, 'max_length_context': 102, 'max_length_question': 27, 'dropout_rate': 0.3, 'regularizer': 0.001}\n",
            "Compile \n",
            "Parameters: {'loss': <__main__.MaskedLoss object at 0x7f5fa9be3c90>, 'optimizer': <keras.optimizer_v2.adam.Adam object at 0x7f5fa9be3490>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys97OwVn61UT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31e675f-72a1-4205-9c2b-ee694bf56f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training \n",
            "Parameters: {'verbose': 1, 'epochs': 20, 'batch_size': 256, 'callbacks': [<__main__.BatchLogs object at 0x7f5f7e3c3f50>, <__main__.BatchLogs object at 0x7f5fa8ea0290>, <__main__.BatchLogs object at 0x7f5fa8ea02d0>]}\n",
            "Epoch 1/20\n",
            "227/227 [==============================] - 309s 1s/step - loss: 7.0926 - accuracy: 0.0904 - perplexity: 1203.0251 - masked_accuracy: 0.1480 - val_loss: 7.0114 - val_accuracy: 0.0796 - val_perplexity: 1109.2246 - val_masked_accuracy: 0.1433\n",
            "Epoch 2/20\n",
            "227/227 [==============================] - 210s 927ms/step - loss: 5.9633 - accuracy: 0.0808 - perplexity: 388.8985 - masked_accuracy: 0.1475 - val_loss: 5.8980 - val_accuracy: 0.0806 - val_perplexity: 364.3171 - val_masked_accuracy: 0.1444\n",
            "Epoch 3/20\n",
            "227/227 [==============================] - 226s 996ms/step - loss: 5.8847 - accuracy: 0.0970 - perplexity: 359.5082 - masked_accuracy: 0.1791 - val_loss: 5.8171 - val_accuracy: 0.1197 - val_perplexity: 335.9850 - val_masked_accuracy: 0.1763\n",
            "Epoch 4/20\n",
            "177/227 [======================>.......] - ETA: 40s - loss: 5.7493 - accuracy: 0.1203 - perplexity: 313.9721 - masked_accuracy: 0.1852"
          ]
        }
      ],
      "source": [
        "if not keras_tuner:\n",
        "  history, qg_model = train_model(model=qg_model, \n",
        "                                  dataset=dataset,\n",
        "                                  training_info=training_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(hist):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20,5))\n",
        "\n",
        "    fig.suptitle('Training trends')\n",
        "    axs[0].plot(hist.history['accuracy'])\n",
        "    axs[0].plot(hist.history['val_accuracy'])\n",
        "    axs[0].plot(hist.history['masked_accuracy'])\n",
        "    axs[0].plot(hist.history['val_masked_accuracy'])\n",
        "    axs[0].set_title(\"model accuracy\")\n",
        "    axs[0].set_ylabel('accuracy')\n",
        "    axs[0].set_xlabel('epoch')\n",
        "    axs[0].legend(['train_acc', 'val_acc', 'train_mask_acc', 'val_mask_acc'], loc='best')\n",
        "    \n",
        "    axs[1].plot(hist.history['loss'])\n",
        "    axs[1].plot(hist.history['val_loss'])\n",
        "    axs[1].set_title(\"model batch loss\")\n",
        "    axs[1].set_ylabel('loss')\n",
        "    axs[1].set_xlabel('epoch')\n",
        "    axs[1].legend(['train_loss', 'val_loss'], loc='best')\n",
        "\n",
        "    axs[2].plot(hist.history['perplexity'])\n",
        "    axs[2].plot(hist.history['val_perplexity'])\n",
        "    axs[2].set_title(\"model perplexity\")\n",
        "    axs[2].set_ylabel('perplexity')\n",
        "    axs[2].set_xlabel('epoch')\n",
        "    axs[2].legend(['train_perplexity', 'val_perplexity'], loc='best')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  \n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "pDReuLzRh_LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorboard --logdir ./logs/gradient_tape"
      ],
      "metadata": {
        "id": "Yfxv7OYsVZij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC-NjKElfqNf"
      },
      "source": [
        "#6. Inference and Evaluation for QG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 Evaluation metrics\n",
        "\n",
        "To evaluate our model we will use the **ROUGE-$n$**, **ROUGE-L** and **METEOR** metrics imported from [Hugginface](https://huggingface.co/datasets)([github/datasets](https://github.com/huggingface/datasets/tree/master/metrics)). For more information the single metrics please refer to the official [repository](https://github.com/huggingface/datasets/tree/master/metrics).\n",
        "\n"
      ],
      "metadata": {
        "id": "wc9WXQ9Dqt2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "class METEOR(keras.metrics.Metric):\n",
        "  def __init__(self, name=f\"meteor_metric\", **kwargs):\n",
        "    \"\"\"\n",
        "    Initialize the metric object for computing METEOR.\n",
        "    \"\"\"\n",
        "    super(METEOR, self).__init__(name=name, **kwargs)\n",
        "    # Reference :- https://github.com/huggingface/datasets/tree/master/metrics/meteor\n",
        "    self.meteor = load_metric(\"meteor\")\n",
        "    self.scores = self.add_weight(name=f\"meteor_scores\", initializer=\"zeros\", dtype=tf.float64)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    results = self.meteor.compute(predictions=y_pred, references=y_true)\n",
        "    self.scores.assign(tf.constant(results['meteor'], dtype=tf.float64))\n",
        "\n",
        "  def result(self):\n",
        "    return {'meteor': self.scores}\n",
        "\n",
        "  def reset_state(self):\n",
        "    # The state of the metric will be reset at the start of each epoch.\n",
        "    self.scores.assign(0.0)\n",
        "\n",
        "class ROUGE(keras.metrics.Metric):\n",
        "  def __init__(self, name=f\"rouge_metric\", **kwargs):\n",
        "    \"\"\"\n",
        "    Initialize the metric object for computing ROUGE.\n",
        "    \"\"\"\n",
        "    super(ROUGE, self).__init__(name=name, **kwargs)\n",
        "    # Reference :- https://github.com/huggingface/datasets/tree/master/metrics/rouge\n",
        "    self.rouge = load_metric(\"rouge\")\n",
        "\n",
        "    self.precision_1_mid = self.add_weight(name=f\"rouge1_precision_mid_scores\", initializer=\"zeros\")\n",
        "    self.recall_1_mid = self.add_weight(name=f\"rouge1_recall_mid_scores\", initializer=\"zeros\")\n",
        "    self.fmeasure_1_mid = self.add_weight(name=f\"rouge1_fmeasure_mid_scores\", initializer=\"zeros\")\n",
        "\n",
        "    self.precision_2_mid = self.add_weight(name=f\"rouge2_precision_mid_scores\", initializer=\"zeros\")\n",
        "    self.recall_2_mid = self.add_weight(name=f\"rouge2_recall_mid_scores\", initializer=\"zeros\")\n",
        "    self.fmeasure_2_mid = self.add_weight(name=f\"rouge2_fmeasure_mid_scores\", initializer=\"zeros\")\n",
        "\n",
        "    self.precisionL_mid = self.add_weight(name=f\"rougeL_precision_high_scores\", initializer=\"zeros\")\n",
        "    self.recallL_mid = self.add_weight(name=f\"rougeL_precision_high_scores\", initializer=\"zeros\")\n",
        "    self.fmeasureL_mid = self.add_weight(name=f\"rougeL_fmeasure_high_scores\", initializer=\"zeros\")\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    results = self.rouge.compute(predictions=y_pred, references=y_true, rouge_types=['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "    self.precision_1_mid.assign(tf.constant(results['rouge1'].mid.precision, dtype=tf.float32))\n",
        "    self.recall_1_mid.assign(tf.constant(results['rouge1'].mid.recall, dtype=tf.float32))\n",
        "    self.fmeasure_1_mid.assign(tf.constant(results['rouge1'].mid.fmeasure, dtype=tf.float32))\n",
        "\n",
        "    self.precision_2_mid.assign(tf.constant(results['rouge2'].mid.precision, dtype=tf.float32))\n",
        "    self.recall_2_mid.assign(tf.constant(results['rouge2'].mid.recall, dtype=tf.float32))\n",
        "    self.fmeasure_2_mid.assign(tf.constant(results['rouge2'].mid.fmeasure, dtype=tf.float32))\n",
        "\n",
        "    self.precisionL_mid.assign(tf.constant(results['rougeL'].mid.precision, dtype=tf.float32))\n",
        "    self.recallL_mid.assign(tf.constant(results['rougeL'].mid.recall, dtype=tf.float32))\n",
        "    self.fmeasureL_mid.assign(tf.constant(results['rougeL'].mid.fmeasure, dtype=tf.float32))\n",
        "\n",
        "  def result(self):\n",
        "    return {'precision_1': self.precision_1_mid.numpy(),\n",
        "            'recall_1': self.recall_1_mid.numpy(),\n",
        "            'fmeasure_1': self.fmeasure_1_mid.numpy(),\n",
        "            'precision_2': self.precision_1_mid.numpy(),\n",
        "            'recall_2': self.recall_1_mid.numpy(),\n",
        "            'fmeasure_2': self.fmeasure_1_mid.numpy(),\n",
        "            'precisionL': self.precisionL_mid.numpy(),\n",
        "            'recallL': self.recallL_mid.numpy(),\n",
        "            'fmeasureL': self.fmeasureL_mid.numpy()}\n",
        "\n",
        "  def reset_state(self):\n",
        "    # The state of the metric will be reset at the start of each epoch.\n",
        "    self.precision_1_mid.assign(0.0)\n",
        "    self.recall_1_mid.assign(0.0)\n",
        "    self.fmeasure_1_mid.assign(0.0)\n",
        "\n",
        "    self.precision_2_mid.assign(0.0)\n",
        "    self.recall_2_mid.assign(0.0)\n",
        "    self.fmeasure_2_mid.assign(0.0)\n",
        "\n",
        "    self.precisionL_mid.assign(0.0)\n",
        "    self.recallL_mid.assign(0.0)\n",
        "    self.fmeasureL_mid.assign(0.0)"
      ],
      "metadata": {
        "id": "jHaP7HgBUzhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezgR7c68_0nv"
      },
      "source": [
        "## 6.2 Inference for QG\n",
        "In this section we will provide the class and the methods for the inference part. More specifically, both auxiliary and inferencing methods:\n",
        "1. `token_to_string()`:\n",
        "2. `string_to_token()`:\n",
        "3. `create_mask()`:\n",
        "4. `temperature_sampling()`:\n",
        "5. `generate_question()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn2NTxAq_2sy"
      },
      "outputs": [],
      "source": [
        "class QuestionGenerator(tf.Module):\n",
        "  def __init__(self, model, evaluation_config, tokenizer_context, tokenizer_question):\n",
        "    self.model = model\n",
        "    self.encoder = model.encoder\n",
        "    self.decoder = model.decoder\n",
        "\n",
        "    self.tokenizer_context = tokenizer_context\n",
        "    # The tokenizer will be used for the conversion from question tokens to\n",
        "    # strings. It requires the tokenizer fit on the questions. \n",
        "    self.tokenizer_question = tokenizer_question\n",
        "   \n",
        "    self.result_tokens = None   # Tokens predicted\n",
        "    self.result_text = None     # Text predicted\n",
        "    self.token_mask = self.create_mask()  # Mask for the tokens\n",
        "\n",
        "    self.start_idx = tokenizer_question.word_index['<sos>']\n",
        "    self.end_idx = tokenizer_question.word_index['<eos>']\n",
        "    self.unk_idx = tokenizer_question.word_index['<unk>']\n",
        "\n",
        "    # Config\n",
        "    self.temperature = evaluation_config['temperature']\n",
        "\n",
        "    # Metrics\n",
        "    # These ones are computed and refreshed for each batch\n",
        "    self.meteor_metric_batch = METEOR()\n",
        "    self.rouge_metric_batch = ROUGE()\n",
        "    \n",
        "    # These ones are the mean value among all the batches\n",
        "    self.meteor_metric = tf.keras.metrics.Mean(name='meteor_mean')\n",
        "    self.rouge1_precision_metric = tf.keras.metrics.Mean(name='rouge1_precision_mean')\n",
        "    self.rouge1_recall_metric = tf.keras.metrics.Mean(name='rouge1_recall_mean')\n",
        "    self.rouge1_fmeasure_metric = tf.keras.metrics.Mean(name='rouge1_fmeasure_mean')\n",
        "    self.rouge2_precision_metric = tf.keras.metrics.Mean(name='rouge2_precision_mean')\n",
        "    self.rouge2_recall_metric = tf.keras.metrics.Mean(name='rouge2_recall_mean')\n",
        "    self.rouge2_fmeasure_metric = tf.keras.metrics.Mean(name='rouge2_fmeasure_mean')\n",
        "    self.rougeL_precision_metric = tf.keras.metrics.Mean(name='rougeL_precision_mean')\n",
        "    self.rougeL_recall_metric = tf.keras.metrics.Mean(name='rougeL_recall_mean')\n",
        "    self.rougeL_fmeasure_metric = tf.keras.metrics.Mean(name='rougeL_fmeasure_mean')\n",
        "\n",
        "  def token_to_string(self, result_tokens: tf.Tensor):  \n",
        "    \"\"\"\n",
        "    This method converts token IDs to text by using a given mapping.\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    list_text = self.tokenizer_question.sequences_to_texts(list_tokens)\n",
        "    \n",
        "    # list_text = tf.convert_to_tensor([list_text])\n",
        "    # result_text = tf.strings.reduce_join(list_text, axis=0, separator=' ')\n",
        "    # result_text = tf.strings.strip(result_text)\n",
        "    result_text = [s.split() for s in list_text]\n",
        "\n",
        "    self.result_tokens = result_tokens\n",
        "    self.result_text = result_text\n",
        "    return result_text\n",
        "  \n",
        "  def create_mask(self):\n",
        "    \"\"\"\n",
        "    This method creates a mask for the padding, the unknwon words and the start/ending tokens.\n",
        "    \"\"\"\n",
        "    masked_words = ['<pad>', '<sos>', '<eos>']\n",
        "    token_mask_ids = [self.tokenizer_question.word_index[mask] for mask in masked_words]\n",
        "\n",
        "    token_mask = np.zeros(shape=(len(self.tokenizer_question.word_index),), dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    return token_mask\n",
        "\n",
        "  def remove_tags(self, result_tokens: tf.Tensor):\n",
        "    \"\"\"\n",
        "    This method removes the padding, start and end of sentence tags\n",
        "    \"\"\"\n",
        "    list_tokens = result_tokens.numpy().tolist()\n",
        "    tag_words = ['<pad>', '<sos>', '<eos>']\n",
        "    token_tag_ids = [self.tokenizer_question.word_index[tag] for tag in tag_words]\n",
        "    \n",
        "    #For every element of the batch we extract the token list and we remove the unwanted tokens\n",
        "    list_tokens = [[token for token in token_list if token not in token_tag_ids] for token_list in list_tokens]\n",
        "    \n",
        "    texts = self.tokenizer_question.sequences_to_texts(list_tokens)\n",
        "    texts = [s.split() for s in texts]\n",
        "\n",
        "    return texts\n",
        "\n",
        "  def _evaluate(self, inputs, max_length):\n",
        "    self.meteor_metric.reset_state()\n",
        "    self.rouge1_precision_metric.reset_state()\n",
        "    self.rouge1_recall_metric.reset_state()\n",
        "    self.rouge1_fmeasure_metric.reset_state()\n",
        "    self.rouge2_precision_metric.reset_state()\n",
        "    self.rouge2_recall_metric.reset_state()\n",
        "    self.rouge2_fmeasure_metric.reset_state()\n",
        "    self.rougeL_precision_metric.reset_state()\n",
        "    self.rougeL_recall_metric.reset_state()\n",
        "    self.rougeL_fmeasure_metric.reset_state()\n",
        "\n",
        "    for (context, question_true) in tqdm(inputs):\n",
        "      self.meteor_metric_batch.reset_state()\n",
        "      self.rouge_metric_batch.reset_state()\n",
        "\n",
        "      question_true = self.remove_tags(question_true)\n",
        "      prediction = self.predict_step(inputs=context, max_length=max_length, return_attention=False, pretty_predict=False)\n",
        "      question_pred = prediction['text']\n",
        "\n",
        "      # Compute the metric for the current batch\n",
        "      self.meteor_metric_batch.update_state(y_true=question_true, y_pred=question_pred)\n",
        "      # Compute the mean over the batches\n",
        "      self.meteor_metric.update_state(self.meteor_metric_batch.result()['meteor'])\n",
        "\n",
        "      self.rouge_metric_batch.update_state(y_true=question_true, y_pred=question_pred)\n",
        "      self.rouge1_precision_metric.update_state(self.rouge_metric_batch.result()['precision_1'])\n",
        "      self.rouge1_recall_metric.update_state(self.rouge_metric_batch.result()['recall_1'])\n",
        "      self.rouge1_fmeasure_metric.update_state(self.rouge_metric_batch.result()['fmeasure_1'])\n",
        "      self.rouge2_precision_metric.update_state(self.rouge_metric_batch.result()['precision_2'])\n",
        "      self.rouge2_recall_metric.update_state(self.rouge_metric_batch.result()['recall_2'])\n",
        "      self.rouge2_fmeasure_metric.update_state(self.rouge_metric_batch.result()['fmeasure_2'])\n",
        "      self.rougeL_precision_metric.update_state(self.rouge_metric_batch.result()['precisionL'])\n",
        "      self.rougeL_recall_metric.update_state(self.rouge_metric_batch.result()['recallL'])\n",
        "      self.rougeL_fmeasure_metric.update_state(self.rouge_metric_batch.result()['precisionL'])\n",
        "\n",
        "      print()\n",
        "      # print('METEOR batch', self.meteor_metric_batch.result())\n",
        "      # print('ROUGE batch', self.rouge_metric_batch.result())\n",
        "\n",
        "    return {'METEOR': self.meteor_metric.result().numpy(),\n",
        "            'ROUGE_1_PRECISION': self.rouge1_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_RECALL': self.rouge1_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_FMEASURE': self.rouge1_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_PRECISION': self.rouge2_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_RECALL': self.rouge2_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_FMEASURE': self.rouge2_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_PRECISION': self.rougeL_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_RECALL': self.rougeL_precision_metric.result().numpy(),\n",
        "            'ROUGE_1_FMEASURE': self.rougeL_precision_metric.result().numpy()}\n",
        "\n",
        "  def predict(self, inputs, max_length, return_attention=False, pretty_predict=False):\n",
        "    \"\"\"\n",
        "    Generates output predictions for the input samples.\n",
        "    In addition it could also return the associated attention weights.\n",
        "\n",
        "    It returns a list of dictionaries.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for (context, _) in tqdm(inputs):\n",
        "      results.append(self.predict_step(inputs=context, \n",
        "                                       max_length=max_length, \n",
        "                                       return_attention=return_attention,\n",
        "                                       pretty_predict=pretty_predict))\n",
        "\n",
        "    return results\n",
        "\n",
        "  def predict_step(self, inputs, max_length, return_attention, pretty_predict):\n",
        "    # Similarly for what it has been done in the train step\n",
        "    encoder_output, encoder_state = self.encoder(inputs)\n",
        "    decoder_state = encoder_state\n",
        "\n",
        "    # Generate the first token of each sentence, that is the <sos> token\n",
        "    new_token = tf.fill([self.model.batch_size, 1], self.start_idx)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "\n",
        "    done = tf.zeros(shape=(self.model.batch_size, 1), dtype=tf.bool)\n",
        "    unk = tf.zeros(shape=(self.model.batch_size, 1), dtype=tf.bool)\n",
        "    \n",
        "    for _ in range(max_length):\n",
        "      # Decode the token at the next timestep\n",
        "      decoder_logits, attention_weights, decoder_state = self.decoder([new_token, encoder_output], state=decoder_state)\n",
        "      \n",
        "      attention.append(attention_weights)\n",
        "\n",
        "      # Sample the new token accordingly to the distribution produced by the decoder\n",
        "      new_token = self.temperature_sampling(decoder_logits)\n",
        "\n",
        "      # if a sequence has reached <eos> set it as done\n",
        "      done = done | (new_token == self.end_idx)\n",
        "      # Once a sequence is done it only produces 0-padding.\n",
        "      new_token = tf.where(done, tf.constant(0, dtype=tf.int64), new_token)\n",
        "\n",
        "      # if a token produce has value <unk> set it as unk\n",
        "      unk = unk | (new_token == self.unk_idx)\n",
        "      # Once a token has been tagged as unk we have to chenage its value with \n",
        "      # the value in the context that has the highest attention\n",
        "      highest_attention = tf.math.argmax(attention_weights, axis=-1)\n",
        "      context_attention = tf.gather(inputs, highest_attention, axis=-1, batch_dims=1)\n",
        "      new_token = tf.where(unk, context_attention, new_token)\n",
        "\n",
        "      result_tokens.append(new_token)\n",
        "\n",
        "      if tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.token_to_string(result_tokens)\n",
        "    if pretty_predict: result_text = self.prettify(result_text)\n",
        "\n",
        "    attention_stack = tf.concat(attention, axis=-1)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}\n",
        "\n",
        "  def prettify(self, result_text):\n",
        "    results = []\n",
        "    for sen in result_text:\n",
        "      results.append(\" \".join(list(sen)))\n",
        "    return results\n",
        "\n",
        "  def temperature_sampling(self, logits):\n",
        "    \"\"\"\n",
        "    For the temperature choice see here:\n",
        "      Reference :- https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "    \"\"\"\n",
        "    # First of all we use broadcast the generated mask to the expected logits' shape\n",
        "    # token_mask shape: (batch_size, timestep, vocab_size)\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    # The logits for all the tokens that have to not be used are set to -1.0\n",
        "    logits = tf.where(token_mask, -1.0, logits)\n",
        "\n",
        "    # Freezing function\n",
        "    # Higher temperature -> greater variety\n",
        "    # Lower temperature -> grammatically correct\n",
        "    if self.temperature == 0.0:\n",
        "      # the freezing function is the argmax, behaving like a greedy search\n",
        "      new_token = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      # the freezing function now scales the logits.\n",
        "      # for temperature == 1.0 is the identity function\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_token = tf.random.categorical(logits / self.temperature, num_samples=1)\n",
        "    return new_token\n",
        "\n",
        "  def show(self, inputs, max_length, complete_tokenizer, max_q):\n",
        "    batch_idx = random.randint(1, inputs.cardinality())\n",
        "    predictions = self.predict_step(inputs.batch(batch_idx)[0], max_length, False, True)\n",
        "    predictions = predictions.numpy().tolist()\n",
        "    predictions = predictions[0][:max_q]\n",
        "    print(predictions)\n",
        "    for i, (context, question) in enumerate(inputs.batch(batch_idx)):\n",
        "      if i == 1: break\n",
        "      question = question.numpy().tolist()\n",
        "      references = question[0][:max_q]\n",
        "      references = self.tokenizer_question.sequences_to_texts(references)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q2pWJ5YDfqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a843a8fb-e4cc-4c17-f5dc-5bd68b28cef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "qg_evaluator = QuestionGenerator(model=qg_model, evaluation_config=evaluation_config, tokenizer_question=dataset_creator.tokenizer_question, tokenizer_context=dataset_creator.tokenizer_context)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qg_evaluator.show(dataset.test, 10, None, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "rrKJjWAG1MBY",
        "outputId": "acb5b69a-901d-4b94-b1f7-b65fa78cd8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-281-f30afc1e94cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqg_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-279-01eecd5d395b>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, inputs, max_length, complete_tokenizer, max_q)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'BatchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred = qg_evaluator.predict(dataset.test, 10, False, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj80soL6yPnW",
        "outputId": "9cc14688-02c7-435a-d087-6f9e13428755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [01:21<00:00,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44.6 s, sys: 1.47 s, total: 46.1 s\n",
            "Wall time: 1min 21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "pred = qg_evaluator._evaluate(dataset.test, 5)"
      ],
      "metadata": {
        "id": "DkDmgT1JPs0I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZS1Y56XayL9_",
        "xl7H67iPmZHL",
        "Y8NrwPONoVQ-",
        "sFBKCIE3Jxf2",
        "MvU7n1LboA6g",
        "kx2f7Nn_4en9",
        "FF5Rtd4uqa_k",
        "wjVfZgIIf1RV",
        "fbjSxPGcFud_",
        "dtM9nOQrf3jq",
        "IF5J42g1l-be",
        "0imUvBAg14Ep",
        "qyRA2RxZNsx4",
        "8WOVZj966EPP",
        "LsdTOIHxEphh",
        "BEW5hmQMhgyM"
      ],
      "name": "main_neuralqg.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}